{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Homework 8\n",
        "author:\n",
        "  - name: 'Shao-Ting Chiu (UIN:433002162)'\n",
        "    url: stchiu@email.tamu.edu\n",
        "    affiliation: 'Department of Electrical and Computer Engineering, Texas A\\&M University'\n",
        "date: today\n",
        "bibliography: ../ref.bib\n",
        "format:\n",
        "  html:\n",
        "    table-of-contents: true\n",
        "    keep-ipynb: true\n",
        "  pdf:\n",
        "    table-of-contents: true\n",
        "    keep-tex: true\n",
        "    include-in-header: |\n",
        "      \\usepackage{fontspec}\n",
        "      \\setmainfont{DejaVu Sans}\n",
        "      \\setmonofont{DejaVu Sans Mono}\n",
        "execute:\n",
        "  echo: true\n",
        "  freeze: auto\n",
        "---"
      ],
      "id": "05f516fe"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Description\n",
        "\n",
        "- Course: STAT638, 2022 Fall\n",
        "\n",
        "> Read Chapter 8 in the Hoff book. Then do the following exercises in Hoff: 8.1 and 8.3.\n",
        "> \n",
        "> Please note some typos in 8.1: All $\\theta_i$'s should be $\\theta_j$'s.\n",
        "> \n",
        "> For 8.1(c), you may find [the law of total (co-)variance](https://en.wikipedia.org/wiki/Law_of_total_covariance) useful. In addition, remember that all of these laws also hold for conditional distributions (e.g., when conditioning on additional quantities such as $\\mu$ and $\\tau^2$ in all terms on the left- and right-hand side of the equation).\n",
        "\n",
        "## Computational Enviromnent Setup[^ques]\n",
        "\n",
        "### Third-party libraries"
      ],
      "id": "81a381d6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Pkg\n",
        "Pkg.activate(\"hw8\")\n",
        "using Distributions\n",
        "using DataFrames\n",
        "using Turing\n",
        "using Plots\n",
        "using DelimitedFiles\n",
        "using LinearAlgebra\n",
        "using Statistics\n",
        "using Turing\n",
        "using StatsBase\n",
        "using StatsPlots\n",
        "import Random\n",
        "Random.seed!(2022)"
      ],
      "id": "94e182bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version"
      ],
      "id": "1c827b58"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Pkg.status()\n",
        "VERSION"
      ],
      "id": "629c83cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[^ques]: I use special character in Julia code. Unfortunately, those are not displayed in PDF version."
      ],
      "id": "e8984bb8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Problem 8.1\n",
        "\n",
        "> Components of variance: Consider the hierarchical model where\n",
        "> \n",
        "> $$\\theta_1, \\dots, \\theta_m | \\mu, \\tau^2 \\sim i.i.d. \\text{normal}(\\mu, \\tau^2)$$\n",
        "> \n",
        "> $$y_{1,j}, \\dots, y_{n_j, j} |\\theta_j, \\sigma^2 \\sim i.i.d. \\text{normal}(\\theta_j, \\sigma^2)$$\n",
        "> For this problem, we will eventually compute the following:\n",
        ">\n",
        "> - $Var[y_{i,j}|\\theta_i, \\sigma^2]$, $Var[\\bar{y}_{\\cdot,j}|\\theta_i, \\sigma^2]$, $Cov[y_{i_1,j}, y_{i_2, j}|\\theta_j, \\sigma^2]$\n",
        "> - $Var[y_{i,j}|\\mu, \\tau^2]$, $Var[\\bar{y}_{\\cdot,j}|\\mu, \\tau^2]$, $Cov[y_{i_1,j}, y_{i_2, j}|\\mu, \\tau^2]$\n",
        "> First, lets use our intuition to guess at the answers:\n",
        "\n",
        "\n",
        "### (a)\n",
        "\n",
        "> Which do you think is bigger, $Var[y_{i,j}|\\theta_i, \\sigma^2]$ or $Var[y_{i,j}|\\mu, \\tau^2]$? To guide your intuition, you can interpret the first as the variability of the $Y$'s when sampling from a fixed group, and the second as the variability in first sampling a group, then sampling a unit from within the group.\n",
        "\n",
        "- $Var[y_{i,j} | \\mu, \\tau^2]$ because $\\theta_j$ is uncertain and the between-group varibability create additional uncertainty.\n",
        "\n",
        "### (b)\n",
        "\n",
        "> Do you think $Cov[y_{i_1,j}, y_{i_2, j}|\\theta_j, \\sigma^2]$ is negative, positive, or zero? Answer the same for $Cov[y_{i_1,j}, y_{i_2, j}|\\mu, \\tau^2]$. You may want to think about what $y_{i_2, j}$ tells you about $y_{i_1, j}$ if $\\theta_j$ is known, and what it tells you when $\\theta_j$ is unknown.\n",
        "\n",
        "\n",
        "$Cov[y_{i_1,j}, y_{i_2, j}|\\theta_j, \\sigma^2]$\n",
        "\n",
        "Because $y_{i_1, j}$ and $y_{i_2, j}$ is i.i.d. sampled, I expect $Cov[y_{i_1,j}, y_{i_2, j}|\\theta_j, \\sigma^2]$ to be zero.\n",
        "\n",
        "$Cov[y_{i_1,j}, y_{i_2, j}|\\mu, \\tau^2]$\n",
        "\n",
        "$y_{1,j}$ does tell information about $y_{2,j}$. The covariance $Cov[y_{i_1,j}, y_{i_2, j}|\\mu, \\tau^2]$ is likely to be positive because values from same $\\theta_j$ tend to be close together.\n",
        "\n",
        "### (c)\n",
        "\n",
        "> Now compute each of the six quantities above and compare to your answers in (a) and (b). [^tot-var]\n",
        "\n",
        "\\begin{align}\n",
        "    Var[y_{i,j}|\\theta_i, \\sigma^2]%\n",
        "    &= \\sigma^2\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "    Var[\\bar{y}_{\\cdot,j}|\\theta_i, \\sigma^2]%\n",
        "    &= Var[\\sum_{i'=1}^{n_j}y_{i',j}/n |\\theta_i, \\sigma^2]\\\\ \n",
        "    &= \\frac{1}{n^2}Var[\\sum_{i'=1}^{n_j}y_{i',j} |\\theta_i, \\sigma^2]\\\\ \n",
        "    &= \\frac{1}{n^2} \\sum_{i'=1}^{n_j} Var[y_{i',j} |\\theta_i, \\sigma^2]\\\\ \n",
        "    &= \\frac{1}{n} Var[y_{i',j} |\\theta_i, \\sigma^2]\\\\\n",
        "    &= \\frac{\\sigma^2}{n} \n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "    Cov[y_{i_1,j}, y_{i_2, j}|\\theta_j, \\sigma^2]% \n",
        "    &= E[y_{i_1, j} y_{i_2, j}] - E[y_{i_1, j}]E[y_{i_2, j}]\\\\ \n",
        "    &= E[y_{i_1, j}]E[y_{i_2, j}] - E[y_{i_1, j}]E[y_{i_2, j}]\\\\ \n",
        "    &= 0\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "    Var[y_{i,j}|\\mu, \\tau^2]%\n",
        "    &= E(Var[y_{i,j}|\\mu, \\tau^2, \\theta, \\sigma^2]|\\mu, \\tau^2) + Var(E[y_{i,j}|\\mu, \\tau^2, \\theta, \\sigma^2]|\\mu, \\tau^2)\\\\ \n",
        "    &= E(\\sigma^2 | \\mu, \\tau^2) + Var(\\theta | \\mu, \\tau^2)\\\\ \n",
        "    &= \\sigma^2 + \\tau^2\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "    Var[\\bar{y}_{\\cdot,j}|\\mu, \\tau^2]%\n",
        "    &= E(Var[\\bar{y}_{\\cdot,j}|\\mu, \\tau^2, \\theta, \\sigma^2]|\\mu, \\tau^2) + Var(E[\\bar{y}_{\\cdot,j}|\\mu, \\tau^2, \\theta, \\sigma^2]|\\mu, \\tau^2)\\\\ \n",
        "    &= E(\\frac{\\sigma^2}{n}|\\mu,\\tau^2) + Var(\\theta | \\mu, \\tau^2)\\\\ \n",
        "    &= \\frac{\\sigma^2}{n} + \\tau^2\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "    Cov[y_{i_1, j}, y_{i_2, j}|\\mu, \\tau^2]%\n",
        "    &= E(Cov[y_{i_1, j}, y_{i_2, j} | \\theta, \\sigma^2, \\mu, \\tau^2]| \\mu, \\tau^2) \\\\ \n",
        "    &+ Cov(E[y_{i_1, j} | \\theta, \\sigma^2, \\mu, \\tau^2], E[y_{i_2, j} | \\theta, \\sigma^2, \\mu, \\tau^2] | \\mu, \\tau^2)\\\\ \n",
        "    &= 0 + Cov(\\theta, \\theta | \\mu, \\tau^2)\\\\ \n",
        "    &= E[\\theta^2|\\mu, \\tau^2] - E[\\theta|\\mu, \\tau^2]^2\\\\\n",
        "    &= Var(\\theta |\\mu, \\tau^2)\\\\ \n",
        "    &= \\tau^2\n",
        "\\end{align}\n",
        "\n",
        "[^tot-var]: $Var(Y) =  E[Var(Y|X)] + Var(E[Y|X])$\n",
        "\n",
        "### (d)\n",
        "\n",
        "> Now assume we have a prior $p(\\mu)$ for $\\mu$. Using Bayes' rule, show that \n",
        "> $$p(\\mu|\\theta_1, \\dots, \\theta_m, \\sigma^2, \\tau^2, y_1, \\dots, y_m) = p(\\mu|\\theta_1, \\dots, \\theta_m, \\tau^2)$$\n",
        "> Interpret in words what this means.\n",
        "\n",
        "\\begin{align}\n",
        "p(\\mu|\\theta_1, \\dots, \\theta_m, \\sigma^2, \\tau^2, y_1, \\dots, y_m)%\n",
        "&= \\frac{p(\\sigma^2, y_1, \\dots, y_m | \\mu, \\theta_1, \\dots, \\theta_m, \\tau^2) p(\\mu |\\theta_1, \\dots, \\theta_m, \\tau^2)}{ p(\\sigma^2, y_1, \\dots, y_m | \\theta_1, \\dots, \\theta_m, \\tau^2) }\\\\ \n",
        "&= p(\\mu|\\theta_1, \\dots, \\theta_m, \\tau^2)\n",
        "\\end{align}\n",
        "\n",
        "where $p(\\sigma^2, y_1, \\dots, y_m | \\mu, \\theta_1, \\dots, \\theta_m, \\tau^2) =  p(\\sigma^2, y_1, \\dots, y_m |  \\theta_1, \\dots, \\theta_m, \\tau^2)$ because knowing $\\mu$ doesn't provide more information when $\\theta_1, \\dots, \\theta_m$ are known.\n",
        "\n",
        "## Problem 8.3\n",
        "\n",
        "> Herarchical modeling: The files [`school1.dat`](data/school1.dat) through [`school8.dat`](data/school8.dat) give weekly hours spent on homework for students sampled from eight different schools. Obtain posterior distributions for the true means for the eight different schools using a herarchical normal model with the following prior parameters:\n",
        "> $$\\mu_0 = 7, \\gamma^{2}_{0} = 5, \\tau^{2}_{0}=10, \\eta_0 = 2, \\sigma^{2}_{0} = 15, \\nu_0 = 2$$\n"
      ],
      "id": "7f682b38"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dsch = Dict()\n",
        "nsch = 8\n",
        "for i in 1:nsch\n",
        "   dsch[i] = readdlm(\"data/school$i.dat\")\n",
        "end"
      ],
      "id": "c842b30c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (a)\n",
        "\n",
        "> Run a Gibbs sampling algorithm to approximate the posterior distribution of $\\{\\theta, \\sigma^2, \\mu, \\tau^2\\}$. Assess the convergence of the Markov chain, and find the effective sample size for $\\{\\sigma^2, \\mu, \\tau^2\\}$. Run the chain long enough so that the effective sample sizes are all above $1000$.\n"
      ],
      "id": "97829ca7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prior\n",
        "μ0 = 7.\n",
        "γ0² = 5.\n",
        "τ0² = 10.\n",
        "η0 = 2.\n",
        "σ0² = 15.\n",
        "ν0 = 2.\n",
        "\n",
        "# Data\n",
        "ns = [ length(dsch[i]) for i in 1:nsch]\n",
        "n = sum(ns)\n",
        "m = length(dsch)\n",
        "ȳs = [mean(dsch[i]) for i in 1:nsch]\n",
        "s²s = [ (ns[i] - 1)^-1 * sum( (dsch[i] .- ȳs[i]).^2) for i in 1:nsch]\n",
        "# posterior\n",
        "\n",
        "function τ²_pos(m, η0, θv, μ, τ0²)\n",
        "    ths² = sum([ (θ- μ)^2 for θ in θv])\n",
        "    α = (m + η0)* 0.5\n",
        "    β = (ths² + η0*τ0²)/2\n",
        "    return InverseGamma(α, β)\n",
        "end\n",
        "\n",
        "function σ²_pos(n, ν0, σ0², ns, s²s, ȳs, θs)\n",
        "    α = (n+ν0)/2\n",
        "    β =( sum((ns .- 1) .* s²s .+ ns .* (ȳs .- θs).^2) + ν0*σ0²)/2\n",
        "    return InverseGamma(α, β)\n",
        "end\n",
        "\n",
        "function μ_pos(m, τ², θs, γ0², μ0)\n",
        "    γm² = (m/τ² + 1/γ0²)^-1\n",
        "    θ̄ = mean(θs)\n",
        "\n",
        "    a = γm²*(m*θ̄/τ² + μ0/τ²)\n",
        "    return Normal(a, γm²)\n",
        "end\n",
        "\n",
        "function θ_pos(τ², ȳ, n, σ², μ)\n",
        "    τ̃² = (n/σ² + 1/τ²)^-1\n",
        "    a = τ̃²*(n*ȳ/σ² + μ/τ²)\n",
        "    return Normal(a, τ̃²)\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "Effective Sample Size\n",
        "\"\"\"\n",
        "function ess(v)\n",
        "    n = length(v)\n",
        "    c = sum(autocov(v, collect(1:n-1)))\n",
        "    return n/(1+2*c)\n",
        "end\n",
        "\n",
        "# Sampling\n",
        "smp = 4000\n",
        "τ²s = zeros(smp)\n",
        "σ²s = zeros(smp)\n",
        "μs = zeros(smp)\n",
        "θs = zeros(smp, m)\n",
        "\n",
        "\n",
        "σ²s[1] = rand(InverseGamma(ν0/2, ν0*σ0²/2))\n",
        "τ²s[1] = rand(InverseGamma(η0 /2, η0 *τ0²/2))\n",
        "μs[1] = rand(Normal(μ0, γ0²))\n",
        "#θs[1,:] = [rand(θ_pos(τ²s[1], ȳs[i], ns[i], σ²s[1], μs[1])) for i in 1:m]\n",
        "θs[1,:] = rand(Normal(μs[1], τ²s[1]), m)\n",
        "\n",
        "for s in 2:smp\n",
        "    σ²s[s] = rand(σ²_pos(n, ν0, σ0², ns, s²s, ȳs, θs[s-1,:]))\n",
        "    τ²s[s] = rand(τ²_pos(m, η0, θs[s-1,:], μs[s-1], τ0²))\n",
        "    θs[s,:] = [rand(θ_pos(τ²s[s-1], ȳs[i], ns[i], σ²s[s-1], μs[s-1])) for i in 1:m]\n",
        "    μs[s] = rand(μ_pos(m, τ²s[s-1], θs[s-1,:], γ0², μ0))\n",
        "end"
      ],
      "id": "65421825",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i in [τ²s, σ²s, μs, θs]\n",
        "    plot(i)\n",
        "end\n",
        "\n",
        "p1 = plot(τ²s[2:end], label=\"τ²\")\n",
        "p2 = plot(σ²s[2:end], label=\"σ²\")\n",
        "p3 = plot(μs[2:end], label=\"μ\")\n",
        "\n",
        "plot(p1, p2, p3)"
      ],
      "id": "f12e2fb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Effetive Sample Size\n",
        "\n",
        "- $\\tau^2$\n",
        "``` julia\n",
        "ess(τ²s)\n",
        "```\n",
        "```\n",
        "1523.525\n",
        "```\n",
        "\n",
        "- $\\sigma^2$\n",
        "``` julia\n",
        "ess(σ²s)\n",
        "```\n",
        "```\n",
        "1234.234\n",
        "```\n",
        "\n",
        "- $\\mu$\n",
        "``` julia\n",
        "ess(μs)\n",
        "```\n",
        "```\n",
        "1045.242\n",
        "```\n",
        "\n",
        "\n",
        "### (b)\n",
        "\n",
        "> Compute posterior means and $95\\%$ confidence regions for $\\{\\sigma^2, \\mu, \\tau^2\\}$. Also, compare the posterior densities to the prior densities, and discuss what was learned from the data.\n",
        "\n",
        "- $\\sigma^2$"
      ],
      "id": "93d609a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "quantile(σ²s, [0.025, 0.5, 0.975])"
      ],
      "id": "a2d5f2ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- $\\mu$"
      ],
      "id": "add8c4f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "quantile(μs, [0.025, 0.5, 0.975])"
      ],
      "id": "1d0c9338",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- $\\tau$"
      ],
      "id": "f9f0a2f8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "quantile(τ²s, [0.025, 0.5, 0.975])"
      ],
      "id": "082755bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pu = density(μs, label=\"μₙ\")\n",
        "pt = density(σ²s,label=\"σ²ₙ\")\n",
        "ps = density(τ²s, label=\"τ²ₙ\")\n",
        "\n",
        "plot!(pu, Normal(μ0 , γ0²), label= \"μ0\")\n",
        "plot!(pt, InverseGamma(η0 /2, η0 *τ0²/2), label= \"σ²0\")\n",
        "plot!(ps, InverseGamma(ν0/2, ν0*σ0²/2), label= \"τ²0\")\n",
        "\n",
        "xlims!(pt, 0,30)\n",
        "xlims!(ps, 0,30)\n",
        "\n",
        "plot(pu, pt, ps)"
      ],
      "id": "01ef109e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estimations of $\\mu$ and $\\tau$ are similar in prior and posterior. However, $\\sigma^2$ is different. \n",
        "\n",
        "### (c)\n",
        "> Plot the posterior density of $R=\\frac{\\tau^2}{\\sigma^2 + \\tau^2}$ and compare it to a plot of the prior density of $R$. Describe the evidence for between-school variation.\n"
      ],
      "id": "8d4b75aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "σ²_prs = rand(InverseGamma(ν0/2, ν0*σ0²/2), 1000)\n",
        "τ²_prs = rand(InverseGamma(η0 /2, η0 *τ0²/2), 1000)\n",
        "\n",
        "\n",
        "\n",
        "R_prs = τ²_prs ./ (σ²_prs .+ τ²_prs)\n",
        "R_pos = τ²s ./ (σ²s .+ τ²s)\n",
        "\n",
        "pr = density(R_prs, label=\"R Prior\", xlabel=\"R\", ylabel=\"density\")\n",
        "density!(pr, R_pos, label=\"R posterior\")"
      ],
      "id": "0c3de995",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$R$ represents the quantity of vairance in between-group. The prior is not certain about the specific quantity, but after applying posterior inference. The posterior probability of $R$ is peaked and more cetain about the value is around $0.3$.\n",
        "\n",
        "### (d)\n",
        "\n",
        "> Obtain the posterior probability that $\\theta_7$ is smaller than $\\theta_6$, as well as the posterior probability that $\\theta_7$ is smaller than of all the $\\theta$'s.\n",
        "\n",
        "-  p($\\theta_7$ is smaller than $\\theta_6$)\n"
      ],
      "id": "e00d8617"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mean(θs[:,7] .< θs[:,6])"
      ],
      "id": "fce50c70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- p($\\theta_7$ is smaller than of all the $\\theta$'s)\n"
      ],
      "id": "70ac1279"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "res = zeros(size(θs)[1])\n",
        "for i in 1 : size(θs)[1]\n",
        "    if argmin(θs[i,:]) == 7\n",
        "        res[i] = 1\n",
        "    end\n",
        "end\n",
        "mean(res)"
      ],
      "id": "5d5cb8d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (e)\n",
        "\n",
        "> Plot the sample averages $\\bar{y}_1, \\dots, \\bar{y}_8$ against the posterior expectations of $\\theta_1, \\dots, \\theta_8$, and describe the relationship. Also compute the sample mean of all observations and compare it to the posterior mean of $\\mu$.\n"
      ],
      "id": "344291b1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "psmp = scatter(ȳs, mean(θs, dims = 1)[1,:], xlabel=\"Sample Average\", ylabel= \"Posterior Expectation\")\n",
        "\n",
        "hline!(psmp, [mean(μs)], label=\"posterior mean (μn)\")\n",
        "hline!(psmp, [sum(ȳs .* ns)/n], label=\"Pooled sample mean (μ)\" )"
      ],
      "id": "95cb0673",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.8",
      "language": "julia",
      "display_name": "Julia 1.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}