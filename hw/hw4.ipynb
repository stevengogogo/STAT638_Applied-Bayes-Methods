{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Homework 4\n",
        "author:\n",
        "  - name: 'Shao-Ting Chiu (UIN:433002162)'\n",
        "    url: stchiu@email.tamu.edu\n",
        "    affiliation: 'Department of Electrical and Computer Engineering, Texas A\\&M University'\n",
        "date: today\n",
        "format:\n",
        "  pdf:\n",
        "    table-of-contents: true\n",
        "    code-line-numbers: true\n",
        "  html:\n",
        "    table-of-contents: true\n",
        "bibliography: ../ref.bib\n",
        "execute:\n",
        "  echo: true\n",
        "  freeze: auto\n",
        "---"
      ],
      "id": "83ca2e37"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Description\n",
        "\n",
        "- Course: STAT638, 2022 Fall\n",
        "\n",
        "> Read Chapter 4 in Hoff.\n",
        "> \n",
        "> Then, do the following exercises in Hoff: 4.1, 4.2, 4.6, 4.8\n",
        "> \n",
        "> All datasets in the Hoff book can be downloaded from https://pdhoff.github.io/book/ (Links to an external site.).\n",
        "\n",
        "- Deadline: `Oct 4 by 12:01pm`"
      ],
      "id": "8ca1a614"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Computational Enviromnent Setup\n",
        "\n",
        "### Third-party libraries"
      ],
      "id": "8678b928"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "import sys # system information\n",
        "import matplotlib # plotting\n",
        "import scipy # scientific computing\n",
        "import random \n",
        "import pandas as pd # data managing\n",
        "from scipy.special import comb\n",
        "from scipy import stats as st\n",
        "from scipy.special import gamma\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Matplotlib setting\n",
        "plt.rcParams['text.usetex'] = True\n",
        "matplotlib.rcParams['figure.dpi']= 300\n",
        "np.random.seed(20220928) # Consistent random effect"
      ],
      "id": "6d2445ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version"
      ],
      "id": "8c8cd3f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(sys.version)\n",
        "print(matplotlib.__version__)\n",
        "print(scipy.__version__)\n",
        "print(np.__version__)\n",
        "print(pd.__version__)"
      ],
      "id": "3cb794da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ],
      "id": "da9b0833"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 4.1\n",
        "\n",
        "> Posterior comparisons: Reconsider the sample survey in Exercise 3.1. Suppose you are interested in comparing the rate of support in that county to the rate in another county. Suppose that a survey of sample size $50$ was done in the second county, and the total number of people in the sample who supported the policy was $30$. Identify the posterior distribution of $\\theta_2$ assuming a uniform prior. Sample $5000$ values of each of $\\theta_1$ and $\\theta_2$ from their posterior distributions and estimate $Pr(\\theta_1 < \\theta_2|\\text{ the data and prior })$.\n",
        "\n",
        "- Prior: \n",
        "  - $\\theta \\sim beta(1,1)$\n",
        "- Model: \n",
        "  - $p(\\sum Y =n | \\theta) = {N \\choose n}\\theta^{n}(1-\\theta)^{N-n}$\n",
        "- Posterior distribution [@hoff2009first, pp. 37]: \n",
        "  - $\\theta | \\sum_{i=1}^{N} Y_i=n \\sim Beta(beta(1+n, 1+N-n))$\n",
        "    - $\\theta_1 \\sim Beta(1+57, 1+100-57) = Beta(58, 44)$\n",
        "    - $\\theta_2 \\sim Beta(1+30, 1+50-30) = Beta(31, 21)$  \n"
      ],
      "id": "19ed0732"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N = 5000\n",
        "t1s = st.beta.rvs(58, 44, size=N)\n",
        "t2s = st.beta.rvs(31, 21, size=N)\n",
        "p_t2bigger = np.mean(t1s < t2s) \n",
        "\n",
        "# Display\n",
        "pd.DataFrame({\"Item\": [\"Pr(theta_1 < theta_2 | the data and prior)\"],\\\n",
        "              \"Value\":[p_t2bigger]})"
      ],
      "id": "76458a63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 4.2\n",
        "\n",
        "> Tumor count comparisons: Reconsider the tumor count data in [Exercise 3.3](hw2.pdf#sec-p-3-3):\n",
        "\n",
        "### (a)\n",
        "\n",
        "> For the prior distribution given in part (a) of that exercise, obtain $Pr(\\theta_B < \\theta_A | y_A, y_B)$ via Monte Carlo sampling.\n",
        "\n",
        "- Prior distribution\n",
        "  - $\\theta_A|y_A \\sim gamma(120+117, 10+10) = gamma(237, 20)$\n",
        "  - $\\theta_B|y_B \\sim gamma(12+113, 1+13) = gamma(125, 14)$\n"
      ],
      "id": "e79acacd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N = 5000\n",
        "theta_A = st.gamma.rvs(237, scale=1/20, size=N)\n",
        "theta_B = st.gamma.rvs(125, scale=1/14, size=N)\n",
        "res = np.mean(theta_B < theta_A)\n",
        "\n",
        "# Display\n",
        "pd.DataFrame({\"Item\": [\"Pr(theta_B < theta_A | y_A, y_B)\"],\\\n",
        "              \"Value\":[res]})"
      ],
      "id": "5ba7d57c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (b)\n",
        "\n",
        "> For a range of values of $n_0$, obtain $Pr(\\theta_B < \\theta_A |y_A, y_B)$ for $\\theta_A \\sim gamma(120,10)$ and $\\theta_B \\sim gamma(12\\times n_0, n_0)$. Describe how sensitive the conclusions about the event $\\{\\theta_B < \\theta_A\\}$ are to the prior distribution on $\\theta_B$.\n",
        "\n",
        "In @fig-42-n0, the $n_0$ decreases the probability of $p(\\{\\theta_B < \\theta_A\\})$ with linear effect. From small to large $n_0$, the prior distribution of $\\theta_B$ keep influences the result.\n"
      ],
      "id": "cd7bb963"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-42-n0\n",
        "#| fig-cap: The effect of n0\n",
        "\n",
        "def get_A_bigger(n0, N=5000):\n",
        "  theta_A = st.gamma.rvs(120 + 117, scale=1/(10+10), size=N)\n",
        "  theta_B = st.gamma.rvs(12*n0+113, scale=1/(n0+13), size=N)\n",
        "  res = np.mean(theta_B < theta_A)\n",
        "  return res\n",
        "\n",
        "n0s = np.arange(1,50, 1)\n",
        "ress = [get_A_bigger(n0) for n0 in n0s]\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(n0s, ress, \"o\", color=\"k\")\n",
        "ax.set_xlabel(\"$n_0$\")\n",
        "ax.set_ylabel(\"$p(\\\\theta_B < \\\\theta_A | y_A, y_B, n_0)$\");"
      ],
      "id": "fig-42-n0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (c)\n",
        "\n",
        "> Repeat part (a) and (b), replacing the event $\\{\\theta_B < \\theta_A\\}$ with the event $\\{\\tilde{Y}_{B} < \\tilde{Y}_{A}\\}$, where $\\tilde{Y}_A$ and $\\tilde{Y}_B$ are samples from the posterior predictive distribution.\n",
        "\n",
        "**Part I (a)**\n",
        "\n",
        "- $\\tilde{Y}_A \\sim nbinom(a+\\sum Y^{(A)}, b + n) = nbinom(120+117,10+10)= nbinom(237, 20)$\n",
        "- $\\tilde{Y}_B \\sim nbinom(a+\\sum Y^{(B)}, b + n) = nbinom(12+113, 1+13) = nbinom(125, 14)$\n",
        "\n",
        "- Find $p(\\tilde{Y}_{B} < \\tilde{Y}_A | y_A, y_B)$\n",
        "\n",
        "- Use `scipy.stats.nbinom`[^nbinom]\n",
        "  - $p = 1 - \\frac{1}{b+n+1}$\n",
        "  - $n = a+\\sum Y$\n",
        "  - $k = \\tilde{Y}$\n"
      ],
      "id": "3c7fc971"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N = 5000\n",
        "tilde_y_A = st.nbinom.rvs(120+117, 1 - 1/(10+10+1), size=N)\n",
        "tilde_y_B = st.nbinom.rvs(12+113, 1 - 1/(1+13+1), size=N)\n",
        "p2 = np.mean(tilde_y_B < tilde_y_A)\n",
        "\n",
        "\n",
        "# Display\n",
        "# Display\n",
        "pd.DataFrame({\"Item\": [\"Pr(tildeY_B < tildeY_A | y_A, y_B)\"],\\\n",
        "              \"Value\":[p2]})"
      ],
      "id": "d67acad1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Part II (b)**\n",
        "\n",
        "In @fig-423, the $n_0$ has nonlinear negative effect on the probability $p(\\{\\tilde{Y}_{B} < \\tilde{Y}_{A}\\})$ with the prior formation of $n_0$.\n"
      ],
      "id": "7a009409"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-423\n",
        "#| fig-cap: The effect of n0\n",
        "\n",
        "def get_A_bigger(n0, N=5000):\n",
        "  tilde_y_A = st.nbinom.rvs(120+117, 1 - 1/(10+10+1), size=N)\n",
        "  tilde_y_B = st.nbinom.rvs(12*n0+113, 1 - 1/(n0+13+1), size=N)\n",
        "  res = np.mean(tilde_y_B < tilde_y_A)\n",
        "  return res\n",
        "\n",
        "n0s = np.arange(1,50, 1)\n",
        "ress = [get_A_bigger(n0) for n0 in n0s]\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(n0s, ress, \"o\", color=\"k\")\n",
        "ax.set_xlabel(\"$n_0$\")\n",
        "ax.set_ylabel(\"$p(\\\\tilde{Y}_B < \\\\tilde{Y}_A | y_A, y_B, n_0)$\");"
      ],
      "id": "fig-423",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[^nbinom]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.nbinom.html#scipy.stats.nbinom\n",
        "\n",
        "## Problem 4.6\n",
        "\n",
        "> Non-informative prior distributions: Suppose for a binary sampling problem we plan on using a uniform, or $beta(1,1)$, prior for the population proportion $\\theta$. Perhaps our reasoning is that this represents “no prior information about $\\theta$.” However, some people like to look at proportions on the log-odds scale, that is, they are interested in $\\gamma = \\log\\frac{\\theta}{1−\\theta}$. Via Monte Carlo sampling or otherwise, find the prior distribution for $\\gamma$ that is induced by the uniform prior for $\\theta$. Is the prior informative about $\\gamma$?\n",
        "\n",
        "\n",
        "**Part I: Analytical Approach**\n",
        "\n",
        "- $\\gamma = g(\\theta) = \\log\\frac{\\theta}{1-\\theta}$\n",
        "- $\\theta = g^{-1}(\\gamma) = \\frac{e^{\\gamma}}{1+e^{\\gamma}}$\n",
        "\n",
        "\\begin{align}\n",
        "p_{\\gamma}(\\gamma) \n",
        "&= \\underbrace{p_{\\theta}(g^{-1}(\\gamma))}_{=1 \\because \\theta\\sim uniform(0,1)} \\times \\left|\\frac{dg^{-1}(\\gamma)}{d\\gamma}\\right|\\\\ \n",
        "&= \\left|\\frac{e^{\\gamma}}{(1+e^{\\gamma})^2}\\right|\n",
        "\\end{align}\n",
        "\n",
        "**Part II: Monte Carlo Approach**\n"
      ],
      "id": "69d1cb28"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def true_gamma_pdf(y):\n",
        "    return np.absolute(np.exp(y)/(1+np.exp(y))**2)\n",
        "\n",
        "ths = st.beta.rvs(1,1,size=1000)\n",
        "gammas = np.linspace(-10,10,100)\n",
        "\n",
        "ys_true = [true_gamma_pdf(g) for g in gammas]\n",
        "ys = [np.log(th/(1-th)) for th in ths]\n",
        "\n",
        "fig, ax = plt.subplots();\n",
        "ax.hist(ys, weights=np.ones_like(ys)/len(ys));\n",
        "ax.plot(gammas, ys_true, label=\"Analytical solution\");\n",
        "ax.set_xlabel(\"$\\\\gamma=\\\\log\\\\frac{\\\\theta}{1-\\\\theta}$\")\n",
        "ax.set_ylabel(\"$p(\\\\gamma)$\")\n",
        "ax.legend();"
      ],
      "id": "4bffa0c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The prior of $\\gamma$ is informative since it is centered around the $0$.\n",
        "\n",
        "## Problem 4.8 {#sec-p48}\n",
        "\n",
        "> More posterior predictive checks: Let $\\theta_A$ and $\\theta_B$ be the average number of children of men in their 30s with and without bachelor's degrees, respectively.\n",
        "\n",
        "### (a)\n",
        "\n",
        "> Using a Poisson sampling model, a $gamma(2,1)$ prior for each $\\theta$ and the data in the files [`menchild30bach.dat`](data/menchild30bach.dat.txt) and [`menchild30nobach.dat`](data/menchild30nobach.dat.txt), obtain $5000$ samples of $\\bar{Y}_{A}$ and $\\bar{Y}_{B}$ from the posterior predictive distribution of the two samples. Plot the Monte Carlo approximations to these two posterior predictive distributions. (data available in [Appendix](#sec-data))\n",
        "\n",
        "``` \n",
        "# Data\n",
        "dataA = np.loadtxt(\"data/menchild30bach.dat\")\n",
        "dataB = np.loadtxt(\"data/menchild30nobach.dat\")\n",
        "\n",
        "# Display \n",
        "pd.DataFrame({\n",
        "  \"Properties\": [\"Sum\", \"N (number of samples)\"],\n",
        "  \"A\": [np.sum(dataA), len(dataA)],\n",
        "  \"B\": [np.sum(dataB), len(dataB)]\n",
        "})\n",
        "```\n",
        "\n",
        "The predictive distribution is\n",
        "\n",
        "- $\\bar{Y_A} | y_A \\sim nbinom(2+54,1+58)$\n",
        "- $\\bar{Y_B} | y_B \\sim nbinom(2+305, 1+218)$\n"
      ],
      "id": "670c89a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N = 5000\n",
        "bar_ya = st.nbinom.rvs(2+ np.sum(dataA), 1 - 1/(1+len(dataA)+1), size=N)\n",
        "bar_yb = st.nbinom.rvs(2+ np.sum(dataB), 1 - 1/(1+len(dataB)+1), size=N)\n",
        "\n",
        "# Display\n",
        "fig, ax = plt.subplots();\n",
        "[ax.hist(ys, weights=np.ones_like(ys)/len(ys), label=n, alpha=0.7, bins=np.arange(0,8,1)) for n,ys in zip([\"A\",\"B\"],[bar_ya, bar_yb])]\n",
        "ax.set_xlabel(\"$N$\")\n",
        "ax.set_ylabel(\"Probability\")\n",
        "ax.legend();"
      ],
      "id": "4c26be29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (b)\n",
        "\n",
        "> Find $95\\%$ quantile-based posterior confidence intervals for $\\theta_B - \\theta_A$ and $\\tilde{Y}_B-\\tilde{Y}_A$. Describe in words the differences between the two populations using these quantities and the plots in (a), along with any other results that may of interest to you.\n",
        "\n",
        "- $\\theta_A | y_A \\sim gamma(2+54, 1+58)$\n",
        "- $\\theta_B | y_B \\sim gamma(2+58, 1+218)$\n",
        "- $\\bar{Y_A} | y_A \\sim nbinom(2+54,1+58)$\n",
        "- $\\bar{Y_B} | y_B \\sim nbinom(2+305, 1+218)$\n",
        "\n",
        "- The $95\\%$ confidence interval does not contain the negative region. Thus, the belief of $\\theta_B > \\theta_A$ is confident.\n",
        "- However, there is more uncertainty about the posterior predictive distribution, leading to the uncertain quantity comparison between $Y_A$ and $Y_B$.\n",
        "\n",
        "```\n",
        "N = 5000\n",
        "thetaAs = st.gamma.rvs(2+np.sum(dataA), scale=1/(1+len(dataA)), size=N)\n",
        "thetaBs = st.gamma.rvs(2+np.sum(dataB), scale=1/(1+len(dataB)), size=N)\n",
        "YAs = st.nbinom.rvs(2+np.sum(dataA), 1 - 1/(1+len(dataA)+1), size=N)\n",
        "YBs = st.nbinom.rvs(2+np.sum(dataB), 1 - 1/(1+len(dataB)+1), size=N)\n",
        "\n",
        "theta_diff = thetaBs - thetaAs\n",
        "Y_diff = YBs - YAs\n",
        "\n",
        "theta_quan = st.mstats.mquantiles(theta_diff, prob=[0.025, 0.975])\n",
        "Y_quan = st.mstats.mquantiles(Y_diff, prob=[0.025, 0.975])\n",
        "\n",
        "# Display\n",
        "pd.DataFrame({\"RVs\":[\"Interval (thetaB-thetaA)\", \"Interval (YB-YA)\"],\\\n",
        "              \"Value (2.5%; 97.5%)\":[theta_quan, Y_quan]})\n",
        "```\n"
      ],
      "id": "72604983"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.hist(theta_diff, weights=np.ones_like(theta_diff)/len(theta_diff), alpha=0.7)\n",
        "plt.title(\"Distribution of ThetaB - ThetaA\")\n",
        "plt.show();"
      ],
      "id": "f501f686",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.hist(Y_diff, weights=np.ones_like(Y_diff)/len(Y_diff), alpha=0.7)\n",
        "plt.title(\"Distribution of YB - YA\")\n",
        "plt.show();"
      ],
      "id": "d74b7545",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (c)\n",
        "\n",
        "> Obtain the empirical distribution of the data in group $B$. Compare this to the Poisson distribution with mean $\\hat{\\theta}=1.4$. Do you think the Poisson model is a good fit? Why or Why not?\n",
        "\n",
        "\n",
        "- $\\bar{Y}_B | y_B \\sim nbinom(307, 219)$\n",
        "- The poisson model is not a good fit to the group $B$. As shown in the historgram, there are more than one peak in the distribution, that is not the characteristic of Poisson distribution.\n"
      ],
      "id": "d8abd259"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xs = np.arange(0,7,1)\n",
        "ps = [st.poisson.pmf(x, 1.4) for x in xs]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(dataB, weights=np.ones_like(dataB)/len(dataB), alpha=0.7);\n",
        "ax.plot(xs, ps, \"o\", label=\"Poisson($\\\\hat{\\\\theta}$=1.4)\")\n",
        "ax.legend();"
      ],
      "id": "74a0d812",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (d)\n",
        "\n",
        "> For each of the $5000$ $\\theta_B$-valeus you sampled, sample $n_B=218$ Poisson random variables and count the number of $0$s and the number of $1$s in each of the $5000$ simulated datasets. You should now have tow sequences of length $5000$ each, one sequence counting the number of people having zero children for each of the $5000$ posterior predictive datasets, the other counting the number of people with one child. Plot the two sequences against one another (one on the $x$-axis, one on the $y$-axis). Add to the plot a point marking how many people in the observed dataset had zero children and one child. Using this plot, describe the adequency of the Poisson model.\n"
      ],
      "id": "daec8a95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "thetaBs = thetaBs\n",
        "nB = 218\n",
        "ybs0 = np.zeros(len(thetaBs))\n",
        "ybs1 = np.zeros(len(thetaBs))\n",
        "\n",
        "for (i, th) in enumerate(thetaBs):\n",
        "  seq = st.poisson.rvs(th, size=nB)\n",
        "  ybs0[i] = len(seq[seq==0])\n",
        "  ybs1[i] = len(seq[seq==1])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(ybs0, ybs1, \"o\", alpha=0.5)\n",
        "ax.plot(len(dataB[dataB==0]), len(dataB[dataB==1]), 'o',color=\"r\")\n",
        "ax.set_xlabel(\"N of people with zero children\")\n",
        "ax.set_ylabel(\"N of people with one children\")"
      ],
      "id": "1d675fe3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The observed data is in red, but the simulated data is centralized far from that point. Thus, the poisson model is not appropriate for this dataset.\n",
        "\n",
        "## Appendix\n",
        "\n",
        "### Data set in [Problem 4.8](#sec-p48) {#sec-data}\n"
      ],
      "id": "dd59234d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"menchild30bach.dat:\\n\", dataA)\n",
        "print(\"menchild30nobach.dat:\\n\", dataB)"
      ],
      "id": "e03a4960",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-hidden when-format=\"html\"}\n",
        "\n",
        "## References\n",
        "\n",
        ":::"
      ],
      "id": "555b402c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}