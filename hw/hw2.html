<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.242">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Shao-Ting Chiu (UIN:433002162)">
<meta name="dcterms.date" content="2023-01-20">

<title>STAT638: Applied Bayesian Methods - 6&nbsp; Homework 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../hw/hw3.html" rel="next">
<link href="../hw/hw1.html" rel="prev">
<link href="../img/favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Homework 2</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">STAT638: Applied Bayesian Methods</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/stevengogogo/STAT638_Applied-Bayes-Methods" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle sidebar-tool" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true"><strong>Lecture Notes</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ch1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Chapter 1 Introduction and examples</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ch2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2: Conditional distributions and Bayes rule</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ch3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: One-parameter models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ch4.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4: Monte Carlo Approximation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true"><strong>Assignments</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Homework 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw2.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Homework 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Homework 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw4.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Homework 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw5.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Homework 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw6.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Homework 6</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw7.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Homework 7</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw8.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Homework 8</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../hw/hw9.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Homework 9</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ref.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#homework-description" id="toc-homework-description" class="nav-link active" data-scroll-target="#homework-description"><span class="toc-section-number">6.1</span>  Homework Description</a></li>
  <li><a href="#computational-enviromnent-setup" id="toc-computational-enviromnent-setup" class="nav-link" data-scroll-target="#computational-enviromnent-setup"><span class="toc-section-number">6.2</span>  Computational Enviromnent Setup</a>
  <ul class="collapse">
  <li><a href="#third-party-libraries" id="toc-third-party-libraries" class="nav-link" data-scroll-target="#third-party-libraries"><span class="toc-section-number">6.2.1</span>  Third-party libraries</a></li>
  <li><a href="#version" id="toc-version" class="nav-link" data-scroll-target="#version"><span class="toc-section-number">6.2.2</span>  Version</a></li>
  </ul></li>
  <li><a href="#problem-3.1" id="toc-problem-3.1" class="nav-link" data-scroll-target="#problem-3.1"><span class="toc-section-number">6.3</span>  Problem 3.1</a>
  <ul class="collapse">
  <li><a href="#a" id="toc-a" class="nav-link" data-scroll-target="#a"><span class="toc-section-number">6.3.1</span>  (a)</a></li>
  <li><a href="#b" id="toc-b" class="nav-link" data-scroll-target="#b"><span class="toc-section-number">6.3.2</span>  (b)</a></li>
  <li><a href="#sec-3-1-c" id="toc-sec-3-1-c" class="nav-link" data-scroll-target="#sec-3-1-c"><span class="toc-section-number">6.3.3</span>  (c)</a></li>
  <li><a href="#d" id="toc-d" class="nav-link" data-scroll-target="#d"><span class="toc-section-number">6.3.4</span>  (d)</a></li>
  <li><a href="#e" id="toc-e" class="nav-link" data-scroll-target="#e"><span class="toc-section-number">6.3.5</span>  (e)</a></li>
  </ul></li>
  <li><a href="#sec-p-3-3" id="toc-sec-p-3-3" class="nav-link" data-scroll-target="#sec-p-3-3"><span class="toc-section-number">6.4</span>  Problem 3.3</a>
  <ul class="collapse">
  <li><a href="#a-1" id="toc-a-1" class="nav-link" data-scroll-target="#a-1"><span class="toc-section-number">6.4.1</span>  (a)</a></li>
  <li><a href="#b-1" id="toc-b-1" class="nav-link" data-scroll-target="#b-1"><span class="toc-section-number">6.4.2</span>  (b)</a></li>
  <li><a href="#c" id="toc-c" class="nav-link" data-scroll-target="#c"><span class="toc-section-number">6.4.3</span>  (c)</a></li>
  </ul></li>
  <li><a href="#problem-3.4" id="toc-problem-3.4" class="nav-link" data-scroll-target="#problem-3.4"><span class="toc-section-number">6.5</span>  Problem 3.4</a>
  <ul class="collapse">
  <li><a href="#sec-3-4-a" id="toc-sec-3-4-a" class="nav-link" data-scroll-target="#sec-3-4-a"><span class="toc-section-number">6.5.1</span>  (a)</a></li>
  <li><a href="#sec-3-4-b" id="toc-sec-3-4-b" class="nav-link" data-scroll-target="#sec-3-4-b"><span class="toc-section-number">6.5.2</span>  (b)</a></li>
  <li><a href="#sec-3-4-c" id="toc-sec-3-4-c" class="nav-link" data-scroll-target="#sec-3-4-c"><span class="toc-section-number">6.5.3</span>  (c)</a></li>
  <li><a href="#sec-3-4-d" id="toc-sec-3-4-d" class="nav-link" data-scroll-target="#sec-3-4-d"><span class="toc-section-number">6.5.4</span>  (d)</a></li>
  <li><a href="#e-1" id="toc-e-1" class="nav-link" data-scroll-target="#e-1"><span class="toc-section-number">6.5.5</span>  (e)</a></li>
  </ul></li>
  <li><a href="#problem-3.7" id="toc-problem-3.7" class="nav-link" data-scroll-target="#problem-3.7"><span class="toc-section-number">6.6</span>  Problem 3.7</a>
  <ul class="collapse">
  <li><a href="#a-2" id="toc-a-2" class="nav-link" data-scroll-target="#a-2"><span class="toc-section-number">6.6.1</span>  (a)</a></li>
  <li><a href="#b-2" id="toc-b-2" class="nav-link" data-scroll-target="#b-2"><span class="toc-section-number">6.6.2</span>  (b)</a></li>
  <li><a href="#sec-3-7-c" id="toc-sec-3-7-c" class="nav-link" data-scroll-target="#sec-3-7-c"><span class="toc-section-number">6.6.3</span>  (c)</a></li>
  <li><a href="#d-1" id="toc-d-1" class="nav-link" data-scroll-target="#d-1"><span class="toc-section-number">6.6.4</span>  (d)</a></li>
  </ul></li>
  <li><a href="#problem-3.12" id="toc-problem-3.12" class="nav-link" data-scroll-target="#problem-3.12"><span class="toc-section-number">6.7</span>  Problem 3.12</a>
  <ul class="collapse">
  <li><a href="#sec-3-12-a" id="toc-sec-3-12-a" class="nav-link" data-scroll-target="#sec-3-12-a"><span class="toc-section-number">6.7.1</span>  (a)</a></li>
  <li><a href="#sec-3-12-b" id="toc-sec-3-12-b" class="nav-link" data-scroll-target="#sec-3-12-b"><span class="toc-section-number">6.7.2</span>  (b)</a></li>
  <li><a href="#c-1" id="toc-c-1" class="nav-link" data-scroll-target="#c-1"><span class="toc-section-number">6.7.3</span>  (c)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Homework 2</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading"></div>
  
    <div class="quarto-title-meta-contents">
    <a href="stchiu@email.tamu.edu">Shao-Ting Chiu (UIN:433002162)</a> 
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Department of Electrical and Computer Engineering, Texas A&amp;M University
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 20, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="homework-description" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="homework-description"><span class="header-section-number">6.1</span> Homework Description</h2>
<blockquote class="blockquote">
<p>Read Chapter 3 in the Hoff book.</p>
<p>Then, do the following exercises in Hoff: 3.1, 3.3, 3.4, 3.7, 3.12.</p>
<p>For problems that require a computer, please do and derive as much as possible “on paper,” and include these derivations in your submission file. Then, for the parts that do require the use of a computer (e.g., creating plots), you are free to use any software (R, Python, …) of your choosing; no need to include your code in your write-up. Please make sure you create a single file for submission here on Canvas.</p>
<p>For computations involving gamma functions (e.g., 3.7), it is often helpful to work with log-gamma functions instead, to avoid numbers that are too large to be represented by a computer. In R, the functions lbeta() and lgamma() compute the (natural) log of the beta and gamma functions, respectively. See more here: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Special.html</p>
</blockquote>
<ul>
<li><a href="https://github.com/stevengogogo/STAT638_Applied-Bayes-Methods/blob/hw/hw2.pdf">PDF version</a></li>
<li>Deadline: <code>Sep 20 by 12:01pm</code></li>
</ul>
<hr>
</section>
<section id="computational-enviromnent-setup" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="computational-enviromnent-setup"><span class="header-section-number">6.2</span> Computational Enviromnent Setup</h2>
<section id="third-party-libraries" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="third-party-libraries"><span class="header-section-number">6.2.1</span> Third-party libraries</h3>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys <span class="co"># system information</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="co"># plotting</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="co"># scientific computing</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd <span class="co"># data managing</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> comb</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats <span class="im">as</span> st</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> gamma</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> comb</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Matplotlib setting</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'text.usetex'</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>matplotlib.rcParams[<span class="st">'figure.dpi'</span>]<span class="op">=</span> <span class="dv">300</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="version" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="version"><span class="header-section-number">6.2.2</span> Version</h3>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sys.version)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(matplotlib.__version__)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(scipy.__version__)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.__version__)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pd.__version__)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3.9.12 (main, Apr  5 2022, 01:52:34) 
[Clang 12.0.0 ]
3.6.2
1.9.3
1.23.4
1.5.1</code></pre>
</div>
</div>
<hr>
</section>
</section>
<section id="problem-3.1" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="problem-3.1"><span class="header-section-number">6.3</span> Problem 3.1</h2>
<blockquote class="blockquote">
<p>Sample survey: Suppose we are going to sample 100 individuals from a county (of size much larger than 100) and ask each sampled person whether they support policy <span class="math inline">\(Z\)</span> or not. Let <span class="math inline">\(Y_i = 1\)</span> if person <span class="math inline">\(i\)</span> in the sample supports the policy, and <span class="math inline">\(Y_i = 0\)</span> otherwise.</p>
</blockquote>
<section id="a" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="a"><span class="header-section-number">6.3.1</span> (a)</h3>
<blockquote class="blockquote">
<p>Assume <span class="math inline">\(Y_1,\dots, Y_{100}\)</span> are, conditional on <span class="math inline">\(\theta\)</span>, i.i.d. binary random variables with expectation <span class="math inline">\(\theta\)</span>. Write down the joint distribution of <span class="math inline">\(Pr(Y_1 =y1,\dots, Y_{100} = y_{100}|\theta)\)</span> in a compact form. Also write down the form of <span class="math inline">\(Pr(\sum Y_i = y|\theta)\)</span>.</p>
</blockquote>
<p><span class="math display">\[Pr(Y_1 = y_1,\dots, Y_{100}=y_{100}|\theta) = \underline{\theta^{\sum_{u=1}^{100}}(1-\theta)^{100-\sum_{u=1}^{100}}}\]</span></p>
<p><span id="eq-sum-prob"><span class="math display">\[Pr(\sum_{i=1}^{100} Y_i = y |\theta)= \underline{{100 \choose y}\theta^{y}(1-\theta)^{100-y}} \tag{6.1}\]</span></span></p>
</section>
<section id="b" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="b"><span class="header-section-number">6.3.2</span> (b)</h3>
<blockquote class="blockquote">
<p>For the moment, suppose you believed that <span class="math inline">\(\theta\in\{0.0,0.1,\dots,0.9,1.0\}\)</span>. Given that the results of the survey were <span class="math inline">\(\sum^{100}_{i=1} Y_i = 57\)</span>, compute <span class="math inline">\(Pr(\sum Y_{i} = 57|\theta)\)</span> for each of these 11 values of <span class="math inline">\(\theta\)</span> and plot these probabilities as a function of <span class="math inline">\(\theta\)</span></p>
</blockquote>
<p>From <a href="#eq-sum-prob">Equation&nbsp;<span>6.1</span></a>, the sum of supports (<span class="math inline">\(y\)</span>) is on the power term. Thus, directly computation is problematic with limited range of floating number. Converting probability to log scale is a way to bypass this problem.Another way is to use <code>scipy.stats.binom</code> function<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>The distribution of <span class="math inline">\(Pr(\sum_{i=1}^{100} Y_i = y |\theta)\)</span> along with <span class="math inline">\(\theta\in\{0.0,0.1,\dots,0.9,1.0\}\)</span> is shown in <a href="#tbl-binom">Table&nbsp;<span>6.1</span></a>. The plot of distribution is shown in <a href="#fig-binom">Figure&nbsp;<span>6.1</span></a>.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="fl">0.0</span>,<span class="fl">1.0</span>,<span class="dv">11</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>tot <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> np.zeros(<span class="bu">len</span>(thetas))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> <span class="dv">57</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i, theta) <span class="kw">in</span> <span class="bu">enumerate</span>(thetas):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  probs[i] <span class="op">=</span> st.binom.pmf(count, tot, theta)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># list of probabilities</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Theta"</span>: thetas, <span class="st">"posteriori"</span>: probs})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div id="tbl-binom" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;6.1:  Probabilities along with priors </caption>
  <thead>
    <tr>
      <th></th>
      <th>Theta</th>
      <th>posteriori</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.000000e+00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.1</td>
      <td>4.107157e-31</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.2</td>
      <td>3.738459e-16</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.3</td>
      <td>1.306895e-08</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.4</td>
      <td>2.285792e-04</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.5</td>
      <td>3.006864e-02</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.6</td>
      <td>6.672895e-02</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.7</td>
      <td>1.853172e-03</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.8</td>
      <td>1.003535e-07</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.9</td>
      <td>9.395858e-18</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1.0</td>
      <td>0.000000e+00</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, probs, <span class="st">'ko'</span>)<span class="op">;</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)<span class="op">;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$Pr(\sum Y_i = 57 |\theta)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-binom" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-binom-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.1: Probabilities along with priors</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-3-1-c" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="sec-3-1-c"><span class="header-section-number">6.3.3</span> (c)</h3>
<blockquote class="blockquote">
<p>Now suppose you originally had no prior information to believe one of these <span class="math inline">\(\theta\)</span>-values over another, and so <span class="math inline">\(Pr(\theta=0.0)=Pr(\theta=0.1)=\dots=Pr(\theta=0.9)=Pr(\theta=1.0)\)</span>. Use Bayes’ rule to compute <span class="math inline">\(p(\theta|\sum^{n}_{i=1} Y_i = 57)\)</span> for each <span class="math inline">\(\theta\)</span>-value. Make a plot of this posterior distribtution as a function of <span class="math inline">\(\theta\)</span>.</p>
</blockquote>
<p><span class="math display">\[\begin{align}
  p(\theta_i |\sum^{n}_{i=1} Y_i = 57) &amp;= \frac{p(\sum^{n}_{i=1} Y_i = 57|\theta)p(\theta_i)}{p(\sum^{n}_{i=1} Y_i = 57)}\\
  &amp;= \frac{p(\sum^{n}_{i=1} Y_i = 57|\theta)p(\theta_i)}{\sum_{\theta\in\Theta}p(\sum^{n}_{i=1} Y_i = 57 | \theta)p(\theta)}
\end{align}\]</span></p>
<p>The following is the calculation of the posterior distribution (shown in <a href="#tbl-post-binom">Table&nbsp;<span>6.2</span></a>), and the result is shown in <a href="#fig-post-binom">Figure&nbsp;<span>6.2</span></a>.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>p_theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span><span class="bu">len</span>(thetas)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>p_y <span class="op">=</span> np.<span class="bu">sum</span>( probs<span class="op">*</span>p_theta)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>post_theta <span class="op">=</span> np.zeros(<span class="bu">len</span>(thetas))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i, theta) <span class="kw">in</span> <span class="bu">enumerate</span>(thetas):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  post_theta[i] <span class="op">=</span> probs[i]<span class="op">*</span>p_theta<span class="op">/</span>p_y</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># list of probabilities</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Theta"</span>: thetas, <span class="st">"posteriori"</span>:post_theta})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div id="tbl-post-binom" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;6.2:  Posterior distribution depends on discrete uniform distribution of theta. </caption>
  <thead>
    <tr>
      <th></th>
      <th>Theta</th>
      <th>posteriori</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.000000e+00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.1</td>
      <td>4.153701e-30</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.2</td>
      <td>3.780824e-15</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.3</td>
      <td>1.321705e-07</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.4</td>
      <td>2.311695e-03</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.5</td>
      <td>3.040939e-01</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.6</td>
      <td>6.748515e-01</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.7</td>
      <td>1.874172e-02</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.8</td>
      <td>1.014907e-06</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.9</td>
      <td>9.502335e-17</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1.0</td>
      <td>0.000000e+00</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, post_theta, <span class="st">'ko'</span>)<span class="op">;</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)<span class="op">;</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$p(\theta_i |\sum^</span><span class="sc">{n}</span><span class="vs">_{i=1} Y_i = 57)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-post-binom" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-post-binom-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.2: Posterior distribution as a function of theta.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="d" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="d"><span class="header-section-number">6.3.4</span> (d)</h3>
<blockquote class="blockquote">
<p>Now suppose you allow <span class="math inline">\(\theta\)</span> to be any value in the interval <span class="math inline">\([0,1]\)</span>. Using the uniform prior density for <span class="math inline">\(\theta\)</span>, so that <span class="math inline">\(p(\theta) = 1\)</span>, plot the posterior density <span class="math inline">\(p(\theta) \times Pr(\sum^{n}_{i=1} Y_i = 57 |\theta)\)</span> as a function of <span class="math inline">\(\theta\)</span>.</p>
</blockquote>
<p>As shown in <a href="#fig-post-binom-cont">Figure&nbsp;<span>6.3</span></a>.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>p_theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span><span class="bu">len</span>(thetas)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> np.zeros(<span class="bu">len</span>(thetas))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>post_theta <span class="op">=</span> np.zeros(<span class="bu">len</span>(thetas))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> <span class="dv">57</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i, theta) <span class="kw">in</span> <span class="bu">enumerate</span>(thetas):</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  probs[i] <span class="op">=</span> st.binom.pmf(count, tot, theta)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  post_theta[i] <span class="op">=</span> probs[i]</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting  </span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, post_theta, <span class="st">'k-'</span>)<span class="op">;</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)<span class="op">;</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$p(\theta_i |\sum^</span><span class="sc">{n}</span><span class="vs">_{i=1} Y_i = 57)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-post-binom-cont" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-post-binom-cont-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.3: Posterior distribution with continouous uniform prior.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="e" class="level3" data-number="6.3.5">
<h3 data-number="6.3.5" class="anchored" data-anchor-id="e"><span class="header-section-number">6.3.5</span> (e)</h3>
<blockquote class="blockquote">
<p>As discussed in this chapter, the posterior distribution of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(beta(1+57, 1+100-57)\)</span>. Plot the posterior density as a function of <span class="math inline">\(\theta\)</span>. Discuss the relationships among all of the plots you have made for this exercise.</p>
</blockquote>
<p>The <span class="math inline">\(\theta\)</span> with beta distribution is plotted in <a href="#fig-post-beta-cont">Figure&nbsp;<span>6.4</span></a>.</p>
<p><a href="#fig-post-binom">Figure&nbsp;<span>6.2</span></a> is the normalized probability via Bayes’ rule (<a href="#sec-3-1-c"><span>Section&nbsp;6.3.3</span></a>). On the other hand, <a href="#fig-binom">Figure&nbsp;<span>6.1</span></a> is not normalized.</p>
<p><a href="#fig-post-binom-cont">Figure&nbsp;<span>6.3</span></a> and <a href="#fig-post-beta-cont">Figure&nbsp;<span>6.4</span></a> has similar distribution, which means the prior <span class="math inline">\(\theta\)</span> has little influcence on the posterior distribution. This is because the sample number is large (<span class="math inline">\(n=57\)</span>), and decrease the importance of the prior.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dv">3000</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>thetas_rv <span class="op">=</span> st.beta(<span class="dv">1</span><span class="op">+</span><span class="dv">57</span>, <span class="dv">1</span><span class="op">+</span><span class="dv">100</span><span class="op">-</span><span class="dv">57</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> [thetas_rv.pdf(x) <span class="cf">for</span> x <span class="kw">in</span> grid]</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>p_theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span><span class="bu">len</span>(thetas)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> np.zeros(<span class="bu">len</span>(thetas))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>post_theta <span class="op">=</span> np.zeros(<span class="bu">len</span>(thetas))</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> <span class="dv">57</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i, theta) <span class="kw">in</span> <span class="bu">enumerate</span>(thetas):</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>  probs[i] <span class="op">=</span> st.binom.pmf(count, tot, theta)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>  post_theta[i] <span class="op">=</span> probs[i]</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting  </span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, post_theta, <span class="st">'k-'</span>)<span class="op">;</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta\sim beta$"</span>)<span class="op">;</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$p(\theta_i |\sum^</span><span class="sc">{n}</span><span class="vs">_{i=1} Y_i = 57)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-post-beta-cont" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-post-beta-cont-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.4: Posterior distribution with continouous beta prior.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="sec-p-3-3" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="sec-p-3-3"><span class="header-section-number">6.4</span> Problem 3.3</h2>
<blockquote class="blockquote">
<p>Tumor counts: A cancer laboratory is estimating the rate of tumorigenesis in two strains of mice, A and B. They have tumor count data for 10 mice in strain A and 13 mice in strain B. Type A mice have been well studied, and information from other laboratories suggests that type A mice have tumor counts that are approximately Poisson-distributed with a mean of 12. Tumor count rates for type B mice are unknown, but type B mice are related to type A mice. The observed tumor counts for the two populations are <span class="math display">\[\mathcal{y}_{A} = (12, 9, 12, 14, 13, 13, 15, 8, 15, 6);\]</span> <span class="math display">\[\mathcal{y}_{B} = (11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7).\]</span></p>
</blockquote>
<section id="a-1" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="a-1"><span class="header-section-number">6.4.1</span> (a)</h3>
<blockquote class="blockquote">
<p>Find the posterior distributions, means, variances and <span class="math inline">\(95\%\)</span> quantile-based confidence intervals for <span class="math inline">\(\theta_A\)</span> and <span class="math inline">\(\theta_B\)</span>, assuming a Poisson sampling distribution for each group and the following prior distribution: <span class="math inline">\(\theta_A \sim gamma(120,10), \theta_B \sim gamma(12,1), p(\theta_A, \theta_B) = p(\theta_A) \times p(\theta_B)\)</span></p>
</blockquote>
<p>According to <span class="citation" data-cites="hoff2009first">Hoff (<a href="../ref.html#ref-hoff2009first" role="doc-biblioref">2009, 580:46–47</a>)</span>,</p>
<p><span class="math display">\[E[\theta_{*} | y_1,\dots, y_{n_{*}}] = \frac{a_* + \sum_{i=1}^{n_*} y_{i}}{b_* + n_*}\]</span></p>
<p>where <span class="math inline">\(*\in \{A, B\}\)</span>. Given</p>
<p><span class="math inline">\(\begin{cases}  \theta_{*} &amp;\sim gamma(a_*,b_*)\\  Y_1,\dots, Y_{n_*}|\theta_{*} &amp;\sim Poisson(\theta_{*}) \end{cases}\)</span></p>
<p><span id="eq-gamma-conj"><span class="math display">\[\Rightarrow\{\theta_{i}|Y_1,\dots,Y_{n_*}\}\sim gamma(a + \sum^{n_*}_{i=1} Y_i, b_* + n_*) \tag{6.2}\]</span></span></p>
<p>The properties of Gamma distribution <span class="citation" data-cites="hoff2009first">(<a href="../ref.html#ref-hoff2009first" role="doc-biblioref">Hoff 2009, 580:45–46</a>)</span>,</p>
<p><span class="math display">\[p(\theta) = \frac{b^a}{\Gamma(a)}\theta^{a-1}e^{-b\theta}, \quad \theta,a,b &gt; 0\]</span></p>
<p><span id="eq-gamma-mean"><span class="math display">\[E[\theta] = \frac{a}{b} \tag{6.3}\]</span></span> <span id="eq-gamma-var"><span class="math display">\[Var[\theta] = \frac{a}{b^2} \tag{6.4}\]</span></span></p>
<p><strong>Type A Mice</strong></p>
<div id="tbl-type-a-mice" class="anchored">
<table class="table">
<caption>Table&nbsp;6.3: Parameters of type A mice</caption>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(a_A\)</span></td>
<td>120</td>
</tr>
<tr class="even">
<td><span class="math inline">\(b_A\)</span></td>
<td>10</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(n_A\)</span></td>
<td>10</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sum_{i=1}^{n_{A}y_{i}}\)</span></td>
<td><span class="math inline">\(12+9+12+14+13+13+15+8+15+6=117\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>The posterior distribution of mice A:</p>
<p><span class="math display">\[\{\theta_A|Y_1,\dots,Y_{n_A} \sim gamma(120 + 117, 10+10)= gamma(237,20)\} \]</span></p>
<ul>
<li><p><span class="math inline">\(E[\theta_{A}|\sum_{i=1}^{n_{A}} Y_{i}] = \frac{237}{20}= \underline{11.85}\)</span></p></li>
<li><p><span class="math inline">\(Var[\theta_{A}|\sum_{i=1}^{n_{A}} Y_{i}] = \frac{237}{20^2}\approx \underline{0.59}\)</span></p></li>
<li><p><span class="math inline">\(95\%\)</span> quantile-based confidence intervals is shown in <a href="#tbl-33a">Table&nbsp;<span>6.4</span></a></p></li>
</ul>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> interval_gamma_95(a,b):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  rvA <span class="op">=</span> st.gamma(a, scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span>b)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  ints <span class="op">=</span> rvA.interval(<span class="fl">0.95</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> pd.DataFrame({<span class="st">"Left bound"</span>:[ints[<span class="dv">0</span>]], <span class="st">"Right bound"</span>:[ints[<span class="dv">1</span>]]})</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>aA <span class="op">=</span> <span class="dv">237</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>bA <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>interval_gamma_95(aA,bA)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<div id="tbl-33a" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;6.4:  95% quantile-based confidence intervals of mice A. </caption>
  <thead>
    <tr>
      <th></th>
      <th>Left bound</th>
      <th>Right bound</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10.389238</td>
      <td>13.405448</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
<p><strong>Type B Mice</strong></p>
<p>similarly,</p>
<div id="tbl-type-b-mice" class="anchored">
<table class="table">
<caption>Table&nbsp;6.5: Parameters of type B mice</caption>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(a_B\)</span></td>
<td>12</td>
</tr>
<tr class="even">
<td><span class="math inline">\(b_B\)</span></td>
<td>1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(n_B\)</span></td>
<td>13</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sum_{i=1}^{n_{B}y_{i}}\)</span></td>
<td><span class="math inline">\(11+11+10+9+9+8+7+10+6+8+8+9+7=113\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>The posterior distribution of mice B:</p>
<p><span class="math display">\[\{\theta_B|Y_1,\dots,Y_{n_B} \sim gamma(12+113, 1+13)= gamma(125, 14)\} \]</span></p>
<ul>
<li><span class="math inline">\(E[\theta_{B}|\sum_{i=1}^{n_{B}} Y_{i}] = \frac{125}{14} \approx \underline{8.93}\)</span></li>
<li><span class="math inline">\(Var[\theta_{B}|\sum_{i=1}^{n_{B}} Y_{i}] = \frac{125}{14^2}\approx \underline{0.64}\)</span></li>
<li><span class="math inline">\(95\%\)</span> quantile-based confidence intervals is shown in <a href="#tbl-33b">Table&nbsp;<span>6.6</span></a></li>
</ul>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>aB <span class="op">=</span> <span class="dv">125</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>bB <span class="op">=</span> <span class="dv">14</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>interval_gamma_95(aB,bB)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<div id="tbl-33b" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;6.6:  95% quantile-based confidence intervals of mice B. </caption>
  <thead>
    <tr>
      <th></th>
      <th>Left bound</th>
      <th>Right bound</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.432064</td>
      <td>10.560308</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="b-1" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="b-1"><span class="header-section-number">6.4.2</span> (b)</h3>
<blockquote class="blockquote">
<p>Computing and plot the posterior expectation of <span class="math inline">\(\theta_B\)</span> under the prior distribution <span class="math inline">\(\theta_B \sim gamma(12\times n_0, n_0)\)</span> for each value of <span class="math inline">\(n_0\in \{1,2,\dots, 50\}\)</span>. Descirbe what sort of prior beliefs about <span class="math inline">\(\theta_B\)</span> to be close to that of <span class="math inline">\(\theta_A\)</span>.</p>
</blockquote>
<p>The posterior distribution can be derived from <a href="#eq-gamma-conj">Equation&nbsp;<span>6.2</span></a>. As shown in <a href="#fig-3-3-b">Figure&nbsp;<span>6.5</span></a>, the mean value of <span class="math inline">\(\theta_B\)</span> with <span class="math inline">\(n_0\)</span> close to <span class="math inline">\(50\)</span> is necessary to have the similar posterior mean as <span class="math inline">\(\theta_A\)</span>.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> post_gamma(a,b, sumY, n):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> st.gamma(a<span class="op">+</span>sumY, scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span>(b <span class="op">+</span> n))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>n0s <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">50</span>, <span class="dv">1</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>sumYB <span class="op">=</span> <span class="dv">11</span><span class="op">+</span><span class="dv">11</span><span class="op">+</span><span class="dv">10</span><span class="op">+</span><span class="dv">9</span><span class="op">+</span><span class="dv">9</span><span class="op">+</span><span class="dv">8</span><span class="op">+</span><span class="dv">7</span><span class="op">+</span><span class="dv">10</span><span class="op">+</span><span class="dv">6</span><span class="op">+</span><span class="dv">8</span><span class="op">+</span><span class="dv">8</span><span class="op">+</span><span class="dv">9</span><span class="op">+</span><span class="dv">7</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>nB <span class="op">=</span> <span class="dv">13</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>post_theta_rvBs <span class="op">=</span> [post_gamma(<span class="dv">12</span><span class="op">*</span>n0, n0, sumYB, nB) <span class="cf">for</span> n0 <span class="kw">in</span> n0s]</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>meanBs <span class="op">=</span> [post_theta_rvBs[i].mean() <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(n0s))]</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>plt.plot(n0s, meanBs, <span class="st">"ko"</span>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$n_0$"</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$E[Pr(</span><span class="ch">\\</span><span class="st">theta_</span><span class="sc">{B}</span><span class="st">|y_</span><span class="sc">{B}</span><span class="st">)]$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-3-3-b" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-3-3-b-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.5: Mean of Posterior distribution of mice B with given n0s.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="c" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="c"><span class="header-section-number">6.4.3</span> (c)</h3>
<blockquote class="blockquote">
<p>Should knowledge about population <span class="math inline">\(A\)</span> tell us anything about population <span class="math inline">\(B\)</span>? Discuss whether or not it makes sense to have <span class="math inline">\(p(\theta_A,\theta_B)=p(\theta_A)\times p(\theta_B)\)</span>.</p>
</blockquote>
<p>The understanding of mice <span class="math inline">\(A\)</span> is well known. Though mice <span class="math inline">\(B\)</span> is related to mice <span class="math inline">\(A\)</span>, there is possibility that mice <span class="math inline">\(B\)</span> is different from the distribution of <span class="math inline">\(A\)</span>. Thus, viewing mice <span class="math inline">\(A\)</span> and mice <span class="math inline">\(B\)</span> with independent prior distribution makes sense.</p>
</section>
</section>
<section id="problem-3.4" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="problem-3.4"><span class="header-section-number">6.5</span> Problem 3.4</h2>
<blockquote class="blockquote">
<p>Mixtures of beta priors: Estimate the probability <span class="math inline">\(\theta\)</span> of teen recidivism based on a study in which there were <span class="math inline">\(n=43\)</span> individuals released from incarceration and <span class="math inline">\(y=15\)</span> re-offenders within <span class="math inline">\(36\)</span> months.</p>
</blockquote>
<section id="sec-3-4-a" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="sec-3-4-a"><span class="header-section-number">6.5.1</span> (a)</h3>
<blockquote class="blockquote">
<p>Using a <span class="math inline">\(beta(2,8)\)</span> prior for <span class="math inline">\(\theta\)</span>, plot <span class="math inline">\(p(\theta)\)</span>, <span class="math inline">\(p(y|\theta)\)</span> and <span class="math inline">\(p(\theta|y)\)</span> as functions of <span class="math inline">\(\theta\)</span>. Find the posterior mean, mode, and standard deviation of <span class="math inline">\(\theta\)</span>. Find a <span class="math inline">\(95\%\)</span> quantile-based condifence interval.</p>
</blockquote>
<ul>
<li><span class="math inline">\(p(\theta) \sim beta(2,8)\)</span>
<ul>
<li>Plotted in <a href="#fig-3-4-prior">Figure&nbsp;<span>6.6</span></a></li>
</ul></li>
</ul>
<p>According to <span class="citation" data-cites="hoff2009first">Hoff (<a href="../ref.html#ref-hoff2009first" role="doc-biblioref">2009</a>)</span> pp.&nbsp;37-38, the conjugate posterior (<span class="math inline">\(\{\theta|Y=y\}\)</span>) given beta as prior is a beta distribution, and <span class="math inline">\(Y\sim binomial(n,\theta)\)</span></p>
<ul>
<li><span class="math inline">\(p(y|\theta) = {n\choose y}\theta^{y}(1-\theta)^{(n-y)}\)</span>
<ul>
<li>Plotted in <a href="#fig-3-4-likelihood">Figure&nbsp;<span>6.7</span></a></li>
</ul></li>
<li><span class="math inline">\(p(\theta | y) \sim beta(a+y, b + n-y) = beta(2+15, 8 + 43 - 15) = beta(17, 36)\)</span>
<ul>
<li>Plotted in <a href="#fig-3-4-posterior">Figure&nbsp;<span>6.8</span></a></li>
<li><span class="math inline">\(E[p(\theta | y)] = \frac{a}{a+b} = \frac{17}{17+36} \approx 0.32\)</span></li>
<li><span class="math inline">\(Mode(p(\theta | y)) = \frac{a-1}{a+b-2} \approx 0.31\)</span></li>
<li><span class="math inline">\(std(p(\theta | y)) = \sqrt{var[p(\theta | y)]} = \sqrt{\frac{ab}{(a+b)^2 (a+b+1)}} = \sqrt{\frac{17\times 36}{(17+36)^2 (17+36+1)}}\approx 0.06\)</span></li>
<li>Properties are shown in <a href="#tbl-3-4-posterior">Table&nbsp;<span>6.7</span></a>.</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>rv_theta <span class="op">=</span> st.beta(<span class="dv">2</span>,<span class="dv">8</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, rv_theta.pdf(thetas), <span class="st">'k-'</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)<span class="op">;</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(</span><span class="ch">\\</span><span class="st">theta)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-3-4-prior" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-3-4-prior-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.6: Prior distribution.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">43</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>pr_like <span class="op">=</span> [st.binom.pmf(y, n, theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas]</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, pr_like, <span class="st">'k-'</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)<span class="op">;</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(y|</span><span class="ch">\\</span><span class="st">theta)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-3-4-likelihood" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-3-4-likelihood-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.7: Likelihood</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>rv_theta <span class="op">=</span> st.beta(<span class="dv">2</span><span class="op">+</span><span class="dv">15</span>, <span class="dv">8</span> <span class="op">+</span> <span class="dv">43</span> <span class="op">-</span> <span class="dv">15</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, rv_theta.pdf(thetas), <span class="st">'k-'</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)<span class="op">;</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(</span><span class="ch">\\</span><span class="st">theta | y)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-3-4-posterior" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-3-4-posterior-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.8: Posterior distribution</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>ints <span class="op">=</span> rv_theta.interval(<span class="fl">0.95</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Properties"</span>: [<span class="st">"Left bound (CI)"</span>, <span class="st">"Right bound (CI)"</span>, <span class="st">"mean"</span>, <span class="st">"mode"</span>, <span class="st">"standard deviation"</span>], <span class="st">"Values"</span>: [ints[<span class="dv">0</span>], ints[<span class="dv">1</span>], rv_theta.mean(), (<span class="dv">17</span><span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>(<span class="dv">17</span><span class="op">+</span><span class="dv">36</span><span class="op">-</span><span class="dv">2</span>), rv_theta.std()]})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<div id="tbl-3-4-posterior" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;6.7:  Properties of posterior distribution. </caption>
  <thead>
    <tr>
      <th></th>
      <th>Properties</th>
      <th>Values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Left bound (CI)</td>
      <td>0.203298</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Right bound (CI)</td>
      <td>0.451024</td>
    </tr>
    <tr>
      <th>2</th>
      <td>mean</td>
      <td>0.320755</td>
    </tr>
    <tr>
      <th>3</th>
      <td>mode</td>
      <td>0.313725</td>
    </tr>
    <tr>
      <th>4</th>
      <td>standard deviation</td>
      <td>0.063519</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="sec-3-4-b" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="sec-3-4-b"><span class="header-section-number">6.5.2</span> (b)</h3>
<blockquote class="blockquote">
<p>Repeat <a href="#sec-3-4-a">(a)</a>, but using a <span class="math inline">\(beta(8,2)\)</span> prior for <span class="math inline">\(\theta\)</span>.</p>
</blockquote>
<ul>
<li><span class="math inline">\(p(\theta) \sim beta(8,2)\)</span>
<ul>
<li>Plotted in <a href="#fig-3-4-prior-2">Figure&nbsp;<span>6.9</span></a></li>
</ul></li>
<li><span class="math inline">\(p(y|\theta) = {n\choose y}\theta^{y}(1-\theta)^{(n-y)}\)</span>
<ul>
<li>Plotted in <a href="#fig-3-4-likelihood-2">Figure&nbsp;<span>6.10</span></a></li>
</ul></li>
<li><span class="math inline">\(p(\theta | y) \sim beta(a+y, b + n-y) = beta(8+15, 2 + 43 - 15) = beta(23, 30)\)</span>
<ul>
<li>Plotted in <a href="#fig-3-4-posterior-2">Figure&nbsp;<span>6.11</span></a></li>
<li><span class="math inline">\(E[p(\theta | y)] = \frac{a}{a+b} = \frac{23}{23+30} \approx 0.434\)</span></li>
<li><span class="math inline">\(Mode(p(\theta | y)) = \frac{a-1}{a+b-2} \approx 0.431\)</span></li>
<li><span class="math inline">\(std(p(\theta | y)) = \sqrt{var[p(\theta | y)]} = \sqrt{\frac{ab}{(a+b)^2 (a+b+1)}} = \sqrt{\frac{23\times 30}{(23+30)^2 (23+30+1)}}\approx 0.07\)</span></li>
<li>Properties are shown in <a href="#tbl-3-4-posterior-2">Table&nbsp;<span>6.8</span></a>.</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>rv_theta <span class="op">=</span> st.beta(<span class="dv">8</span>,<span class="dv">2</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, rv_theta.pdf(thetas), <span class="st">'k-'</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)<span class="op">;</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(</span><span class="ch">\\</span><span class="st">theta)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-3-4-prior-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-3-4-prior-2-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.9: Prior distribution.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">43</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>pr_like <span class="op">=</span> [st.binom.pmf(y, n, theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas]</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, pr_like, <span class="st">'k-'</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)<span class="op">;</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(y|</span><span class="ch">\\</span><span class="st">theta)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-3-4-likelihood-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-3-4-likelihood-2-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.10: Likelihood</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>rv_theta <span class="op">=</span> st.beta(<span class="dv">8</span><span class="op">+</span><span class="dv">15</span>, <span class="dv">2</span> <span class="op">+</span> <span class="dv">43</span> <span class="op">-</span> <span class="dv">15</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, rv_theta.pdf(thetas), <span class="st">'k-'</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)<span class="op">;</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(</span><span class="ch">\\</span><span class="st">theta | y)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-3-4-posterior-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-3-4-posterior-2-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.11: Posterior distribution</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>ints <span class="op">=</span> rv_theta.interval(<span class="fl">0.95</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Properties"</span>: [<span class="st">"Left bound (CI)"</span>, <span class="st">"Right bound (CI)"</span>, <span class="st">"mean"</span>, <span class="st">"mode"</span>, <span class="st">"standard deviation"</span>], <span class="st">"Values"</span>: [ints[<span class="dv">0</span>], ints[<span class="dv">1</span>], rv_theta.mean(), (<span class="dv">23</span><span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>(<span class="dv">23</span><span class="op">+</span><span class="dv">30</span><span class="op">-</span><span class="dv">2</span>), rv_theta.std()]})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<div id="tbl-3-4-posterior-2" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;6.8:  Properties of posterior distribution. </caption>
  <thead>
    <tr>
      <th></th>
      <th>Properties</th>
      <th>Values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Left bound (CI)</td>
      <td>0.304696</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Right bound (CI)</td>
      <td>0.567953</td>
    </tr>
    <tr>
      <th>2</th>
      <td>mean</td>
      <td>0.433962</td>
    </tr>
    <tr>
      <th>3</th>
      <td>mode</td>
      <td>0.431373</td>
    </tr>
    <tr>
      <th>4</th>
      <td>standard deviation</td>
      <td>0.067445</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="sec-3-4-c" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="sec-3-4-c"><span class="header-section-number">6.5.3</span> (c)</h3>
<blockquote class="blockquote">
<p>Consider the following prior distribution for <span class="math inline">\(\theta\)</span>: <span class="math display">\[p(\theta) = \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}[3\theta(1-\theta)^7+\theta^7(1-\theta)]\]</span> which is a <span class="math inline">\(75-25\%\)</span> mixture of a <span class="math inline">\(beta(2,8)\)</span> and a <span class="math inline">\(beta(8,2)\)</span> prior distribution. Plot this prior distribution and compare it to the priors in <a href="#sec-3-4-a">(a)</a> and <a href="#sec-3-4-b">(b)</a>. Describe what sort of prior opinion this may represent.</p>
</blockquote>
<p>The mixture of beta distribution is plotted in <a href="#fig-3-4-mixbeta">Figure&nbsp;<span>6.12</span></a>. This opinion merges two opposite suggestions with different weights:</p>
<ol type="1">
<li><span class="math inline">\(\theta\)</span> is low (<a href="#fig-3-4-prior">Figure&nbsp;<span>6.6</span></a>).</li>
<li><span class="math inline">\(\theta\)</span> is high (<a href="#fig-3-4-prior-2">Figure&nbsp;<span>6.9</span></a>).</li>
</ol>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mixBeta(th):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">0.25</span><span class="op">*</span>gamma(<span class="dv">10</span>)<span class="op">/</span>(gamma(<span class="dv">2</span>)<span class="op">*</span>gamma(<span class="dv">8</span>))<span class="op">*</span>( <span class="dv">3</span><span class="op">*</span>th<span class="op">*</span>((<span class="dv">1</span><span class="op">-</span>th)<span class="op">**</span><span class="dv">7</span>) <span class="op">+</span> (th<span class="op">**</span><span class="dv">7</span>)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>th) )</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>prs <span class="op">=</span> [mixBeta(theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas]</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, prs, <span class="st">"k-"</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$p(</span><span class="ch">\\</span><span class="st">theta)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-3-4-mixbeta" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-3-4-mixbeta-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.12: Mixture beta distribution</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-3-4-d" class="level3" data-number="6.5.4">
<h3 data-number="6.5.4" class="anchored" data-anchor-id="sec-3-4-d"><span class="header-section-number">6.5.4</span> (d)</h3>
<blockquote class="blockquote">
<p>For the prior in <a href="#sec-3-4-c">(c)</a>:</p>
<ol type="1">
<li><p>Write out mathematically <span class="math inline">\(p(\theta)\times p(y|\theta)\)</span> and simplify as much as possible.</p></li>
<li><p>The posterior distribution is a mixture of two distributions you know. Identify these distributions.</p></li>
<li><p>On a computer, calculate and plot <span class="math inline">\(p(\theta) \times p(y|\theta)\)</span> for a variety of <span class="math inline">\(\theta\)</span> values. Also find (approximately) the posterior mode, and discuss its relation to the modes in <a href="#sec-3-4-a">(a)</a> and <a href="#sec-3-4-b">(b)</a>.</p></li>
</ol>
</blockquote>
<p><strong>Part 1.</strong></p>
<p>Noted that <span class="math inline">\({43 \choose 15} = \frac{43!}{15!28!}=\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}\)</span></p>
<p><span class="math display">\[\begin{align}
  p(\theta)\times p(y|\theta) &amp;= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}[3\theta(1-\theta)^7+\theta^7(1-\theta)] \times {43\choose 15}\theta^{15}(1-\theta)^{(43-15)}\\
  &amp;= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}\underbrace{{43 \choose 15}}_{\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}}(\theta^{22}(1-\theta)^{29} + 3\theta^{16}(1-\theta)^{35})\\
  &amp;= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}(\underline{\theta^{22}(1-\theta)^{29}} + \underline{3\theta^{16}(1-\theta)^{35}})\\
\end{align}\]</span></p>
<p><em>The simplification is by the aid of walfram-alpha<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</em></p>
<p><strong>Part 2.</strong></p>
<p>The distribution is the mixture of <span class="math inline">\(Beta(23,30)\)</span> and <span class="math inline">\(Beta(17,36)\)</span> with certain weights.</p>
<p><strong>Part 3.</strong></p>
<ul>
<li>The mode of <span class="math inline">\(p(\theta) \times p(y|\theta)\)</span> is <span class="math inline">\(0.314\)</span> (See <a href="#fig-3-4-3-mixbeta">Figure&nbsp;<span>6.13</span></a>).</li>
<li>The mode in <a href="#sec-3-4-a">(a)</a>: <span class="math inline">\(0.313725\)</span></li>
<li>The mode in <a href="#sec-3-4-b">(b)</a>: <span class="math inline">\(0.431373\)</span></li>
</ul>
<p>Thus, the posterior distribution has the mode between <span class="math inline">\(Beta(2,8)\)</span> (<a href="#sec-3-4-a">(a)</a>) and <span class="math inline">\(Beta(8,2)\)</span>(<a href="#sec-3-4-b">(b)</a>), and more close to <span class="math inline">\(Beta(2.8)\)</span></p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mixture_post(th):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  scale <span class="op">=</span> <span class="fl">0.25</span> <span class="op">*</span> gamma(<span class="dv">10</span>)<span class="op">/</span>(gamma(<span class="dv">2</span>)<span class="op">*</span>gamma(<span class="dv">8</span>)) <span class="op">*</span> gamma(<span class="dv">44</span>)<span class="op">/</span>(gamma(<span class="dv">16</span>)<span class="op">*</span>gamma(<span class="dv">29</span>))</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  beta <span class="op">=</span> (th<span class="op">**</span><span class="dv">22</span>)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>th)<span class="op">**</span><span class="dv">29</span> <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>(th<span class="op">**</span><span class="dv">16</span>)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>th)<span class="op">**</span><span class="dv">35</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> scale<span class="op">*</span>beta</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>prs <span class="op">=</span> [mixture_post(theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas]</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>maxTh <span class="op">=</span> thetas[np.argmax(prs)]</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, prs, <span class="st">'k-'</span>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$p(</span><span class="ch">\\</span><span class="st">theta)</span><span class="ch">\\</span><span class="st">times p(y|</span><span class="ch">\\</span><span class="st">theta)$"</span>)<span class="op">;</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>maxTh, linestyle<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>, label<span class="op">=</span> <span class="st">"Mode=</span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(maxTh))<span class="op">;</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-3-4-3-mixbeta" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-3-4-3-mixbeta-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.13: Posterior distribution with mixture of two beta distributions.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="e-1" class="level3" data-number="6.5.5">
<h3 data-number="6.5.5" class="anchored" data-anchor-id="e-1"><span class="header-section-number">6.5.5</span> (e)</h3>
<blockquote class="blockquote">
<p>Find a general formula for the weights of the mixture distribution in <a href="#sec-3-4-d">(d) 2.</a>, and provide an interpretation for their values.</p>
</blockquote>
<p>Let <span class="math inline">\(c_1 = \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}\)</span></p>
<p><span class="math display">\[\begin{align}
  p(\theta)\times p(y|\theta) &amp;= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}(\underline{\theta^{22}(1-\theta)^{29}} + \underline{3\theta^{16}(1-\theta)^{35}})\\
                              &amp;= c_1 (\theta^{22}(1-\theta)^{29} + 3\theta^{16}(1-\theta)^{35})\\
                              &amp;= c_1 \theta^{22}(1-\theta)^{29} + 3c_1 \theta^{16}(1-\theta)^{35})\\
                              &amp;= c_1 \frac{\Gamma(23)\Gamma(30)}{\Gamma(53)} Beta(\theta, 23,30) + 3 c_1 \frac{\Gamma(17)\Gamma(36)}{\Gamma(51)} Beta(\theta, 17,36)\\
                              &amp;= 0.0003 \times Beta(\theta, 23,30) + 58.16 \times Beta(\theta, 17,36)\\
                              &amp;= \omega_1 \cdot Beta(\theta, 23,30) + \omega_2 \cdot Beta(\theta, 17,36)
\end{align}\]</span></p>
<p>That means <span class="math inline">\(Beta(17,36)\)</span> is preferred to <span class="math inline">\(Beta(23,30)\)</span>. The updated posterior information is more close to the <a href="#sec-3-4-d">(a)</a>. That is because the mixture of priors has more weights (<span class="math inline">\(75\%\)</span>) on the prior of <span class="math inline">\(Beta(2,8)\)</span>.</p>
</section>
</section>
<section id="problem-3.7" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="problem-3.7"><span class="header-section-number">6.6</span> Problem 3.7</h2>
<blockquote class="blockquote">
<p>Posterior prediction: Consider a pilot study in which <span class="math inline">\(n_1 = 15\)</span> children enrolled in special education classes were randomly selected and tested for a certain type of learning disability. In the pilot study, <span class="math inline">\(y_1 = 2\)</span> children tested positive for the disability.</p>
</blockquote>
<section id="a-2" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="a-2"><span class="header-section-number">6.6.1</span> (a)</h3>
<blockquote class="blockquote">
<p>Using a uniform prior distribution, find the posterior distribution of <span class="math inline">\(\theta\)</span>, the fraction of students in special education classes who have the disability. Find the posterior mean, mode and standard deviation of <span class="math inline">\(\theta\)</span>, and plot the posterior density.</p>
</blockquote>
<p><span class="math display">\[\theta \sim beta(1,1) (uniform)\]</span> <span class="math display">\[Y\sim binomial(n_1,\theta)\]</span></p>
<p><span class="math display">\[\begin{align}
  \theta|Y=y &amp;\sim beta(1+y_1, 1+n_1 - y_1)\\
             &amp;= beta(1 + 2, 1+15-2)\\
             &amp;= beta(3, 14)\\
             &amp;= beta(a_p, b_p)\\
\end{align}\]</span></p>
<ul>
<li>The distribution is plotted in <a href="#fig-3-7-a">Figure&nbsp;<span>6.14</span></a></li>
<li><span class="math inline">\(E[\theta|Y] = \frac{a_p}{a_p+b_p} = \frac{3}{3+14} \approx 0.1764\)</span></li>
<li><span class="math inline">\(Mode[\theta|Y] = \frac{(a_p - 1)}{a_p - 1 + b_p - 1} = \frac{(3 - 1)}{3 - 1 + 14 - 1} \approx 0.1333\)</span></li>
<li><span class="math inline">\(Std[\theta|Y] = \sqrt{ \frac{a_p b_p}{(a_p+b_p+1)(a_p+b_p)^2} } = \sqrt{ \frac{3\cdot 14}{(3+14+1)(3+14)^2}} \approx 0.0899\)</span></li>
</ul>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> st.beta(<span class="dv">3</span>, <span class="dv">14</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>pr_pos <span class="op">=</span> [pos.pdf(theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas]</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, pr_pos, <span class="st">"k-"</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(</span><span class="ch">\\</span><span class="st">theta|Y)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-3-7-a" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-3-7-a-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.14: Posterior distribution</figcaption><p></p>
</figure>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Researchers would like to recruit students with the disability to participate in a long-term study, but first they need to make sure they can recruit enough students. Let <span class="math inline">\(n_2 = 278\)</span> be the number of children in special education classes in this particular school district, and let <span class="math inline">\(Y_2\)</span> be the number of students with the disability.</p>
</blockquote>
</section>
<section id="b-2" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="b-2"><span class="header-section-number">6.6.2</span> (b)</h3>
<blockquote class="blockquote">
<p>Find <span class="math inline">\(Pr(Y_2=y_2|Y_1 =2)\)</span>, the posterior predictive distribution of <span class="math inline">\(Y_2\)</span>, as follows:</p>
<ol type="1">
<li><p>Discuss what assumptions are needed about the joint distribution of <span class="math inline">\((Y_1, Y_2)\)</span> such that the fololowing is true: <span id="eq-3-7-b-p"><span class="math display">\[Pr(Y_2=y_2 |Y_1=2) = \int^{1}_{0} Pr(Y_2=y_2|\theta)p(\theta|Y_1=2)d\theta \tag{6.5}\]</span></span></p></li>
<li><p>Now plug in the forms of <span class="math inline">\(Pr(Y_2=y_2|\theta)\)</span> and <span class="math inline">\(p(\theta|Y_1 =2)\)</span> in the above integral.</p></li>
<li><p>Figure out what the above integral must be by using the calculus result discussed in Section 3.1.</p></li>
</ol>
</blockquote>
<p><strong>Part 1</strong></p>
<ul>
<li>The assumption is that <span class="math inline">\(Y_2\)</span> is <em>conditionally independent</em> on <span class="math inline">\(Y_1\)</span> over <span class="math inline">\(\theta\)</span></li>
</ul>
<p>Thus,</p>
<p><span class="math display">\[\begin{align}
  \int^{1}_{0} Pr(Y_2=y_2|\theta)p(\theta|Y_1=2)d\theta &amp;= \int^{1}_{0} Pr(Y_2=y_2)|\theta, Y_1=2)p(\theta |Y_1=2)d\theta\\
  &amp;= \int^{1}_{0} Pr(Y_2=y_2, \theta |Y_1 =2) d\theta\\
  &amp;= Pr(Y_2 = y_2 | Y_1=2)
\end{align}\]</span></p>
<p>The equality of <a href="#eq-3-7-b-p">Equation&nbsp;<span>6.5</span></a> holds.</p>
<p><strong>Part 2</strong></p>
<p><span class="math display">\[\begin{align}
  Pr(Y_2=y_2 |Y_1=2) &amp;= \int^{1}_{0} Pr(Y_2=y_2|\theta)p(\theta|Y_1=2)d\theta\\
                     &amp;= \int^{1}_{0} binomial(y_2, n_2, \theta) beta(\theta, 3,14) d\theta\\
                     &amp;= \int^{1}_{0} {n_2 \choose y_2}\theta^{y_2}(1-\theta)^{n_2-y_2} \frac{\Gamma(17)}{\Gamma(3)\Gamma(14)}\theta^{2}(1-\theta)^{13} d\theta\\
                     &amp;= {n_2 \choose y_2}\frac{\Gamma(17)}{\Gamma(3)\Gamma(14)} \int^{1}_{0} \theta^{y_2}(1-\theta)^{n_2-y_2} \theta^{2}(1-\theta)^{13} d\theta\\&amp;= {n_2 \choose y_2}\frac{\Gamma(17)}{\Gamma(3)\Gamma(14)} \int^{1}_{0} \theta^{(2+y_2)}(1-\theta)^{n_2 - y_2 +13} d\theta\\
                     &amp;= {278\choose y_2}\frac{\Gamma(17)}{\Gamma(3)\Gamma(14)} \int^{1}_{0} \theta^{(2+y_2)}(1-\theta)^{278 - y_2 +13} d\theta\\
                     &amp;= {278\choose y_2}\frac{\Gamma(17)}{\Gamma(3)\Gamma(14)} \int^{1}_{0} \theta^{(2+y_2)}(1-\theta)^{291 - y_2} d\theta\\
\end{align}\]</span></p>
<p><strong>Part 3</strong></p>
<p>Use the calculus trick:</p>
<p><span class="math display">\[\int^{1}_{0} \theta^{a-1}(1-\theta)^{b-1}d\theta = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\]</span></p>
<p><span class="math display">\[\begin{align}
  \int^{1}_{0} \theta^{(2+y_2)}(1-\theta)^{291 - y_2} d\theta &amp;= \int^{1}_{0} \theta^{(3+y_2 - 1)}(1-\theta)^{292 - y_2 - 1} d\theta\\
  &amp;= \frac{\Gamma(3+y_2)\Gamma(292 - y_2)}{\Gamma(3+y_2 + 292 - y_2)}\\
  &amp;= \frac{\Gamma(3+y_2)\Gamma(292 - y_2)}{\Gamma(295)}\\
\end{align}\]</span></p>
<p><span class="math inline">\(\therefore\)</span> <span class="math display">\[\begin{align}
Pr(Y_2=y_2 |Y_1=2) &amp;= {278\choose y_2}\frac{\Gamma(17)}{\Gamma(3)\Gamma(14)}\frac{\Gamma(3+y_2)\Gamma(292 - y_2)}{\Gamma(295)}\\
                   &amp;= \frac{\Gamma(278)}{\Gamma(y_2)\Gamma(278-y_2)} \frac{\Gamma(17)}{\Gamma(3)\Gamma(14)}\frac{\Gamma(3+y_2)\Gamma(292 - y_2)}{\Gamma(295)}\\
                   &amp;= \frac{\Gamma(3+y_2)}{\Gamma(y_2)}\frac{\Gamma(278)}{\Gamma(295)}\frac{\Gamma(292-y_2)}{\Gamma(278-y_2)} \frac{\Gamma(17)}{\Gamma(3)\Gamma(14)}\\
                   &amp;= \prod^{3+y_2 -1}_{i=y_2} i \times \frac{1}{\prod^{295-1}_{i=278}i}\prod^{292-y_2 - 1}_{i=278-y_2}i \times 1680\\
                   &amp;= \prod^{2+y_2}_{i=y_2} i \times \frac{1}{\prod^{294}_{i=278}i}\prod^{291-y_2}_{i=278-y_2}i \times 1680\\
\end{align}\]</span></p>
</section>
<section id="sec-3-7-c" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="sec-3-7-c"><span class="header-section-number">6.6.3</span> (c)</h3>
<blockquote class="blockquote">
<p>Plot the function <span class="math inline">\(Pr(Y_2 = y_2 | Y_1 =2)\)</span> as a function of <span class="math inline">\(y_2\)</span>. Obtain the mean and standard deviation of <span class="math inline">\(Y_2\)</span>, given <span class="math inline">\(Y_1 = 2\)</span>.</p>
</blockquote>
<ul>
<li>The plot of <span class="math inline">\(Pr(Y_2 = y_2 | Y_1 =2)\)</span> is in <a href="#fig-3-7-c">Figure&nbsp;<span>6.15</span></a>.</li>
<li>mean and standard deviation are displayed in <a href="#tbl-3-7-c">Table&nbsp;<span>6.9</span></a>.</li>
</ul>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prod(a, b):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  s <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> np.arange(a,b<span class="op">+</span><span class="dv">1</span>, <span class="fl">1.0</span>):</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> s<span class="op">*</span>i</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> s</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pred_prob(y2, n2<span class="op">=</span><span class="dv">278</span>):</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> prod(y2, <span class="dv">2</span><span class="op">+</span>y2)<span class="op">*</span>(<span class="dv">1</span><span class="op">/</span>prod(<span class="dv">278</span>, <span class="dv">294</span>))<span class="op">*</span>prod(<span class="dv">278</span><span class="op">-</span>y2, <span class="dv">291</span><span class="op">-</span>y2)<span class="op">*</span><span class="dv">1680</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>y2s <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">278</span>, <span class="dv">279</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>prs <span class="op">=</span> [pred_prob(y2) <span class="cf">for</span> y2 <span class="kw">in</span> y2s]</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>prs <span class="op">=</span> prs<span class="op">/</span>np.<span class="bu">sum</span>(prs)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>plt.plot(y2s, prs, <span class="st">'ko'</span>)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$y_</span><span class="sc">{2}</span><span class="st">$"</span>)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$p(Y_2 | Y_1=2)$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-3-7-c" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-3-7-c-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.15: Predictive distribution given Y1=2</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.<span class="bu">sum</span>(prs <span class="op">*</span> y2s)<span class="op">/</span>np.<span class="bu">sum</span>(prs)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span>  np.sqrt(np.<span class="bu">sum</span>(y2s<span class="op">*</span>y2s<span class="op">*</span>prs) <span class="op">-</span> (np.<span class="bu">sum</span>(y2s<span class="op">*</span>prs))<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"mean"</span>: [np.<span class="bu">sum</span>(prs <span class="op">*</span> y2s)<span class="op">/</span>np.<span class="bu">sum</span>(prs)], <span class="st">"std"</span>: [std]})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<div id="tbl-3-7-c" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;6.9:  Predictive distribution given Y1=2 </caption>
  <thead>
    <tr>
      <th></th>
      <th>mean</th>
      <th>std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>59.105263</td>
      <td>26.01193</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="d-1" class="level3" data-number="6.6.4">
<h3 data-number="6.6.4" class="anchored" data-anchor-id="d-1"><span class="header-section-number">6.6.4</span> (d)</h3>
<blockquote class="blockquote">
<p>The posterior mode and the MLE (maximum likelihood estimate) of <span class="math inline">\(\theta\)</span>, based on data from the pilot study, are both <span class="math inline">\(\hat{\theta} = \frac{2}{15}\)</span>. Plot the distribution <span class="math inline">\(Pr(Y_2 = y_2|\theta=\hat{\theta})\)</span>, and find the mean and standard deviation of <span class="math inline">\(Y_2\)</span> given <span class="math inline">\(\theta=\hat{\theta}\)</span>. Compare these results to the plots and calculation in <a href="#sec-3-7-c">(c)</a> and discuss any differences. Which distribution for <span class="math inline">\(Y_2\)</span> would you used to make predictions, and why?</p>
</blockquote>
<p><span class="math display">\[\begin{align}
  Pr(Y_2 = y_2 |\theta=\hat{\theta}) &amp;= binomial(y_2, n_2, \hat{\theta})\\
\end{align}\]</span></p>
<ul>
<li><p>The plot of <span class="math inline">\(Pr(Y_2 = y_2|\theta=\hat{\theta})\)</span> distribution along with <span class="math inline">\(y_2\)</span> is <a href="#fig-3-7-d">Figure&nbsp;<span>6.16</span></a>.</p></li>
<li><p>Mean and standard deviation are shown in <a href="#tbl-3-7-d">Table&nbsp;<span>6.10</span></a>.</p></li>
<li><p>Compare to <a href="#fig-3-7-c">Figure&nbsp;<span>6.15</span></a>, <a href="#fig-3-7-d">Figure&nbsp;<span>6.16</span></a> has less variation and less mean, which is more close to the original average of <span class="math inline">\(Y_1\)</span> data (<span class="math inline">\(=\frac{2}{15}\)</span>).</p></li>
<li><p><a href="#fig-3-7-c">Figure&nbsp;<span>6.15</span></a> provides better prediction with MLE <span class="math inline">\(\theta\)</span> because its properties is more related to the original average, and the likelihood is maximized with MLE method.</p></li>
</ul>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>n2 <span class="op">=</span> <span class="dv">278</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>th <span class="op">=</span> <span class="dv">2</span><span class="op">/</span><span class="dv">15</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>rv <span class="op">=</span> st.binom(n2, th)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>y2s <span class="op">=</span> np.linspace(<span class="dv">0</span>,n2, n2<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>prs <span class="op">=</span> [rv.pmf(y2) <span class="cf">for</span> y2 <span class="kw">in</span> y2s]</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>plt.plot(y2s, prs, <span class="st">"ko"</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$y_2$"</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(Y_2 = y_2|</span><span class="ch">\\</span><span class="st">theta=</span><span class="ch">\\</span><span class="st">hat{</span><span class="ch">\\</span><span class="st">theta})$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-3-7-d" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="hw2_files/figure-html/fig-3-7-d-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.16: Predictive distribution given MLE theta</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.<span class="bu">sum</span>(y2s<span class="op">*</span>prs)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>(y2s<span class="op">*</span>y2s<span class="op">*</span>prs) <span class="op">-</span> (np.<span class="bu">sum</span>(y2s<span class="op">*</span>prs))<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"mean"</span>: [np.<span class="bu">sum</span>(prs <span class="op">*</span> y2s)<span class="op">/</span>np.<span class="bu">sum</span>(prs)], <span class="st">"std"</span>: [std]})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<div id="tbl-3-7-d" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;6.10:  Predictive distribution given MLE theta </caption>
  <thead>
    <tr>
      <th></th>
      <th>mean</th>
      <th>std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>37.066667</td>
      <td>5.667843</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="problem-3.12" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="problem-3.12"><span class="header-section-number">6.7</span> Problem 3.12</h2>
<blockquote class="blockquote">
<p>Jeffrey’s prior: Jeffreys (1961) suggested a default rule for gnerating a prior distribution of a parameter <span class="math inline">\(\theta\)</span> in a sampling model <span class="math inline">\(p(y|\theta)\)</span>. Jeffreys’ prior is given by <span class="math inline">\(p_{J}\propto \sqrt{I(\theta)}\)</span>, where <span class="math inline">\(I(\theta) = - E[\frac{\partial^{2} \log p(Y|\theta)}{\partial\theta^2} | \theta]\)</span> is the <em>Fisher information</em>.</p>
</blockquote>
<section id="sec-3-12-a" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="sec-3-12-a"><span class="header-section-number">6.7.1</span> (a)</h3>
<blockquote class="blockquote">
<p>Let <span class="math inline">\(Y\sim binomial(n,\theta)\)</span>. Obtain Jeffreys’ prior distribution <span class="math inline">\(p_J(\theta)\)</span> for this model.</p>
</blockquote>
<p><span class="math inline">\(\because\)</span> <span class="math inline">\(Y\sim binomial(n,\theta)\)</span> <span class="math inline">\(\therefore\)</span> <span class="math inline">\(E[Y]=n\theta\)</span></p>
<p><span class="math display">\[\begin{align}
p(y|\theta) &amp;= {n\choose y}\theta^{y}(1-\theta)^{n-y}\\
\log(p(y|\theta)) &amp;= \log {n\choose y} + y\log \theta + (n-y)\log (1-\theta)\\
\frac{\partial \log(p(y|\theta))}{\partial \theta} &amp;= \frac{y}{\theta} - \frac{n-y}{1-\theta}\\
\frac{\partial^2 \log(p(y|\theta))}{\partial^2 \theta} &amp;= \frac{-y}{\theta^2} - \frac{n-y}{(1-\theta)^2}\\
E[\frac{\partial^2 \log(p(y|\theta))}{\partial^2 \theta} |\theta] &amp;= -\frac{n\theta}{\theta^2} - \frac{n-n\theta}{(1-\theta)^2}\\
E[\frac{\partial^2 \log(p(y|\theta))}{\partial^2 \theta} |\theta] &amp;= \frac{-n}{\theta} - \frac{n}{1-\theta}\\
- E[\frac{\partial^2 \log(p(y|\theta))}{\partial^2 \theta} |\theta] &amp;= \frac{n}{\theta} + \frac{n}{1-\theta}\\
I(\theta) &amp;= \frac{n}{\theta} + \frac{n}{1-\theta}\\
          &amp;= \frac{n}{\theta(1-\theta)}\\
\end{align}\]</span></p>
<p><span class="math inline">\(\because\)</span> <span class="math inline">\(p_J \propto \sqrt{I(\theta)}\)</span></p>
<p><span class="math display">\[\begin{align}
  p_J(\theta) \propto &amp;\sqrt{I(\theta)}\\
                      &amp;= \sqrt{\frac{n}{\theta(1-\theta)}}\\
\end{align}\]</span></p>
<p>Let <span class="math inline">\(c\)</span> be the scalar. By the fact that <span class="math inline">\(\frac{d}{dx}(\sin^{-1}x)=\frac{1}{\sqrt{1-x^2}}\)</span>,</p>
<p><span class="math display">\[P_J (\theta) = c \times \sqrt{\frac{n}{\theta(1-\theta)}}\]</span></p>
<p><span class="math display">\[\begin{align}
1 &amp;= c \int^{1}_{0} \sqrt{\frac{n}{\theta(1-\theta)}} d\theta\\
1 &amp;= nc \int^{1}_{0} \sqrt{\frac{1}{\theta(1-\theta)}} d\theta\\
1 &amp;= nc \left[  -2 \sin^{-1}(\sqrt{1-x})   \right]^{1}_{0} \\
1 &amp;= -2\times nc (\underbrace{\sin^{-1}(0)}_{=0} - \underbrace{\sin^{-1}(1)}_{=\frac{\pi}{2}})\\
1 &amp;= \pi n c \\
c &amp;= \frac{1}{\pi n}\\
\end{align}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[p_J(\theta) = \frac{1}{\pi n} \sqrt{\frac{n}{\theta(1-\theta)}}\]</span></p>
<p><span id="eq-3-12-prior"><span class="math display">\[p_J(\theta) = \underline{\frac{1}{\pi \sqrt{n}}\frac{1}{\sqrt{\theta(1-\theta)}}} \tag{6.6}\]</span></span></p>
</section>
<section id="sec-3-12-b" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="sec-3-12-b"><span class="header-section-number">6.7.2</span> (b)</h3>
<blockquote class="blockquote">
<p>Reparameterize the binomial sampling model with <span class="math inline">\(\psi = \log \theta / (1-\theta)\)</span>, so that <span class="math inline">\(p(y|\psi) = {n\choose y} e^{\psi y} (1+e^{\psi})^{-n}\)</span>. Obtain Jefferys’ prior distribution <span class="math inline">\(p_J (\psi)\)</span> for this model.</p>
</blockquote>
<p><span class="math display">\[\begin{align}
  p(y|\psi) &amp;= {n\choose y} e^{\psi y} (1+e^{\psi})^{-n}\\
  \log(p(y|\psi)) &amp;= {n\choose y} + \psi y \underbrace{\log(e)}_{=1} - n\log(1+e^{\psi})\\
  \log(p(y|\psi)) &amp;= {n\choose y} + \psi y  - n\log(1+e^{\psi})\\
  \frac{\partial \log p(y|\psi)}{\partial \psi} &amp;= y - n\frac{e^{\psi}}{1+e^{\psi}}\\
  \frac{\partial^2 \log p(y|\psi)}{\partial^2 \psi} &amp;= -n \frac{e^{\psi}}{(1+e^{\psi})^2}\\
  E[ \frac{\partial^2 \log p(y|\psi)}{\partial^2 \psi} | \psi] &amp;= -n \frac{e^{\psi}}{(1+e^{\psi})^2}\\
  I(\psi) = -E[ \frac{\partial^2 \log p(y|\psi)}{\partial^2 \psi} | \psi] &amp;= n \frac{e^{\psi}}{(1+e^{\psi})^2}\\
\end{align}\]</span></p>
<p><span class="math inline">\(\therefore\)</span> <span class="math inline">\(p_{J}(\psi) \propto \sqrt{I(\psi)} = \sqrt{\frac{n e^{\psi}}{(1+e^{\psi})^2}}\)</span></p>
<p><span class="math display">\[p_{J}(\psi) \propto \frac{\sqrt{n e^{\psi}}}{1+e^{\psi}}\]</span></p>
</section>
<section id="c-1" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="c-1"><span class="header-section-number">6.7.3</span> (c)</h3>
<blockquote class="blockquote">
<p>Take the prior distribution from <a href="#sec-3-12-a">(a)</a> and apply the change of variables formula from Exercise 3.10 to obtain the induced prior density on <span class="math inline">\(\psi\)</span>.</p>
<p>This density should be the same as the one derived in part <a href="#sec-3-12-b">(b)</a> of this exercise. This consistency under reparameterization is the defining characteristic of Jeffrey’s’ prior.</p>
</blockquote>
<p><span class="math display">\[\psi = g(\theta) = \log[\frac{\theta}{1-\theta}]\]</span></p>
<p><span class="math display">\[\theta = h(\psi) = \frac{e^{\psi}}{1+e^{\psi}}\]</span></p>
<p>From <a href="#eq-3-12-prior">Equation&nbsp;<span>6.6</span></a>, <span class="math inline">\(p_{\theta}(\theta) = \frac{1}{\pi \sqrt{n}}\frac{1}{\sqrt{\theta(1-\theta)}}\)</span>,</p>
<p><span class="math display">\[\begin{align}
  p_{\psi}(\psi) &amp;= \frac{1}{\pi \sqrt{n}} p_{\theta}(h(\psi)) \times |\frac{dh}{d\psi}|\\
                 &amp;=  \frac{1}{\pi \sqrt{n}} \frac{1+e^{\psi}}{\sqrt{e^{\psi}(1+e^{\psi}-e^{\psi})}}\times \frac{e^{\psi}}{(1+e^{\psi})^2}\\
                 &amp;= \frac{1}{\pi \sqrt{n}} \frac{1+e^{\psi}}{\sqrt{e^{\psi}}}\times \frac{e^{\psi}}{(1+e^{\psi})^2}\\
                 &amp;= \frac{1}{\pi\sqrt{n}} \frac{\sqrt{e^{\psi}}}{1+e^{\psi}}\\
                 &amp;\propto \frac{\sqrt{e^{\psi}}}{1+e^{\psi}}\\
                 &amp;\propto p_{J}(\psi)
\end{align}\]</span></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-hoff2009first" class="csl-entry" role="doc-biblioentry">
Hoff, Peter D. 2009. <em>A First Course in Bayesian Statistical Methods</em>. Vol. 580. Springer.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>https://www.wolframalpha.com/input?i=%283<em>x</em>%281-x%29%5E7+%2B+x%5E7+<em>+%281-x%29%29+</em>x%5E15+*+%281-x%29%5E28<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../hw/hw1.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Homework 1</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../hw/hw3.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Homework 3</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb28" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Homework 2</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> </span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Shao-Ting Chiu (UIN:433002162)</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co">    url: stchiu@email.tamu.edu</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: Department of Electrical and Computer Engineering, Texas A\&amp;M University</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> ../ref.bib</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3  </span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span><span class="co"> </span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="co">    echo: true</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a><span class="co">    freeze: auto</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="fu">## Homework Description</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Read Chapter 3 in the Hoff book.</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Then, do the following exercises in Hoff: 3.1, 3.3, 3.4, 3.7, 3.12.</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; For problems that require a computer, please do and derive as much as possible "on paper," and include these derivations in your submission file. Then, for the parts that do require the use of a computer (e.g., creating plots), you are free to use any software (R, Python, ...) of your choosing; no need to include your code in your write-up. Please make sure you create a single file for submission here on Canvas.</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; For computations involving gamma functions (e.g., 3.7), it is often helpful to work with log-gamma functions instead, to avoid numbers that are too large to be represented by a computer. In R, the functions lbeta() and lgamma() compute the (natural) log of the beta and gamma functions, respectively. See more here: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Special.html</span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">PDF version</span><span class="co">](https://github.com/stevengogogo/STAT638_Applied-Bayes-Methods/blob/hw/hw2.pdf)</span></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Deadline: <span class="in">`Sep 20 by 12:01pm`</span></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational Enviromnent Setup</span></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a><span class="fu">### Third-party libraries</span></span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys <span class="co"># system information</span></span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="co"># plotting</span></span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="co"># scientific computing</span></span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd <span class="co"># data managing</span></span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> comb</span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats <span class="im">as</span> st</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> gamma</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> comb</span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Matplotlib setting</span></span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'text.usetex'</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a>matplotlib.rcParams[<span class="st">'figure.dpi'</span>]<span class="op">=</span> <span class="dv">300</span></span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a><span class="fu">### Version</span></span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sys.version)</span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(matplotlib.__version__)</span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(scipy.__version__)</span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.__version__)</span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pd.__version__)</span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a><span class="fu">## Problem 3.1</span></span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-64"><a href="#cb28-64" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Sample survey: Suppose we are going to sample 100 individuals from a county (of size much larger than 100) and ask each sampled person whether they support policy $Z$ or not. Let $Y_i = 1$ if person $i$ in the sample supports the policy, and $Y_i = 0$ otherwise.</span></span>
<span id="cb28-65"><a href="#cb28-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-66"><a href="#cb28-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-67"><a href="#cb28-67" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a) </span></span>
<span id="cb28-68"><a href="#cb28-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-69"><a href="#cb28-69" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Assume $Y_1,\dots, Y_{100}$ are, conditional on $\theta$, i.i.d. binary random variables with expectation $\theta$. Write down the joint distribution of $Pr(Y_1 =y1,\dots, Y_{100} = y_{100}|\theta)$ in a compact form. Also write down the form of $Pr(\sum Y_i = y|\theta)$.</span></span>
<span id="cb28-70"><a href="#cb28-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-71"><a href="#cb28-71" aria-hidden="true" tabindex="-1"></a>$$Pr(Y_1 = y_1,\dots, Y_{100}=y_{100}|\theta) = \underline{\theta^{\sum_{u=1}^{100}}(1-\theta)^{100-\sum_{u=1}^{100}}}$$</span>
<span id="cb28-72"><a href="#cb28-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-73"><a href="#cb28-73" aria-hidden="true" tabindex="-1"></a>$$Pr(\sum_{i=1}^{100} Y_i = y |\theta)= \underline{{100 \choose y}\theta^{y}(1-\theta)^{100-y}}$$ {#eq-sum-prob}</span>
<span id="cb28-74"><a href="#cb28-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-75"><a href="#cb28-75" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b) </span></span>
<span id="cb28-76"><a href="#cb28-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-77"><a href="#cb28-77" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; For the moment, suppose you believed that $\theta\in</span><span class="sc">\{</span><span class="at">0.0,0.1,\dots,0.9,1.0</span><span class="sc">\}</span><span class="at">$. Given that the results of the survey were $\sum^{100}_{i=1} Y_i = 57$, compute $Pr(\sum Y_{i} = 57|\theta)$ for each of these 11 values of $\theta$ and plot these probabilities as a function of $\theta$</span></span>
<span id="cb28-78"><a href="#cb28-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-79"><a href="#cb28-79" aria-hidden="true" tabindex="-1"></a>From @eq-sum-prob, the sum of supports ($y$) is on the power term. Thus, directly computation is problematic with limited range of floating number. Converting probability to log scale is a way to bypass this problem.Another way is to use <span class="in">`scipy.stats.binom`</span> function<span class="ot">[^binom]</span></span>
<span id="cb28-80"><a href="#cb28-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-81"><a href="#cb28-81" aria-hidden="true" tabindex="-1"></a>The distribution of $Pr(\sum_{i=1}^{100} Y_i = y |\theta)$ along with $\theta\in<span class="sc">\{</span>0.0,0.1,\dots,0.9,1.0<span class="sc">\}</span>$ is shown in @tbl-binom. The plot of distribution is shown in @fig-binom.</span>
<span id="cb28-82"><a href="#cb28-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-83"><a href="#cb28-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-84"><a href="#cb28-84" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-85"><a href="#cb28-85" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-binom</span></span>
<span id="cb28-86"><a href="#cb28-86" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Probabilities along with priors"</span></span>
<span id="cb28-87"><a href="#cb28-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-88"><a href="#cb28-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-89"><a href="#cb28-89" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="fl">0.0</span>,<span class="fl">1.0</span>,<span class="dv">11</span>)</span>
<span id="cb28-90"><a href="#cb28-90" aria-hidden="true" tabindex="-1"></a>tot <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb28-91"><a href="#cb28-91" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> np.zeros(<span class="bu">len</span>(thetas))</span>
<span id="cb28-92"><a href="#cb28-92" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> <span class="dv">57</span></span>
<span id="cb28-93"><a href="#cb28-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-94"><a href="#cb28-94" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i, theta) <span class="kw">in</span> <span class="bu">enumerate</span>(thetas):</span>
<span id="cb28-95"><a href="#cb28-95" aria-hidden="true" tabindex="-1"></a>  probs[i] <span class="op">=</span> st.binom.pmf(count, tot, theta)</span>
<span id="cb28-96"><a href="#cb28-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-97"><a href="#cb28-97" aria-hidden="true" tabindex="-1"></a><span class="co"># list of probabilities</span></span>
<span id="cb28-98"><a href="#cb28-98" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Theta"</span>: thetas, <span class="st">"posteriori"</span>: probs})</span>
<span id="cb28-99"><a href="#cb28-99" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-100"><a href="#cb28-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-101"><a href="#cb28-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-102"><a href="#cb28-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-103"><a href="#cb28-103" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-104"><a href="#cb28-104" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-binom</span></span>
<span id="cb28-105"><a href="#cb28-105" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Probabilities along with priors"</span></span>
<span id="cb28-106"><a href="#cb28-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-107"><a href="#cb28-107" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, probs, <span class="st">'ko'</span>)<span class="op">;</span></span>
<span id="cb28-108"><a href="#cb28-108" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)<span class="op">;</span></span>
<span id="cb28-109"><a href="#cb28-109" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$Pr(\sum Y_i = 57 |\theta)$"</span>)<span class="op">;</span></span>
<span id="cb28-110"><a href="#cb28-110" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-111"><a href="#cb28-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-112"><a href="#cb28-112" aria-hidden="true" tabindex="-1"></a><span class="ot">[^binom]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html</span></span>
<span id="cb28-113"><a href="#cb28-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-114"><a href="#cb28-114" aria-hidden="true" tabindex="-1"></a><span class="fu">### (c) {#sec-3-1-c}</span></span>
<span id="cb28-115"><a href="#cb28-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-116"><a href="#cb28-116" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Now suppose you originally had no prior information to believe one of these $\theta$-values over another, and so $Pr(\theta=0.0)=Pr(\theta=0.1)=\dots=Pr(\theta=0.9)=Pr(\theta=1.0)$. Use Bayes' rule to compute $p(\theta|\sum^{n}_{i=1} Y_i = 57)$ for each $\theta$-value. Make a plot of this posterior distribtution as a function of $\theta$.</span></span>
<span id="cb28-117"><a href="#cb28-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-118"><a href="#cb28-118" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-119"><a href="#cb28-119" aria-hidden="true" tabindex="-1"></a>  p(\theta_i |\sum^{n}_{i=1} Y_i = 57) &amp;= \frac{p(\sum^{n}_{i=1} Y_i = 57|\theta)p(\theta_i)}{p(\sum^{n}_{i=1} Y_i = 57)}<span class="sc">\\</span></span>
<span id="cb28-120"><a href="#cb28-120" aria-hidden="true" tabindex="-1"></a>  &amp;= \frac{p(\sum^{n}_{i=1} Y_i = 57|\theta)p(\theta_i)}{\sum_{\theta\in\Theta}p(\sum^{n}_{i=1} Y_i = 57 | \theta)p(\theta)}</span>
<span id="cb28-121"><a href="#cb28-121" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-122"><a href="#cb28-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-123"><a href="#cb28-123" aria-hidden="true" tabindex="-1"></a>The following is the calculation of the posterior distribution (shown in @tbl-post-binom), and the result is shown in @fig-post-binom.</span>
<span id="cb28-124"><a href="#cb28-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-125"><a href="#cb28-125" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-126"><a href="#cb28-126" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-post-binom</span></span>
<span id="cb28-127"><a href="#cb28-127" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Posterior distribution depends on discrete uniform distribution of theta."</span></span>
<span id="cb28-128"><a href="#cb28-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-129"><a href="#cb28-129" aria-hidden="true" tabindex="-1"></a>p_theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span><span class="bu">len</span>(thetas)</span>
<span id="cb28-130"><a href="#cb28-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-131"><a href="#cb28-131" aria-hidden="true" tabindex="-1"></a>p_y <span class="op">=</span> np.<span class="bu">sum</span>( probs<span class="op">*</span>p_theta)</span>
<span id="cb28-132"><a href="#cb28-132" aria-hidden="true" tabindex="-1"></a>post_theta <span class="op">=</span> np.zeros(<span class="bu">len</span>(thetas))</span>
<span id="cb28-133"><a href="#cb28-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-134"><a href="#cb28-134" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i, theta) <span class="kw">in</span> <span class="bu">enumerate</span>(thetas):</span>
<span id="cb28-135"><a href="#cb28-135" aria-hidden="true" tabindex="-1"></a>  post_theta[i] <span class="op">=</span> probs[i]<span class="op">*</span>p_theta<span class="op">/</span>p_y</span>
<span id="cb28-136"><a href="#cb28-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-137"><a href="#cb28-137" aria-hidden="true" tabindex="-1"></a><span class="co"># list of probabilities</span></span>
<span id="cb28-138"><a href="#cb28-138" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Theta"</span>: thetas, <span class="st">"posteriori"</span>:post_theta})</span>
<span id="cb28-139"><a href="#cb28-139" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-140"><a href="#cb28-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-141"><a href="#cb28-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-142"><a href="#cb28-142" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-143"><a href="#cb28-143" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-post-binom</span></span>
<span id="cb28-144"><a href="#cb28-144" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Posterior distribution as a function of theta."</span></span>
<span id="cb28-145"><a href="#cb28-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-146"><a href="#cb28-146" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, post_theta, <span class="st">'ko'</span>)<span class="op">;</span></span>
<span id="cb28-147"><a href="#cb28-147" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)<span class="op">;</span></span>
<span id="cb28-148"><a href="#cb28-148" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$p(\theta_i |\sum^</span><span class="sc">{n}</span><span class="vs">_{i=1} Y_i = 57)$"</span>)<span class="op">;</span></span>
<span id="cb28-149"><a href="#cb28-149" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-150"><a href="#cb28-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-151"><a href="#cb28-151" aria-hidden="true" tabindex="-1"></a><span class="fu">### (d) </span></span>
<span id="cb28-152"><a href="#cb28-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-153"><a href="#cb28-153" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Now suppose you allow $\theta$ to be any value in the interval $</span><span class="co">[</span><span class="ot">0,1</span><span class="co">]</span><span class="at">$. Using the uniform prior density for $\theta$, so that $p(\theta) = 1$, plot the posterior density $p(\theta) \times Pr(\sum^{n}_{i=1} Y_i = 57 |\theta)$ as a function of $\theta$.</span></span>
<span id="cb28-154"><a href="#cb28-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-155"><a href="#cb28-155" aria-hidden="true" tabindex="-1"></a>As shown in @fig-post-binom-cont.</span>
<span id="cb28-156"><a href="#cb28-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-157"><a href="#cb28-157" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-158"><a href="#cb28-158" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-post-binom-cont</span></span>
<span id="cb28-159"><a href="#cb28-159" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Posterior distribution with continouous uniform prior."</span></span>
<span id="cb28-160"><a href="#cb28-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-161"><a href="#cb28-161" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb28-162"><a href="#cb28-162" aria-hidden="true" tabindex="-1"></a>p_theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span><span class="bu">len</span>(thetas)</span>
<span id="cb28-163"><a href="#cb28-163" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> np.zeros(<span class="bu">len</span>(thetas))</span>
<span id="cb28-164"><a href="#cb28-164" aria-hidden="true" tabindex="-1"></a>post_theta <span class="op">=</span> np.zeros(<span class="bu">len</span>(thetas))</span>
<span id="cb28-165"><a href="#cb28-165" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> <span class="dv">57</span></span>
<span id="cb28-166"><a href="#cb28-166" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i, theta) <span class="kw">in</span> <span class="bu">enumerate</span>(thetas):</span>
<span id="cb28-167"><a href="#cb28-167" aria-hidden="true" tabindex="-1"></a>  probs[i] <span class="op">=</span> st.binom.pmf(count, tot, theta)</span>
<span id="cb28-168"><a href="#cb28-168" aria-hidden="true" tabindex="-1"></a>  post_theta[i] <span class="op">=</span> probs[i]</span>
<span id="cb28-169"><a href="#cb28-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-170"><a href="#cb28-170" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting  </span></span>
<span id="cb28-171"><a href="#cb28-171" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, post_theta, <span class="st">'k-'</span>)<span class="op">;</span></span>
<span id="cb28-172"><a href="#cb28-172" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)<span class="op">;</span></span>
<span id="cb28-173"><a href="#cb28-173" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$p(\theta_i |\sum^</span><span class="sc">{n}</span><span class="vs">_{i=1} Y_i = 57)$"</span>)<span class="op">;</span></span>
<span id="cb28-174"><a href="#cb28-174" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-175"><a href="#cb28-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-176"><a href="#cb28-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-177"><a href="#cb28-177" aria-hidden="true" tabindex="-1"></a><span class="fu">### (e) </span></span>
<span id="cb28-178"><a href="#cb28-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-179"><a href="#cb28-179" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; As discussed in this chapter, the posterior distribution of $\theta$ is $beta(1+57, 1+100-57)$. Plot the posterior density as a function of $\theta$. Discuss the relationships among all of the plots you have made for this exercise.</span></span>
<span id="cb28-180"><a href="#cb28-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-181"><a href="#cb28-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-182"><a href="#cb28-182" aria-hidden="true" tabindex="-1"></a>The $\theta$ with beta distribution is plotted in @fig-post-beta-cont.</span>
<span id="cb28-183"><a href="#cb28-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-184"><a href="#cb28-184" aria-hidden="true" tabindex="-1"></a>@fig-post-binom is the normalized probability via Bayes' rule (@sec-3-1-c). On the other hand, @fig-binom is not normalized.</span>
<span id="cb28-185"><a href="#cb28-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-186"><a href="#cb28-186" aria-hidden="true" tabindex="-1"></a>@fig-post-binom-cont and @fig-post-beta-cont has similar distribution, which means the prior $\theta$ has little influcence on the posterior distribution. This is because the sample number is large ($n=57$), and decrease the importance of the prior.</span>
<span id="cb28-187"><a href="#cb28-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-188"><a href="#cb28-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-189"><a href="#cb28-189" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-190"><a href="#cb28-190" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-post-beta-cont</span></span>
<span id="cb28-191"><a href="#cb28-191" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Posterior distribution with continouous beta prior."</span></span>
<span id="cb28-192"><a href="#cb28-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-193"><a href="#cb28-193" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dv">3000</span>)</span>
<span id="cb28-194"><a href="#cb28-194" aria-hidden="true" tabindex="-1"></a>thetas_rv <span class="op">=</span> st.beta(<span class="dv">1</span><span class="op">+</span><span class="dv">57</span>, <span class="dv">1</span><span class="op">+</span><span class="dv">100</span><span class="op">-</span><span class="dv">57</span>)</span>
<span id="cb28-195"><a href="#cb28-195" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> [thetas_rv.pdf(x) <span class="cf">for</span> x <span class="kw">in</span> grid]</span>
<span id="cb28-196"><a href="#cb28-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-197"><a href="#cb28-197" aria-hidden="true" tabindex="-1"></a>p_theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span><span class="bu">len</span>(thetas)</span>
<span id="cb28-198"><a href="#cb28-198" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> np.zeros(<span class="bu">len</span>(thetas))</span>
<span id="cb28-199"><a href="#cb28-199" aria-hidden="true" tabindex="-1"></a>post_theta <span class="op">=</span> np.zeros(<span class="bu">len</span>(thetas))</span>
<span id="cb28-200"><a href="#cb28-200" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> <span class="dv">57</span></span>
<span id="cb28-201"><a href="#cb28-201" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i, theta) <span class="kw">in</span> <span class="bu">enumerate</span>(thetas):</span>
<span id="cb28-202"><a href="#cb28-202" aria-hidden="true" tabindex="-1"></a>  probs[i] <span class="op">=</span> st.binom.pmf(count, tot, theta)</span>
<span id="cb28-203"><a href="#cb28-203" aria-hidden="true" tabindex="-1"></a>  post_theta[i] <span class="op">=</span> probs[i]</span>
<span id="cb28-204"><a href="#cb28-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-205"><a href="#cb28-205" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting  </span></span>
<span id="cb28-206"><a href="#cb28-206" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, post_theta, <span class="st">'k-'</span>)<span class="op">;</span></span>
<span id="cb28-207"><a href="#cb28-207" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta\sim beta$"</span>)<span class="op">;</span></span>
<span id="cb28-208"><a href="#cb28-208" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$p(\theta_i |\sum^</span><span class="sc">{n}</span><span class="vs">_{i=1} Y_i = 57)$"</span>)<span class="op">;</span></span>
<span id="cb28-209"><a href="#cb28-209" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-210"><a href="#cb28-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-211"><a href="#cb28-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-212"><a href="#cb28-212" aria-hidden="true" tabindex="-1"></a><span class="fu">## Problem 3.3 {#sec-p-3-3}</span></span>
<span id="cb28-213"><a href="#cb28-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-214"><a href="#cb28-214" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Tumor counts: A cancer laboratory is estimating the rate of tumorigenesis in two strains of mice, A and B. They have tumor count data for 10 mice in strain A and 13 mice in strain B. Type A mice have been well studied, and information from other laboratories suggests that type A mice have tumor counts that are approximately Poisson-distributed with a mean of 12. Tumor count rates for type B mice are unknown, but type B mice are related to type A mice. The observed tumor counts for the two populations are</span></span>
<span id="cb28-215"><a href="#cb28-215" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $$\mathcal{y}_{A} = (12, 9, 12, 14, 13, 13, 15, 8, 15, 6);$$</span></span>
<span id="cb28-216"><a href="#cb28-216" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $$\mathcal{y}_{B} = (11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7).$$</span></span>
<span id="cb28-217"><a href="#cb28-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-218"><a href="#cb28-218" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a) </span></span>
<span id="cb28-219"><a href="#cb28-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-220"><a href="#cb28-220" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Find the posterior distributions, means, variances and $95\%$ quantile-based confidence intervals for $\theta_A$ and $\theta_B$, assuming a Poisson sampling distribution for each group and the following prior distribution:</span></span>
<span id="cb28-221"><a href="#cb28-221" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $\theta_A \sim gamma(120,10), \theta_B \sim gamma(12,1), p(\theta_A, \theta_B) = p(\theta_A) \times p(\theta_B)$</span></span>
<span id="cb28-222"><a href="#cb28-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-223"><a href="#cb28-223" aria-hidden="true" tabindex="-1"></a>According to @hoff2009first <span class="co">[</span><span class="ot">pp. 46-47</span><span class="co">]</span>,</span>
<span id="cb28-224"><a href="#cb28-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-225"><a href="#cb28-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-226"><a href="#cb28-226" aria-hidden="true" tabindex="-1"></a>$$E<span class="co">[</span><span class="ot">\theta_{*} | y_1,\dots, y_{n_{*}}</span><span class="co">]</span> = \frac{a_* + \sum_{i=1}^{n_*} y_{i}}{b_* + n_*}$$</span>
<span id="cb28-227"><a href="#cb28-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-228"><a href="#cb28-228" aria-hidden="true" tabindex="-1"></a>where $*\in <span class="sc">\{</span>A, B<span class="sc">\}</span>$. Given</span>
<span id="cb28-229"><a href="#cb28-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-230"><a href="#cb28-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-231"><a href="#cb28-231" aria-hidden="true" tabindex="-1"></a>$\begin{cases}</span>
<span id="cb28-232"><a href="#cb28-232" aria-hidden="true" tabindex="-1"></a>    \theta_{*} &amp;\sim gamma(a_*,b_*)<span class="sc">\\</span></span>
<span id="cb28-233"><a href="#cb28-233" aria-hidden="true" tabindex="-1"></a>    Y_1,\dots, Y_{n_*}|\theta_{*} &amp;\sim Poisson(\theta_{*})</span>
<span id="cb28-234"><a href="#cb28-234" aria-hidden="true" tabindex="-1"></a>\end{cases}$</span>
<span id="cb28-235"><a href="#cb28-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-236"><a href="#cb28-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-237"><a href="#cb28-237" aria-hidden="true" tabindex="-1"></a>$$\Rightarrow<span class="sc">\{</span>\theta_{i}|Y_1,\dots,Y_{n_*}\}\sim gamma(a + \sum^{n_*}_{i=1} Y_i, b_* + n_*)$$ {#eq-gamma-conj}</span>
<span id="cb28-238"><a href="#cb28-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-239"><a href="#cb28-239" aria-hidden="true" tabindex="-1"></a>The properties of Gamma distribution <span class="co">[</span><span class="ot">@hoff2009first pp. 45-46</span><span class="co">]</span>,</span>
<span id="cb28-240"><a href="#cb28-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-241"><a href="#cb28-241" aria-hidden="true" tabindex="-1"></a>$$p(\theta) = \frac{b^a}{\Gamma(a)}\theta^{a-1}e^{-b\theta}, \quad \theta,a,b &gt; 0$$ </span>
<span id="cb28-242"><a href="#cb28-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-243"><a href="#cb28-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-244"><a href="#cb28-244" aria-hidden="true" tabindex="-1"></a>$$E<span class="co">[</span><span class="ot">\theta</span><span class="co">]</span> = \frac{a}{b}$$ {#eq-gamma-mean}</span>
<span id="cb28-245"><a href="#cb28-245" aria-hidden="true" tabindex="-1"></a>$$Var<span class="co">[</span><span class="ot">\theta</span><span class="co">]</span> = \frac{a}{b^2}$$ {#eq-gamma-var}</span>
<span id="cb28-246"><a href="#cb28-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-247"><a href="#cb28-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-248"><a href="#cb28-248" aria-hidden="true" tabindex="-1"></a>**Type A Mice**</span>
<span id="cb28-249"><a href="#cb28-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-250"><a href="#cb28-250" aria-hidden="true" tabindex="-1"></a>|Parameter|Value|</span>
<span id="cb28-251"><a href="#cb28-251" aria-hidden="true" tabindex="-1"></a>|---|---|</span>
<span id="cb28-252"><a href="#cb28-252" aria-hidden="true" tabindex="-1"></a>|$a_A$|120|</span>
<span id="cb28-253"><a href="#cb28-253" aria-hidden="true" tabindex="-1"></a>|$b_A$|10|</span>
<span id="cb28-254"><a href="#cb28-254" aria-hidden="true" tabindex="-1"></a>|$n_A$|10|</span>
<span id="cb28-255"><a href="#cb28-255" aria-hidden="true" tabindex="-1"></a>|$\sum_{i=1}^{n_{A}y_{i}}$|$12+9+12+14+13+13+15+8+15+6=117$|</span>
<span id="cb28-256"><a href="#cb28-256" aria-hidden="true" tabindex="-1"></a>: Parameters of type A mice {#tbl-type-a-mice}</span>
<span id="cb28-257"><a href="#cb28-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-258"><a href="#cb28-258" aria-hidden="true" tabindex="-1"></a>The posterior distribution of mice A:</span>
<span id="cb28-259"><a href="#cb28-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-260"><a href="#cb28-260" aria-hidden="true" tabindex="-1"></a>$$<span class="sc">\{</span>\theta_A|Y_1,\dots,Y_{n_A} \sim gamma(120 + 117, 10+10)= gamma(237,20)<span class="sc">\}</span> $$</span>
<span id="cb28-261"><a href="#cb28-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-262"><a href="#cb28-262" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$E<span class="co">[</span><span class="ot">\theta_{A}|\sum_{i=1}^{n_{A}} Y_{i}</span><span class="co">]</span> = \frac{237}{20}= \underline{11.85}$</span>
<span id="cb28-263"><a href="#cb28-263" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Var<span class="co">[</span><span class="ot">\theta_{A}|\sum_{i=1}^{n_{A}} Y_{i}</span><span class="co">]</span> = \frac{237}{20^2}\approx \underline{0.59}$</span>
<span id="cb28-264"><a href="#cb28-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-265"><a href="#cb28-265" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$95\%$ quantile-based confidence intervals is shown in @tbl-33a</span>
<span id="cb28-266"><a href="#cb28-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-267"><a href="#cb28-267" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-268"><a href="#cb28-268" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-33a</span></span>
<span id="cb28-269"><a href="#cb28-269" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "95% quantile-based confidence intervals of mice A."</span></span>
<span id="cb28-270"><a href="#cb28-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-271"><a href="#cb28-271" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> interval_gamma_95(a,b):</span>
<span id="cb28-272"><a href="#cb28-272" aria-hidden="true" tabindex="-1"></a>  rvA <span class="op">=</span> st.gamma(a, scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span>b)</span>
<span id="cb28-273"><a href="#cb28-273" aria-hidden="true" tabindex="-1"></a>  ints <span class="op">=</span> rvA.interval(<span class="fl">0.95</span>)</span>
<span id="cb28-274"><a href="#cb28-274" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> pd.DataFrame({<span class="st">"Left bound"</span>:[ints[<span class="dv">0</span>]], <span class="st">"Right bound"</span>:[ints[<span class="dv">1</span>]]})</span>
<span id="cb28-275"><a href="#cb28-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-276"><a href="#cb28-276" aria-hidden="true" tabindex="-1"></a>aA <span class="op">=</span> <span class="dv">237</span></span>
<span id="cb28-277"><a href="#cb28-277" aria-hidden="true" tabindex="-1"></a>bA <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb28-278"><a href="#cb28-278" aria-hidden="true" tabindex="-1"></a>interval_gamma_95(aA,bA)</span>
<span id="cb28-279"><a href="#cb28-279" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-280"><a href="#cb28-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-281"><a href="#cb28-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-282"><a href="#cb28-282" aria-hidden="true" tabindex="-1"></a>**Type B Mice**</span>
<span id="cb28-283"><a href="#cb28-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-284"><a href="#cb28-284" aria-hidden="true" tabindex="-1"></a>similarly,</span>
<span id="cb28-285"><a href="#cb28-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-286"><a href="#cb28-286" aria-hidden="true" tabindex="-1"></a>|Parameter|Value|</span>
<span id="cb28-287"><a href="#cb28-287" aria-hidden="true" tabindex="-1"></a>|---|---|</span>
<span id="cb28-288"><a href="#cb28-288" aria-hidden="true" tabindex="-1"></a>|$a_B$|12|</span>
<span id="cb28-289"><a href="#cb28-289" aria-hidden="true" tabindex="-1"></a>|$b_B$|1|</span>
<span id="cb28-290"><a href="#cb28-290" aria-hidden="true" tabindex="-1"></a>|$n_B$|13|</span>
<span id="cb28-291"><a href="#cb28-291" aria-hidden="true" tabindex="-1"></a>|$\sum_{i=1}^{n_{B}y_{i}}$|$11+11+10+9+9+8+7+10+6+8+8+9+7=113$|</span>
<span id="cb28-292"><a href="#cb28-292" aria-hidden="true" tabindex="-1"></a>: Parameters of type B mice {#tbl-type-b-mice}</span>
<span id="cb28-293"><a href="#cb28-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-294"><a href="#cb28-294" aria-hidden="true" tabindex="-1"></a>The posterior distribution of mice B:</span>
<span id="cb28-295"><a href="#cb28-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-296"><a href="#cb28-296" aria-hidden="true" tabindex="-1"></a>$$<span class="sc">\{</span>\theta_B|Y_1,\dots,Y_{n_B} \sim gamma(12+113, 1+13)= gamma(125, 14)<span class="sc">\}</span> $$</span>
<span id="cb28-297"><a href="#cb28-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-298"><a href="#cb28-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$E<span class="co">[</span><span class="ot">\theta_{B}|\sum_{i=1}^{n_{B}} Y_{i}</span><span class="co">]</span> = \frac{125}{14} \approx \underline{8.93}$</span>
<span id="cb28-299"><a href="#cb28-299" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Var<span class="co">[</span><span class="ot">\theta_{B}|\sum_{i=1}^{n_{B}} Y_{i}</span><span class="co">]</span> = \frac{125}{14^2}\approx \underline{0.64}$</span>
<span id="cb28-300"><a href="#cb28-300" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$95\%$ quantile-based confidence intervals is shown in @tbl-33b</span>
<span id="cb28-301"><a href="#cb28-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-302"><a href="#cb28-302" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-303"><a href="#cb28-303" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-33b</span></span>
<span id="cb28-304"><a href="#cb28-304" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "95% quantile-based confidence intervals of mice B."</span></span>
<span id="cb28-305"><a href="#cb28-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-306"><a href="#cb28-306" aria-hidden="true" tabindex="-1"></a>aB <span class="op">=</span> <span class="dv">125</span></span>
<span id="cb28-307"><a href="#cb28-307" aria-hidden="true" tabindex="-1"></a>bB <span class="op">=</span> <span class="dv">14</span></span>
<span id="cb28-308"><a href="#cb28-308" aria-hidden="true" tabindex="-1"></a>interval_gamma_95(aB,bB)</span>
<span id="cb28-309"><a href="#cb28-309" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-310"><a href="#cb28-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-311"><a href="#cb28-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-312"><a href="#cb28-312" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b)</span></span>
<span id="cb28-313"><a href="#cb28-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-314"><a href="#cb28-314" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Computing and plot the posterior expectation of $\theta_B$ under the prior distribution $\theta_B \sim gamma(12\times n_0, n_0)$ for each value of $n_0\in </span><span class="sc">\{</span><span class="at">1,2,\dots, 50</span><span class="sc">\}</span><span class="at">$.</span></span>
<span id="cb28-315"><a href="#cb28-315" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Descirbe what sort of prior beliefs about $\theta_B$ to be close to that of $\theta_A$.</span></span>
<span id="cb28-316"><a href="#cb28-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-317"><a href="#cb28-317" aria-hidden="true" tabindex="-1"></a>The posterior distribution can be derived from @eq-gamma-conj. As shown in @fig-3-3-b, the mean value of $\theta_B$ with $n_0$ close to $50$ is necessary to have the similar posterior mean as $\theta_A$.</span>
<span id="cb28-318"><a href="#cb28-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-319"><a href="#cb28-319" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-320"><a href="#cb28-320" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-3-3-b</span></span>
<span id="cb28-321"><a href="#cb28-321" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Mean of Posterior distribution of mice B with given n0s."</span></span>
<span id="cb28-322"><a href="#cb28-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-323"><a href="#cb28-323" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> post_gamma(a,b, sumY, n):</span>
<span id="cb28-324"><a href="#cb28-324" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> st.gamma(a<span class="op">+</span>sumY, scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span>(b <span class="op">+</span> n))</span>
<span id="cb28-325"><a href="#cb28-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-326"><a href="#cb28-326" aria-hidden="true" tabindex="-1"></a>n0s <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">50</span>, <span class="dv">1</span>)</span>
<span id="cb28-327"><a href="#cb28-327" aria-hidden="true" tabindex="-1"></a>sumYB <span class="op">=</span> <span class="dv">11</span><span class="op">+</span><span class="dv">11</span><span class="op">+</span><span class="dv">10</span><span class="op">+</span><span class="dv">9</span><span class="op">+</span><span class="dv">9</span><span class="op">+</span><span class="dv">8</span><span class="op">+</span><span class="dv">7</span><span class="op">+</span><span class="dv">10</span><span class="op">+</span><span class="dv">6</span><span class="op">+</span><span class="dv">8</span><span class="op">+</span><span class="dv">8</span><span class="op">+</span><span class="dv">9</span><span class="op">+</span><span class="dv">7</span></span>
<span id="cb28-328"><a href="#cb28-328" aria-hidden="true" tabindex="-1"></a>nB <span class="op">=</span> <span class="dv">13</span></span>
<span id="cb28-329"><a href="#cb28-329" aria-hidden="true" tabindex="-1"></a>post_theta_rvBs <span class="op">=</span> [post_gamma(<span class="dv">12</span><span class="op">*</span>n0, n0, sumYB, nB) <span class="cf">for</span> n0 <span class="kw">in</span> n0s]</span>
<span id="cb28-330"><a href="#cb28-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-331"><a href="#cb28-331" aria-hidden="true" tabindex="-1"></a>meanBs <span class="op">=</span> [post_theta_rvBs[i].mean() <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(n0s))]</span>
<span id="cb28-332"><a href="#cb28-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-333"><a href="#cb28-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-334"><a href="#cb28-334" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb28-335"><a href="#cb28-335" aria-hidden="true" tabindex="-1"></a>plt.plot(n0s, meanBs, <span class="st">"ko"</span>)</span>
<span id="cb28-336"><a href="#cb28-336" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$n_0$"</span>)</span>
<span id="cb28-337"><a href="#cb28-337" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$E[Pr(</span><span class="ch">\\</span><span class="st">theta_</span><span class="sc">{B}</span><span class="st">|y_</span><span class="sc">{B}</span><span class="st">)]$"</span>)<span class="op">;</span></span>
<span id="cb28-338"><a href="#cb28-338" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-339"><a href="#cb28-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-340"><a href="#cb28-340" aria-hidden="true" tabindex="-1"></a><span class="fu">### (c)</span></span>
<span id="cb28-341"><a href="#cb28-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-342"><a href="#cb28-342" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Should knowledge about population $A$ tell us anything about population $B$? Discuss whether or not it makes sense to have $p(\theta_A,\theta_B)=p(\theta_A)\times p(\theta_B)$.</span></span>
<span id="cb28-343"><a href="#cb28-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-344"><a href="#cb28-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-345"><a href="#cb28-345" aria-hidden="true" tabindex="-1"></a>The understanding of mice $A$ is well known. Though mice $B$ is related to mice $A$, there is possibility that mice $B$ is different from the distribution of $A$. Thus, viewing mice $A$ and mice $B$ with independent prior distribution makes sense.</span>
<span id="cb28-346"><a href="#cb28-346" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb28-347"><a href="#cb28-347" aria-hidden="true" tabindex="-1"></a><span class="fu">## Problem 3.4</span></span>
<span id="cb28-348"><a href="#cb28-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-349"><a href="#cb28-349" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Mixtures of beta priors: Estimate the probability $\theta$ of teen recidivism based on a study in which there were $n=43$ individuals released from incarceration and $y=15$ re-offenders within $36$ months.</span></span>
<span id="cb28-350"><a href="#cb28-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-351"><a href="#cb28-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-352"><a href="#cb28-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-353"><a href="#cb28-353" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a) {#sec-3-4-a}</span></span>
<span id="cb28-354"><a href="#cb28-354" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Using a $beta(2,8)$ prior for $\theta$, plot $p(\theta)$, $p(y|\theta)$ and $p(\theta|y)$ as functions of $\theta$. Find the posterior mean, mode, and standard deviation of $\theta$.</span></span>
<span id="cb28-355"><a href="#cb28-355" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Find a $95\%$ quantile-based condifence interval.</span></span>
<span id="cb28-356"><a href="#cb28-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-357"><a href="#cb28-357" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p(\theta) \sim beta(2,8)$</span>
<span id="cb28-358"><a href="#cb28-358" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Plotted in @fig-3-4-prior </span>
<span id="cb28-359"><a href="#cb28-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-360"><a href="#cb28-360" aria-hidden="true" tabindex="-1"></a>According to @hoff2009first pp. 37-38, the conjugate posterior ($<span class="sc">\{</span>\theta|Y=y<span class="sc">\}</span>$) given beta as prior is a beta distribution, and $Y\sim binomial(n,\theta)$</span>
<span id="cb28-361"><a href="#cb28-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-362"><a href="#cb28-362" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p(y|\theta) = {n\choose y}\theta^{y}(1-\theta)^{(n-y)}$</span>
<span id="cb28-363"><a href="#cb28-363" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Plotted in @fig-3-4-likelihood </span>
<span id="cb28-364"><a href="#cb28-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-365"><a href="#cb28-365" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p(\theta | y) \sim beta(a+y, b + n-y) = beta(2+15, 8 + 43 - 15) = beta(17, 36)$</span>
<span id="cb28-366"><a href="#cb28-366" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Plotted in @fig-3-4-posterior </span>
<span id="cb28-367"><a href="#cb28-367" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$E<span class="co">[</span><span class="ot">p(\theta | y)</span><span class="co">]</span> = \frac{a}{a+b} = \frac{17}{17+36} \approx 0.32$</span>
<span id="cb28-368"><a href="#cb28-368" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$Mode(p(\theta | y)) = \frac{a-1}{a+b-2} \approx 0.31$</span>
<span id="cb28-369"><a href="#cb28-369" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$std(p(\theta | y)) = \sqrt{var<span class="co">[</span><span class="ot">p(\theta | y)</span><span class="co">]</span>} = \sqrt{\frac{ab}{(a+b)^2 (a+b+1)}} =  \sqrt{\frac{17\times 36}{(17+36)^2 (17+36+1)}}\approx 0.06$</span>
<span id="cb28-370"><a href="#cb28-370" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Properties are shown in @tbl-3-4-posterior.</span>
<span id="cb28-371"><a href="#cb28-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-372"><a href="#cb28-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-373"><a href="#cb28-373" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-374"><a href="#cb28-374" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-3-4-prior</span></span>
<span id="cb28-375"><a href="#cb28-375" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Prior distribution."</span></span>
<span id="cb28-376"><a href="#cb28-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-377"><a href="#cb28-377" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb28-378"><a href="#cb28-378" aria-hidden="true" tabindex="-1"></a>rv_theta <span class="op">=</span> st.beta(<span class="dv">2</span>,<span class="dv">8</span>)</span>
<span id="cb28-379"><a href="#cb28-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-380"><a href="#cb28-380" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb28-381"><a href="#cb28-381" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, rv_theta.pdf(thetas), <span class="st">'k-'</span>)</span>
<span id="cb28-382"><a href="#cb28-382" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)<span class="op">;</span></span>
<span id="cb28-383"><a href="#cb28-383" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(</span><span class="ch">\\</span><span class="st">theta)$"</span>)<span class="op">;</span></span>
<span id="cb28-384"><a href="#cb28-384" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-385"><a href="#cb28-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-386"><a href="#cb28-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-387"><a href="#cb28-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-388"><a href="#cb28-388" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-389"><a href="#cb28-389" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-3-4-likelihood</span></span>
<span id="cb28-390"><a href="#cb28-390" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Likelihood"</span></span>
<span id="cb28-391"><a href="#cb28-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-392"><a href="#cb28-392" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb28-393"><a href="#cb28-393" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">43</span></span>
<span id="cb28-394"><a href="#cb28-394" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb28-395"><a href="#cb28-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-396"><a href="#cb28-396" aria-hidden="true" tabindex="-1"></a>pr_like <span class="op">=</span> [st.binom.pmf(y, n, theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas]</span>
<span id="cb28-397"><a href="#cb28-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-398"><a href="#cb28-398" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb28-399"><a href="#cb28-399" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, pr_like, <span class="st">'k-'</span>)</span>
<span id="cb28-400"><a href="#cb28-400" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)<span class="op">;</span></span>
<span id="cb28-401"><a href="#cb28-401" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(y|</span><span class="ch">\\</span><span class="st">theta)$"</span>)<span class="op">;</span></span>
<span id="cb28-402"><a href="#cb28-402" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-403"><a href="#cb28-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-404"><a href="#cb28-404" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-405"><a href="#cb28-405" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-3-4-posterior</span></span>
<span id="cb28-406"><a href="#cb28-406" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Posterior distribution"</span></span>
<span id="cb28-407"><a href="#cb28-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-408"><a href="#cb28-408" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb28-409"><a href="#cb28-409" aria-hidden="true" tabindex="-1"></a>rv_theta <span class="op">=</span> st.beta(<span class="dv">2</span><span class="op">+</span><span class="dv">15</span>, <span class="dv">8</span> <span class="op">+</span> <span class="dv">43</span> <span class="op">-</span> <span class="dv">15</span>)</span>
<span id="cb28-410"><a href="#cb28-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-411"><a href="#cb28-411" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb28-412"><a href="#cb28-412" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, rv_theta.pdf(thetas), <span class="st">'k-'</span>)</span>
<span id="cb28-413"><a href="#cb28-413" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)<span class="op">;</span></span>
<span id="cb28-414"><a href="#cb28-414" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(</span><span class="ch">\\</span><span class="st">theta | y)$"</span>)<span class="op">;</span></span>
<span id="cb28-415"><a href="#cb28-415" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-416"><a href="#cb28-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-417"><a href="#cb28-417" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-418"><a href="#cb28-418" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-3-4-posterior</span></span>
<span id="cb28-419"><a href="#cb28-419" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Properties of posterior distribution."</span></span>
<span id="cb28-420"><a href="#cb28-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-421"><a href="#cb28-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-422"><a href="#cb28-422" aria-hidden="true" tabindex="-1"></a>ints <span class="op">=</span> rv_theta.interval(<span class="fl">0.95</span>)</span>
<span id="cb28-423"><a href="#cb28-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-424"><a href="#cb28-424" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Properties"</span>: [<span class="st">"Left bound (CI)"</span>, <span class="st">"Right bound (CI)"</span>, <span class="st">"mean"</span>, <span class="st">"mode"</span>, <span class="st">"standard deviation"</span>], <span class="st">"Values"</span>: [ints[<span class="dv">0</span>], ints[<span class="dv">1</span>], rv_theta.mean(), (<span class="dv">17</span><span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>(<span class="dv">17</span><span class="op">+</span><span class="dv">36</span><span class="op">-</span><span class="dv">2</span>), rv_theta.std()]})</span>
<span id="cb28-425"><a href="#cb28-425" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-426"><a href="#cb28-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-427"><a href="#cb28-427" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b) {#sec-3-4-b}</span></span>
<span id="cb28-428"><a href="#cb28-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-429"><a href="#cb28-429" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Repeat </span><span class="co">[</span><span class="ot">(a)</span><span class="co">](#sec-3-4-a)</span><span class="at">, but using a $beta(8,2)$ prior for $\theta$.</span></span>
<span id="cb28-430"><a href="#cb28-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-431"><a href="#cb28-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-432"><a href="#cb28-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-433"><a href="#cb28-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-434"><a href="#cb28-434" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p(\theta) \sim beta(8,2)$</span>
<span id="cb28-435"><a href="#cb28-435" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Plotted in @fig-3-4-prior-2 </span>
<span id="cb28-436"><a href="#cb28-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-437"><a href="#cb28-437" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p(y|\theta) = {n\choose y}\theta^{y}(1-\theta)^{(n-y)}$</span>
<span id="cb28-438"><a href="#cb28-438" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Plotted in @fig-3-4-likelihood-2</span>
<span id="cb28-439"><a href="#cb28-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-440"><a href="#cb28-440" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p(\theta | y) \sim beta(a+y, b + n-y) = beta(8+15, 2 + 43 - 15) = beta(23, 30)$</span>
<span id="cb28-441"><a href="#cb28-441" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Plotted in @fig-3-4-posterior-2 </span>
<span id="cb28-442"><a href="#cb28-442" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$E<span class="co">[</span><span class="ot">p(\theta | y)</span><span class="co">]</span> = \frac{a}{a+b} = \frac{23}{23+30} \approx 0.434$</span>
<span id="cb28-443"><a href="#cb28-443" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$Mode(p(\theta | y)) = \frac{a-1}{a+b-2} \approx 0.431$</span>
<span id="cb28-444"><a href="#cb28-444" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$std(p(\theta | y)) = \sqrt{var<span class="co">[</span><span class="ot">p(\theta | y)</span><span class="co">]</span>} = \sqrt{\frac{ab}{(a+b)^2 (a+b+1)}} =  \sqrt{\frac{23\times 30}{(23+30)^2 (23+30+1)}}\approx 0.07$</span>
<span id="cb28-445"><a href="#cb28-445" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Properties are shown in @tbl-3-4-posterior-2.</span>
<span id="cb28-446"><a href="#cb28-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-447"><a href="#cb28-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-448"><a href="#cb28-448" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-449"><a href="#cb28-449" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-3-4-prior-2</span></span>
<span id="cb28-450"><a href="#cb28-450" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Prior distribution."</span></span>
<span id="cb28-451"><a href="#cb28-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-452"><a href="#cb28-452" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb28-453"><a href="#cb28-453" aria-hidden="true" tabindex="-1"></a>rv_theta <span class="op">=</span> st.beta(<span class="dv">8</span>,<span class="dv">2</span>)</span>
<span id="cb28-454"><a href="#cb28-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-455"><a href="#cb28-455" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb28-456"><a href="#cb28-456" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, rv_theta.pdf(thetas), <span class="st">'k-'</span>)</span>
<span id="cb28-457"><a href="#cb28-457" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)<span class="op">;</span></span>
<span id="cb28-458"><a href="#cb28-458" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(</span><span class="ch">\\</span><span class="st">theta)$"</span>)<span class="op">;</span></span>
<span id="cb28-459"><a href="#cb28-459" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-460"><a href="#cb28-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-461"><a href="#cb28-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-462"><a href="#cb28-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-463"><a href="#cb28-463" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-464"><a href="#cb28-464" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-3-4-likelihood-2</span></span>
<span id="cb28-465"><a href="#cb28-465" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Likelihood"</span></span>
<span id="cb28-466"><a href="#cb28-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-467"><a href="#cb28-467" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb28-468"><a href="#cb28-468" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">43</span></span>
<span id="cb28-469"><a href="#cb28-469" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb28-470"><a href="#cb28-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-471"><a href="#cb28-471" aria-hidden="true" tabindex="-1"></a>pr_like <span class="op">=</span> [st.binom.pmf(y, n, theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas]</span>
<span id="cb28-472"><a href="#cb28-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-473"><a href="#cb28-473" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb28-474"><a href="#cb28-474" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, pr_like, <span class="st">'k-'</span>)</span>
<span id="cb28-475"><a href="#cb28-475" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)<span class="op">;</span></span>
<span id="cb28-476"><a href="#cb28-476" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(y|</span><span class="ch">\\</span><span class="st">theta)$"</span>)<span class="op">;</span></span>
<span id="cb28-477"><a href="#cb28-477" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-478"><a href="#cb28-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-479"><a href="#cb28-479" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-480"><a href="#cb28-480" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-3-4-posterior-2</span></span>
<span id="cb28-481"><a href="#cb28-481" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Posterior distribution"</span></span>
<span id="cb28-482"><a href="#cb28-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-483"><a href="#cb28-483" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb28-484"><a href="#cb28-484" aria-hidden="true" tabindex="-1"></a>rv_theta <span class="op">=</span> st.beta(<span class="dv">8</span><span class="op">+</span><span class="dv">15</span>, <span class="dv">2</span> <span class="op">+</span> <span class="dv">43</span> <span class="op">-</span> <span class="dv">15</span>)</span>
<span id="cb28-485"><a href="#cb28-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-486"><a href="#cb28-486" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb28-487"><a href="#cb28-487" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, rv_theta.pdf(thetas), <span class="st">'k-'</span>)</span>
<span id="cb28-488"><a href="#cb28-488" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)<span class="op">;</span></span>
<span id="cb28-489"><a href="#cb28-489" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(</span><span class="ch">\\</span><span class="st">theta | y)$"</span>)<span class="op">;</span></span>
<span id="cb28-490"><a href="#cb28-490" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-491"><a href="#cb28-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-492"><a href="#cb28-492" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-493"><a href="#cb28-493" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-3-4-posterior-2</span></span>
<span id="cb28-494"><a href="#cb28-494" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Properties of posterior distribution."</span></span>
<span id="cb28-495"><a href="#cb28-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-496"><a href="#cb28-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-497"><a href="#cb28-497" aria-hidden="true" tabindex="-1"></a>ints <span class="op">=</span> rv_theta.interval(<span class="fl">0.95</span>)</span>
<span id="cb28-498"><a href="#cb28-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-499"><a href="#cb28-499" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Properties"</span>: [<span class="st">"Left bound (CI)"</span>, <span class="st">"Right bound (CI)"</span>, <span class="st">"mean"</span>, <span class="st">"mode"</span>, <span class="st">"standard deviation"</span>], <span class="st">"Values"</span>: [ints[<span class="dv">0</span>], ints[<span class="dv">1</span>], rv_theta.mean(), (<span class="dv">23</span><span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>(<span class="dv">23</span><span class="op">+</span><span class="dv">30</span><span class="op">-</span><span class="dv">2</span>), rv_theta.std()]})</span>
<span id="cb28-500"><a href="#cb28-500" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-501"><a href="#cb28-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-502"><a href="#cb28-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-503"><a href="#cb28-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-504"><a href="#cb28-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-505"><a href="#cb28-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-506"><a href="#cb28-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-507"><a href="#cb28-507" aria-hidden="true" tabindex="-1"></a><span class="fu">### (c) {#sec-3-4-c}</span></span>
<span id="cb28-508"><a href="#cb28-508" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Consider the following prior distribution for $\theta$: </span></span>
<span id="cb28-509"><a href="#cb28-509" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $$p(\theta) = \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}</span><span class="co">[</span><span class="ot">3\theta(1-\theta)^7+\theta^7(1-\theta)</span><span class="co">]</span><span class="at">$$</span></span>
<span id="cb28-510"><a href="#cb28-510" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; which is a $75-25\%$ mixture of a $beta(2,8)$ and a $beta(8,2)$ prior distribution. Plot this prior distribution and compare it to the priors in </span><span class="co">[</span><span class="ot">(a)</span><span class="co">](#sec-3-4-a)</span><span class="at"> and </span><span class="co">[</span><span class="ot">(b)</span><span class="co">](#sec-3-4-b)</span><span class="at">. Describe what sort of prior opinion this may represent.</span></span>
<span id="cb28-511"><a href="#cb28-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-512"><a href="#cb28-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-513"><a href="#cb28-513" aria-hidden="true" tabindex="-1"></a>The mixture of beta distribution is plotted in @fig-3-4-mixbeta. This opinion merges two opposite suggestions with different weights:</span>
<span id="cb28-514"><a href="#cb28-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-515"><a href="#cb28-515" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\theta$ is low (<span class="co">[</span><span class="ot">@fig-3-4-prior</span><span class="co">]</span>).</span>
<span id="cb28-516"><a href="#cb28-516" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\theta$ is high (<span class="co">[</span><span class="ot">@fig-3-4-prior-2</span><span class="co">]</span>).</span>
<span id="cb28-517"><a href="#cb28-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-518"><a href="#cb28-518" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-519"><a href="#cb28-519" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-3-4-mixbeta</span></span>
<span id="cb28-520"><a href="#cb28-520" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Mixture beta distribution"</span></span>
<span id="cb28-521"><a href="#cb28-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-522"><a href="#cb28-522" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mixBeta(th):</span>
<span id="cb28-523"><a href="#cb28-523" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">0.25</span><span class="op">*</span>gamma(<span class="dv">10</span>)<span class="op">/</span>(gamma(<span class="dv">2</span>)<span class="op">*</span>gamma(<span class="dv">8</span>))<span class="op">*</span>( <span class="dv">3</span><span class="op">*</span>th<span class="op">*</span>((<span class="dv">1</span><span class="op">-</span>th)<span class="op">**</span><span class="dv">7</span>) <span class="op">+</span> (th<span class="op">**</span><span class="dv">7</span>)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>th) )</span>
<span id="cb28-524"><a href="#cb28-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-525"><a href="#cb28-525" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb28-526"><a href="#cb28-526" aria-hidden="true" tabindex="-1"></a>prs <span class="op">=</span> [mixBeta(theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas]</span>
<span id="cb28-527"><a href="#cb28-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-528"><a href="#cb28-528" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb28-529"><a href="#cb28-529" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, prs, <span class="st">"k-"</span>)</span>
<span id="cb28-530"><a href="#cb28-530" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)</span>
<span id="cb28-531"><a href="#cb28-531" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$p(</span><span class="ch">\\</span><span class="st">theta)$"</span>)<span class="op">;</span></span>
<span id="cb28-532"><a href="#cb28-532" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-533"><a href="#cb28-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-534"><a href="#cb28-534" aria-hidden="true" tabindex="-1"></a><span class="fu">### (d) {#sec-3-4-d}</span></span>
<span id="cb28-535"><a href="#cb28-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-536"><a href="#cb28-536" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; For the prior in </span><span class="co">[</span><span class="ot">(c)</span><span class="co">](#sec-3-4-c)</span><span class="at">:</span></span>
<span id="cb28-537"><a href="#cb28-537" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb28-538"><a href="#cb28-538" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 1. Write out mathematically $p(\theta)\times p(y|\theta)$ and simplify as much as possible.</span></span>
<span id="cb28-539"><a href="#cb28-539" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb28-540"><a href="#cb28-540" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 2. The posterior distribution is a mixture of two distributions you know. Identify these distributions.</span></span>
<span id="cb28-541"><a href="#cb28-541" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb28-542"><a href="#cb28-542" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 3. On a computer, calculate and plot $p(\theta) \times p(y|\theta)$ for a variety of $\theta$ values. Also find (approximately) the posterior mode, and discuss its relation to the modes in </span><span class="co">[</span><span class="ot">(a)</span><span class="co">](#sec-3-4-a)</span><span class="at"> and </span><span class="co">[</span><span class="ot">(b)</span><span class="co">](#sec-3-4-b)</span><span class="at">.</span></span>
<span id="cb28-543"><a href="#cb28-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-544"><a href="#cb28-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-545"><a href="#cb28-545" aria-hidden="true" tabindex="-1"></a>**Part 1.**</span>
<span id="cb28-546"><a href="#cb28-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-547"><a href="#cb28-547" aria-hidden="true" tabindex="-1"></a>Noted that ${43 \choose 15} = \frac{43!}{15!28!}=\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}$</span>
<span id="cb28-548"><a href="#cb28-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-549"><a href="#cb28-549" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-550"><a href="#cb28-550" aria-hidden="true" tabindex="-1"></a>  p(\theta)\times p(y|\theta) &amp;= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}<span class="co">[</span><span class="ot">3\theta(1-\theta)^7+\theta^7(1-\theta)</span><span class="co">]</span> \times {43\choose 15}\theta^{15}(1-\theta)^{(43-15)}<span class="sc">\\</span></span>
<span id="cb28-551"><a href="#cb28-551" aria-hidden="true" tabindex="-1"></a>  &amp;= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}\underbrace{{43 \choose 15}}_{\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}}(\theta^{22}(1-\theta)^{29} + 3\theta^{16}(1-\theta)^{35})<span class="sc">\\</span></span>
<span id="cb28-552"><a href="#cb28-552" aria-hidden="true" tabindex="-1"></a>  &amp;= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}(\underline{\theta^{22}(1-\theta)^{29}} + \underline{3\theta^{16}(1-\theta)^{35}})<span class="sc">\\</span></span>
<span id="cb28-553"><a href="#cb28-553" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-554"><a href="#cb28-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-555"><a href="#cb28-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-556"><a href="#cb28-556" aria-hidden="true" tabindex="-1"></a>*The simplification is by the aid of walfram-alpha[^wolfram].*</span>
<span id="cb28-557"><a href="#cb28-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-558"><a href="#cb28-558" aria-hidden="true" tabindex="-1"></a><span class="ot">[^wolfram]: https://www.wolframalpha.com/input?i=%283*x*%281-x%29%5E7+%2B+x%5E7+*+%281-x%29%29+*x%5E15+*+%281-x%29%5E28</span></span>
<span id="cb28-559"><a href="#cb28-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-560"><a href="#cb28-560" aria-hidden="true" tabindex="-1"></a>**Part 2.**</span>
<span id="cb28-561"><a href="#cb28-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-562"><a href="#cb28-562" aria-hidden="true" tabindex="-1"></a>The distribution is the mixture of $Beta(23,30)$ and $Beta(17,36)$ with certain weights.</span>
<span id="cb28-563"><a href="#cb28-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-564"><a href="#cb28-564" aria-hidden="true" tabindex="-1"></a>**Part 3.**</span>
<span id="cb28-565"><a href="#cb28-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-566"><a href="#cb28-566" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The mode of $p(\theta) \times p(y|\theta)$  is $0.314$ (See <span class="co">[</span><span class="ot">@fig-3-4-3-mixbeta</span><span class="co">]</span>).</span>
<span id="cb28-567"><a href="#cb28-567" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The mode in <span class="co">[</span><span class="ot">(a)</span><span class="co">](#sec-3-4-a)</span>: $0.313725$</span>
<span id="cb28-568"><a href="#cb28-568" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The mode in <span class="co">[</span><span class="ot">(b)</span><span class="co">](#sec-3-4-b)</span>: $0.431373$</span>
<span id="cb28-569"><a href="#cb28-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-570"><a href="#cb28-570" aria-hidden="true" tabindex="-1"></a>Thus, the posterior distribution has the mode between $Beta(2,8)$ (<span class="co">[</span><span class="ot">(a)</span><span class="co">](#sec-3-4-a)</span>) and $Beta(8,2)$(<span class="co">[</span><span class="ot">(b)</span><span class="co">](#sec-3-4-b)</span>), and more close to $Beta(2.8)$</span>
<span id="cb28-571"><a href="#cb28-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-572"><a href="#cb28-572" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-573"><a href="#cb28-573" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-3-4-3-mixbeta</span></span>
<span id="cb28-574"><a href="#cb28-574" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Posterior distribution with mixture of two beta distributions."</span></span>
<span id="cb28-575"><a href="#cb28-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-576"><a href="#cb28-576" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mixture_post(th):</span>
<span id="cb28-577"><a href="#cb28-577" aria-hidden="true" tabindex="-1"></a>  scale <span class="op">=</span> <span class="fl">0.25</span> <span class="op">*</span> gamma(<span class="dv">10</span>)<span class="op">/</span>(gamma(<span class="dv">2</span>)<span class="op">*</span>gamma(<span class="dv">8</span>)) <span class="op">*</span> gamma(<span class="dv">44</span>)<span class="op">/</span>(gamma(<span class="dv">16</span>)<span class="op">*</span>gamma(<span class="dv">29</span>))</span>
<span id="cb28-578"><a href="#cb28-578" aria-hidden="true" tabindex="-1"></a>  beta <span class="op">=</span> (th<span class="op">**</span><span class="dv">22</span>)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>th)<span class="op">**</span><span class="dv">29</span> <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>(th<span class="op">**</span><span class="dv">16</span>)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>th)<span class="op">**</span><span class="dv">35</span></span>
<span id="cb28-579"><a href="#cb28-579" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> scale<span class="op">*</span>beta</span>
<span id="cb28-580"><a href="#cb28-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-581"><a href="#cb28-581" aria-hidden="true" tabindex="-1"></a>prs <span class="op">=</span> [mixture_post(theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas]</span>
<span id="cb28-582"><a href="#cb28-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-583"><a href="#cb28-583" aria-hidden="true" tabindex="-1"></a>maxTh <span class="op">=</span> thetas[np.argmax(prs)]</span>
<span id="cb28-584"><a href="#cb28-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-585"><a href="#cb28-585" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, prs, <span class="st">'k-'</span>)</span>
<span id="cb28-586"><a href="#cb28-586" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)</span>
<span id="cb28-587"><a href="#cb28-587" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$p(</span><span class="ch">\\</span><span class="st">theta)</span><span class="ch">\\</span><span class="st">times p(y|</span><span class="ch">\\</span><span class="st">theta)$"</span>)<span class="op">;</span></span>
<span id="cb28-588"><a href="#cb28-588" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>maxTh, linestyle<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>, label<span class="op">=</span> <span class="st">"Mode=</span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(maxTh))<span class="op">;</span></span>
<span id="cb28-589"><a href="#cb28-589" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span>
<span id="cb28-590"><a href="#cb28-590" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-591"><a href="#cb28-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-592"><a href="#cb28-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-593"><a href="#cb28-593" aria-hidden="true" tabindex="-1"></a><span class="fu">### (e)</span></span>
<span id="cb28-594"><a href="#cb28-594" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Find a general formula for the weights of the mixture distribution in </span><span class="co">[</span><span class="ot">(d) 2.</span><span class="co">](#sec-3-4-d)</span><span class="at">, and provide an interpretation for their values.</span></span>
<span id="cb28-595"><a href="#cb28-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-596"><a href="#cb28-596" aria-hidden="true" tabindex="-1"></a>Let $c_1 =  \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}$</span>
<span id="cb28-597"><a href="#cb28-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-598"><a href="#cb28-598" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-599"><a href="#cb28-599" aria-hidden="true" tabindex="-1"></a>  p(\theta)\times p(y|\theta) &amp;= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}(\underline{\theta^{22}(1-\theta)^{29}} + \underline{3\theta^{16}(1-\theta)^{35}})<span class="sc">\\</span> </span>
<span id="cb28-600"><a href="#cb28-600" aria-hidden="true" tabindex="-1"></a>                              &amp;= c_1 (\theta^{22}(1-\theta)^{29} + 3\theta^{16}(1-\theta)^{35})<span class="sc">\\</span></span>
<span id="cb28-601"><a href="#cb28-601" aria-hidden="true" tabindex="-1"></a>                              &amp;= c_1 \theta^{22}(1-\theta)^{29} + 3c_1 \theta^{16}(1-\theta)^{35})<span class="sc">\\</span></span>
<span id="cb28-602"><a href="#cb28-602" aria-hidden="true" tabindex="-1"></a>                              &amp;= c_1 \frac{\Gamma(23)\Gamma(30)}{\Gamma(53)} Beta(\theta, 23,30) + 3 c_1 \frac{\Gamma(17)\Gamma(36)}{\Gamma(51)} Beta(\theta, 17,36)<span class="sc">\\</span> </span>
<span id="cb28-603"><a href="#cb28-603" aria-hidden="true" tabindex="-1"></a>                              &amp;= 0.0003 \times Beta(\theta, 23,30) + 58.16 \times Beta(\theta, 17,36)<span class="sc">\\</span></span>
<span id="cb28-604"><a href="#cb28-604" aria-hidden="true" tabindex="-1"></a>                              &amp;= \omega_1 \cdot Beta(\theta, 23,30) + \omega_2 \cdot Beta(\theta, 17,36)</span>
<span id="cb28-605"><a href="#cb28-605" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-606"><a href="#cb28-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-607"><a href="#cb28-607" aria-hidden="true" tabindex="-1"></a>That means $Beta(17,36)$ is preferred to $Beta(23,30)$. The updated posterior information is more close to the <span class="co">[</span><span class="ot">(a)</span><span class="co">](#sec-3-4-d)</span>. That is because the mixture of priors has more weights ($75\%$) on the prior of $Beta(2,8)$.</span>
<span id="cb28-608"><a href="#cb28-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-609"><a href="#cb28-609" aria-hidden="true" tabindex="-1"></a><span class="fu">## Problem 3.7</span></span>
<span id="cb28-610"><a href="#cb28-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-611"><a href="#cb28-611" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Posterior prediction: Consider a pilot study in which $n_1 = 15$ children enrolled in special education classes were randomly selected and tested for a certain type of learning disability. In the pilot study, $y_1 = 2$ children tested positive for the disability.</span></span>
<span id="cb28-612"><a href="#cb28-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-613"><a href="#cb28-613" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a)</span></span>
<span id="cb28-614"><a href="#cb28-614" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Using a uniform prior distribution, find the posterior distribution of $\theta$, the fraction of students in special education classes who have the disability. Find the posterior mean, mode and standard deviation of $\theta$, and plot the posterior density.</span></span>
<span id="cb28-615"><a href="#cb28-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-616"><a href="#cb28-616" aria-hidden="true" tabindex="-1"></a>$$\theta \sim beta(1,1) (uniform)$$</span>
<span id="cb28-617"><a href="#cb28-617" aria-hidden="true" tabindex="-1"></a>$$Y\sim binomial(n_1,\theta)$$</span>
<span id="cb28-618"><a href="#cb28-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-619"><a href="#cb28-619" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-620"><a href="#cb28-620" aria-hidden="true" tabindex="-1"></a>  \theta|Y=y &amp;\sim beta(1+y_1, 1+n_1 - y_1)<span class="sc">\\</span> </span>
<span id="cb28-621"><a href="#cb28-621" aria-hidden="true" tabindex="-1"></a>             &amp;= beta(1 + 2, 1+15-2)<span class="sc">\\</span> </span>
<span id="cb28-622"><a href="#cb28-622" aria-hidden="true" tabindex="-1"></a>             &amp;= beta(3, 14)<span class="sc">\\</span> </span>
<span id="cb28-623"><a href="#cb28-623" aria-hidden="true" tabindex="-1"></a>             &amp;= beta(a_p, b_p)<span class="sc">\\</span> </span>
<span id="cb28-624"><a href="#cb28-624" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-625"><a href="#cb28-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-626"><a href="#cb28-626" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The distribution is plotted in @fig-3-7-a</span>
<span id="cb28-627"><a href="#cb28-627" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$E<span class="co">[</span><span class="ot">\theta|Y</span><span class="co">]</span> = \frac{a_p}{a_p+b_p} = \frac{3}{3+14} \approx 0.1764$</span>
<span id="cb28-628"><a href="#cb28-628" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Mode<span class="co">[</span><span class="ot">\theta|Y</span><span class="co">]</span> = \frac{(a_p - 1)}{a_p - 1 + b_p - 1} = \frac{(3 - 1)}{3 - 1 + 14 - 1} \approx 0.1333$</span>
<span id="cb28-629"><a href="#cb28-629" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$Std<span class="co">[</span><span class="ot">\theta|Y</span><span class="co">]</span> = \sqrt{ \frac{a_p b_p}{(a_p+b_p+1)(a_p+b_p)^2} } = \sqrt{ \frac{3\cdot 14}{(3+14+1)(3+14)^2}} \approx 0.0899$</span>
<span id="cb28-630"><a href="#cb28-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-631"><a href="#cb28-631" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-632"><a href="#cb28-632" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-3-7-a</span></span>
<span id="cb28-633"><a href="#cb28-633" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Posterior distribution"</span></span>
<span id="cb28-634"><a href="#cb28-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-635"><a href="#cb28-635" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1000</span>)</span>
<span id="cb28-636"><a href="#cb28-636" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> st.beta(<span class="dv">3</span>, <span class="dv">14</span>)</span>
<span id="cb28-637"><a href="#cb28-637" aria-hidden="true" tabindex="-1"></a>pr_pos <span class="op">=</span> [pos.pdf(theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas]</span>
<span id="cb28-638"><a href="#cb28-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-639"><a href="#cb28-639" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, pr_pos, <span class="st">"k-"</span>)</span>
<span id="cb28-640"><a href="#cb28-640" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta$"</span>)</span>
<span id="cb28-641"><a href="#cb28-641" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(</span><span class="ch">\\</span><span class="st">theta|Y)$"</span>)<span class="op">;</span></span>
<span id="cb28-642"><a href="#cb28-642" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-643"><a href="#cb28-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-644"><a href="#cb28-644" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Researchers would like to recruit students with the disability to participate in a long-term study, but first they need to make sure they can recruit enough students. Let $n_2 = 278$ be the number of children in special education classes in this particular school district, and let $Y_2$ be the number of students with the disability.</span></span>
<span id="cb28-645"><a href="#cb28-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-646"><a href="#cb28-646" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b) </span></span>
<span id="cb28-647"><a href="#cb28-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-648"><a href="#cb28-648" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Find $Pr(Y_2=y_2|Y_1 =2)$, the posterior predictive distribution of $Y_2$, as follows:</span></span>
<span id="cb28-649"><a href="#cb28-649" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb28-650"><a href="#cb28-650" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 1. Discuss what assumptions are needed about the joint distribution of $(Y_1, Y_2)$ such that the fololowing is true:</span></span>
<span id="cb28-651"><a href="#cb28-651" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $$Pr(Y_2=y_2 |Y_1=2) = \int^{1}_{0} Pr(Y_2=y_2|\theta)p(\theta|Y_1=2)d\theta$$ {#eq-3-7-b-p}</span></span>
<span id="cb28-652"><a href="#cb28-652" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb28-653"><a href="#cb28-653" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 2. Now plug in the forms of $Pr(Y_2=y_2|\theta)$ and $p(\theta|Y_1 =2)$ in the above integral.</span></span>
<span id="cb28-654"><a href="#cb28-654" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb28-655"><a href="#cb28-655" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 3. Figure out what the above integral must be by using the calculus result discussed in Section 3.1.</span></span>
<span id="cb28-656"><a href="#cb28-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-657"><a href="#cb28-657" aria-hidden="true" tabindex="-1"></a>**Part 1**</span>
<span id="cb28-658"><a href="#cb28-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-659"><a href="#cb28-659" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The assumption is that $Y_2$ is *conditionally independent* on $Y_1$ over $\theta$ </span>
<span id="cb28-660"><a href="#cb28-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-661"><a href="#cb28-661" aria-hidden="true" tabindex="-1"></a>Thus,</span>
<span id="cb28-662"><a href="#cb28-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-663"><a href="#cb28-663" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-664"><a href="#cb28-664" aria-hidden="true" tabindex="-1"></a>  \int^{1}_{0} Pr(Y_2=y_2|\theta)p(\theta|Y_1=2)d\theta &amp;= \int^{1}_{0} Pr(Y_2=y_2)|\theta, Y_1=2)p(\theta |Y_1=2)d\theta<span class="sc">\\</span></span>
<span id="cb28-665"><a href="#cb28-665" aria-hidden="true" tabindex="-1"></a>  &amp;= \int^{1}_{0} Pr(Y_2=y_2, \theta |Y_1 =2) d\theta<span class="sc">\\</span> </span>
<span id="cb28-666"><a href="#cb28-666" aria-hidden="true" tabindex="-1"></a>  &amp;= Pr(Y_2 = y_2 | Y_1=2)</span>
<span id="cb28-667"><a href="#cb28-667" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-668"><a href="#cb28-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-669"><a href="#cb28-669" aria-hidden="true" tabindex="-1"></a>The equality of @eq-3-7-b-p holds.</span>
<span id="cb28-670"><a href="#cb28-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-671"><a href="#cb28-671" aria-hidden="true" tabindex="-1"></a>**Part 2**</span>
<span id="cb28-672"><a href="#cb28-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-673"><a href="#cb28-673" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-674"><a href="#cb28-674" aria-hidden="true" tabindex="-1"></a>  Pr(Y_2=y_2 |Y_1=2) &amp;= \int^{1}_{0} Pr(Y_2=y_2|\theta)p(\theta|Y_1=2)d\theta<span class="sc">\\</span> </span>
<span id="cb28-675"><a href="#cb28-675" aria-hidden="true" tabindex="-1"></a>                     &amp;= \int^{1}_{0} binomial(y_2, n_2, \theta) beta(\theta, 3,14) d\theta<span class="sc">\\</span></span>
<span id="cb28-676"><a href="#cb28-676" aria-hidden="true" tabindex="-1"></a>                     &amp;= \int^{1}_{0} {n_2 \choose y_2}\theta^{y_2}(1-\theta)^{n_2-y_2} \frac{\Gamma(17)}{\Gamma(3)\Gamma(14)}\theta^{2}(1-\theta)^{13} d\theta<span class="sc">\\</span> </span>
<span id="cb28-677"><a href="#cb28-677" aria-hidden="true" tabindex="-1"></a>                     &amp;= {n_2 \choose y_2}\frac{\Gamma(17)}{\Gamma(3)\Gamma(14)} \int^{1}_{0} \theta^{y_2}(1-\theta)^{n_2-y_2} \theta^{2}(1-\theta)^{13} d\theta\\&amp;= {n_2 \choose y_2}\frac{\Gamma(17)}{\Gamma(3)\Gamma(14)} \int^{1}_{0} \theta^{(2+y_2)}(1-\theta)^{n_2 - y_2 +13} d\theta<span class="sc">\\</span></span>
<span id="cb28-678"><a href="#cb28-678" aria-hidden="true" tabindex="-1"></a>                     &amp;= {278\choose y_2}\frac{\Gamma(17)}{\Gamma(3)\Gamma(14)} \int^{1}_{0} \theta^{(2+y_2)}(1-\theta)^{278 - y_2 +13} d\theta<span class="sc">\\</span></span>
<span id="cb28-679"><a href="#cb28-679" aria-hidden="true" tabindex="-1"></a>                     &amp;= {278\choose y_2}\frac{\Gamma(17)}{\Gamma(3)\Gamma(14)} \int^{1}_{0} \theta^{(2+y_2)}(1-\theta)^{291 - y_2} d\theta<span class="sc">\\</span></span>
<span id="cb28-680"><a href="#cb28-680" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-681"><a href="#cb28-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-682"><a href="#cb28-682" aria-hidden="true" tabindex="-1"></a>**Part 3**</span>
<span id="cb28-683"><a href="#cb28-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-684"><a href="#cb28-684" aria-hidden="true" tabindex="-1"></a>Use the calculus trick:</span>
<span id="cb28-685"><a href="#cb28-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-686"><a href="#cb28-686" aria-hidden="true" tabindex="-1"></a>$$\int^{1}_{0} \theta^{a-1}(1-\theta)^{b-1}d\theta = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$$</span>
<span id="cb28-687"><a href="#cb28-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-688"><a href="#cb28-688" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-689"><a href="#cb28-689" aria-hidden="true" tabindex="-1"></a>  \int^{1}_{0} \theta^{(2+y_2)}(1-\theta)^{291 - y_2} d\theta &amp;= \int^{1}_{0} \theta^{(3+y_2 - 1)}(1-\theta)^{292 - y_2 - 1} d\theta<span class="sc">\\</span></span>
<span id="cb28-690"><a href="#cb28-690" aria-hidden="true" tabindex="-1"></a>  &amp;= \frac{\Gamma(3+y_2)\Gamma(292 - y_2)}{\Gamma(3+y_2 + 292 - y_2)}<span class="sc">\\</span> </span>
<span id="cb28-691"><a href="#cb28-691" aria-hidden="true" tabindex="-1"></a>  &amp;= \frac{\Gamma(3+y_2)\Gamma(292 - y_2)}{\Gamma(295)}<span class="sc">\\</span></span>
<span id="cb28-692"><a href="#cb28-692" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-693"><a href="#cb28-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-694"><a href="#cb28-694" aria-hidden="true" tabindex="-1"></a>$\therefore$</span>
<span id="cb28-695"><a href="#cb28-695" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-696"><a href="#cb28-696" aria-hidden="true" tabindex="-1"></a>Pr(Y_2=y_2 |Y_1=2) &amp;= {278\choose y_2}\frac{\Gamma(17)}{\Gamma(3)\Gamma(14)}\frac{\Gamma(3+y_2)\Gamma(292 - y_2)}{\Gamma(295)}<span class="sc">\\</span></span>
<span id="cb28-697"><a href="#cb28-697" aria-hidden="true" tabindex="-1"></a>                   &amp;= \frac{\Gamma(278)}{\Gamma(y_2)\Gamma(278-y_2)} \frac{\Gamma(17)}{\Gamma(3)\Gamma(14)}\frac{\Gamma(3+y_2)\Gamma(292 - y_2)}{\Gamma(295)}<span class="sc">\\</span></span>
<span id="cb28-698"><a href="#cb28-698" aria-hidden="true" tabindex="-1"></a>                   &amp;= \frac{\Gamma(3+y_2)}{\Gamma(y_2)}\frac{\Gamma(278)}{\Gamma(295)}\frac{\Gamma(292-y_2)}{\Gamma(278-y_2)} \frac{\Gamma(17)}{\Gamma(3)\Gamma(14)}<span class="sc">\\</span></span>
<span id="cb28-699"><a href="#cb28-699" aria-hidden="true" tabindex="-1"></a>                   &amp;= \prod^{3+y_2 -1}_{i=y_2} i \times \frac{1}{\prod^{295-1}_{i=278}i}\prod^{292-y_2 - 1}_{i=278-y_2}i \times 1680<span class="sc">\\</span></span>
<span id="cb28-700"><a href="#cb28-700" aria-hidden="true" tabindex="-1"></a>                   &amp;= \prod^{2+y_2}_{i=y_2} i \times \frac{1}{\prod^{294}_{i=278}i}\prod^{291-y_2}_{i=278-y_2}i \times 1680<span class="sc">\\</span></span>
<span id="cb28-701"><a href="#cb28-701" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-702"><a href="#cb28-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-703"><a href="#cb28-703" aria-hidden="true" tabindex="-1"></a><span class="fu">### (c) {#sec-3-7-c}</span></span>
<span id="cb28-704"><a href="#cb28-704" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Plot the function $Pr(Y_2 = y_2 | Y_1 =2)$ as a function of $y_2$. Obtain the mean and standard deviation of $Y_2$, given $Y_1 = 2$.</span></span>
<span id="cb28-705"><a href="#cb28-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-706"><a href="#cb28-706" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The plot of $Pr(Y_2 = y_2 | Y_1 =2)$ is in @fig-3-7-c.</span>
<span id="cb28-707"><a href="#cb28-707" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>mean and standard deviation are displayed in @tbl-3-7-c.</span>
<span id="cb28-708"><a href="#cb28-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-709"><a href="#cb28-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-710"><a href="#cb28-710" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-711"><a href="#cb28-711" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-3-7-c</span></span>
<span id="cb28-712"><a href="#cb28-712" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Predictive distribution given Y1=2"</span></span>
<span id="cb28-713"><a href="#cb28-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-714"><a href="#cb28-714" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prod(a, b):</span>
<span id="cb28-715"><a href="#cb28-715" aria-hidden="true" tabindex="-1"></a>  s <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb28-716"><a href="#cb28-716" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> np.arange(a,b<span class="op">+</span><span class="dv">1</span>, <span class="fl">1.0</span>):</span>
<span id="cb28-717"><a href="#cb28-717" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> s<span class="op">*</span>i</span>
<span id="cb28-718"><a href="#cb28-718" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> s</span>
<span id="cb28-719"><a href="#cb28-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-720"><a href="#cb28-720" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pred_prob(y2, n2<span class="op">=</span><span class="dv">278</span>):</span>
<span id="cb28-721"><a href="#cb28-721" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> prod(y2, <span class="dv">2</span><span class="op">+</span>y2)<span class="op">*</span>(<span class="dv">1</span><span class="op">/</span>prod(<span class="dv">278</span>, <span class="dv">294</span>))<span class="op">*</span>prod(<span class="dv">278</span><span class="op">-</span>y2, <span class="dv">291</span><span class="op">-</span>y2)<span class="op">*</span><span class="dv">1680</span></span>
<span id="cb28-722"><a href="#cb28-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-723"><a href="#cb28-723" aria-hidden="true" tabindex="-1"></a>y2s <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">278</span>, <span class="dv">279</span>)</span>
<span id="cb28-724"><a href="#cb28-724" aria-hidden="true" tabindex="-1"></a>prs <span class="op">=</span> [pred_prob(y2) <span class="cf">for</span> y2 <span class="kw">in</span> y2s]</span>
<span id="cb28-725"><a href="#cb28-725" aria-hidden="true" tabindex="-1"></a>prs <span class="op">=</span> prs<span class="op">/</span>np.<span class="bu">sum</span>(prs)</span>
<span id="cb28-726"><a href="#cb28-726" aria-hidden="true" tabindex="-1"></a>plt.plot(y2s, prs, <span class="st">'ko'</span>)</span>
<span id="cb28-727"><a href="#cb28-727" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$y_</span><span class="sc">{2}</span><span class="st">$"</span>)</span>
<span id="cb28-728"><a href="#cb28-728" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$p(Y_2 | Y_1=2)$"</span>)<span class="op">;</span></span>
<span id="cb28-729"><a href="#cb28-729" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-730"><a href="#cb28-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-731"><a href="#cb28-731" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-732"><a href="#cb28-732" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-3-7-c</span></span>
<span id="cb28-733"><a href="#cb28-733" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Predictive distribution given Y1=2"</span></span>
<span id="cb28-734"><a href="#cb28-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-735"><a href="#cb28-735" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.<span class="bu">sum</span>(prs <span class="op">*</span> y2s)<span class="op">/</span>np.<span class="bu">sum</span>(prs)</span>
<span id="cb28-736"><a href="#cb28-736" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span>  np.sqrt(np.<span class="bu">sum</span>(y2s<span class="op">*</span>y2s<span class="op">*</span>prs) <span class="op">-</span> (np.<span class="bu">sum</span>(y2s<span class="op">*</span>prs))<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb28-737"><a href="#cb28-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-738"><a href="#cb28-738" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"mean"</span>: [np.<span class="bu">sum</span>(prs <span class="op">*</span> y2s)<span class="op">/</span>np.<span class="bu">sum</span>(prs)], <span class="st">"std"</span>: [std]})</span>
<span id="cb28-739"><a href="#cb28-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-740"><a href="#cb28-740" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-741"><a href="#cb28-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-742"><a href="#cb28-742" aria-hidden="true" tabindex="-1"></a><span class="fu">### (d)</span></span>
<span id="cb28-743"><a href="#cb28-743" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The posterior mode and the MLE (maximum likelihood estimate) of $\theta$, based on data from the pilot study, are both $\hat{\theta} = \frac{2}{15}$. Plot the distribution $Pr(Y_2 = y_2|\theta=\hat{\theta})$, and find the mean and standard deviation of $Y_2$ given $\theta=\hat{\theta}$. Compare these results to the plots and calculation in </span><span class="co">[</span><span class="ot">(c)</span><span class="co">](#sec-3-7-c)</span><span class="at"> and discuss any differences. Which distribution for $Y_2$ would you used to make predictions, and why?</span></span>
<span id="cb28-744"><a href="#cb28-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-745"><a href="#cb28-745" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-746"><a href="#cb28-746" aria-hidden="true" tabindex="-1"></a>  Pr(Y_2 = y_2 |\theta=\hat{\theta}) &amp;= binomial(y_2, n_2, \hat{\theta})<span class="sc">\\</span></span>
<span id="cb28-747"><a href="#cb28-747" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-748"><a href="#cb28-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-749"><a href="#cb28-749" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The plot of $Pr(Y_2 = y_2|\theta=\hat{\theta})$ distribution along with $y_2$ is @fig-3-7-d.</span>
<span id="cb28-750"><a href="#cb28-750" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mean and standard deviation are shown in @tbl-3-7-d.</span>
<span id="cb28-751"><a href="#cb28-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-752"><a href="#cb28-752" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Compare to @fig-3-7-c, @fig-3-7-d has less variation and less mean, which is more close to the original average of $Y_1$ data ($=\frac{2}{15}$). </span>
<span id="cb28-753"><a href="#cb28-753" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@fig-3-7-c provides better prediction with MLE $\theta$ because its properties is more related to the original average, and the likelihood is maximized with MLE method.</span>
<span id="cb28-754"><a href="#cb28-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-755"><a href="#cb28-755" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-756"><a href="#cb28-756" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-3-7-d</span></span>
<span id="cb28-757"><a href="#cb28-757" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Predictive distribution given MLE theta"</span></span>
<span id="cb28-758"><a href="#cb28-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-759"><a href="#cb28-759" aria-hidden="true" tabindex="-1"></a>n2 <span class="op">=</span> <span class="dv">278</span></span>
<span id="cb28-760"><a href="#cb28-760" aria-hidden="true" tabindex="-1"></a>th <span class="op">=</span> <span class="dv">2</span><span class="op">/</span><span class="dv">15</span></span>
<span id="cb28-761"><a href="#cb28-761" aria-hidden="true" tabindex="-1"></a>rv <span class="op">=</span> st.binom(n2, th)</span>
<span id="cb28-762"><a href="#cb28-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-763"><a href="#cb28-763" aria-hidden="true" tabindex="-1"></a>y2s <span class="op">=</span> np.linspace(<span class="dv">0</span>,n2, n2<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb28-764"><a href="#cb28-764" aria-hidden="true" tabindex="-1"></a>prs <span class="op">=</span> [rv.pmf(y2) <span class="cf">for</span> y2 <span class="kw">in</span> y2s]</span>
<span id="cb28-765"><a href="#cb28-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-766"><a href="#cb28-766" aria-hidden="true" tabindex="-1"></a>plt.plot(y2s, prs, <span class="st">"ko"</span>)</span>
<span id="cb28-767"><a href="#cb28-767" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$y_2$"</span>)</span>
<span id="cb28-768"><a href="#cb28-768" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$Pr(Y_2 = y_2|</span><span class="ch">\\</span><span class="st">theta=</span><span class="ch">\\</span><span class="st">hat{</span><span class="ch">\\</span><span class="st">theta})$"</span>)<span class="op">;</span></span>
<span id="cb28-769"><a href="#cb28-769" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-770"><a href="#cb28-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-771"><a href="#cb28-771" aria-hidden="true" tabindex="-1"></a><span class="in">``` {python}</span></span>
<span id="cb28-772"><a href="#cb28-772" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-3-7-d</span></span>
<span id="cb28-773"><a href="#cb28-773" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Predictive distribution given MLE theta"</span></span>
<span id="cb28-774"><a href="#cb28-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-775"><a href="#cb28-775" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.<span class="bu">sum</span>(y2s<span class="op">*</span>prs)</span>
<span id="cb28-776"><a href="#cb28-776" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>(y2s<span class="op">*</span>y2s<span class="op">*</span>prs) <span class="op">-</span> (np.<span class="bu">sum</span>(y2s<span class="op">*</span>prs))<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb28-777"><a href="#cb28-777" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"mean"</span>: [np.<span class="bu">sum</span>(prs <span class="op">*</span> y2s)<span class="op">/</span>np.<span class="bu">sum</span>(prs)], <span class="st">"std"</span>: [std]})</span>
<span id="cb28-778"><a href="#cb28-778" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-779"><a href="#cb28-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-780"><a href="#cb28-780" aria-hidden="true" tabindex="-1"></a><span class="fu">## Problem 3.12</span></span>
<span id="cb28-781"><a href="#cb28-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-782"><a href="#cb28-782" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Jeffrey's prior: Jeffreys (1961) suggested a default rule for gnerating a prior distribution of a parameter $\theta$ in a sampling model $p(y|\theta)$. Jeffreys' prior is given by $p_{J}\propto \sqrt{I(\theta)}$, where $I(\theta) = - E</span><span class="co">[</span><span class="ot">\frac{\partial^{2} \log p(Y|\theta)}{\partial\theta^2} | \theta</span><span class="co">]</span><span class="at">$ is the *Fisher information*.</span></span>
<span id="cb28-783"><a href="#cb28-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-784"><a href="#cb28-784" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a) {#sec-3-12-a}</span></span>
<span id="cb28-785"><a href="#cb28-785" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Let $Y\sim binomial(n,\theta)$. Obtain Jeffreys' prior  distribution $p_J(\theta)$ for this model.</span></span>
<span id="cb28-786"><a href="#cb28-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-787"><a href="#cb28-787" aria-hidden="true" tabindex="-1"></a>$\because$ $Y\sim binomial(n,\theta)$ $\therefore$ $E<span class="co">[</span><span class="ot">Y</span><span class="co">]</span>=n\theta$</span>
<span id="cb28-788"><a href="#cb28-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-789"><a href="#cb28-789" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-790"><a href="#cb28-790" aria-hidden="true" tabindex="-1"></a>p(y|\theta) &amp;= {n\choose y}\theta^{y}(1-\theta)^{n-y}<span class="sc">\\</span> </span>
<span id="cb28-791"><a href="#cb28-791" aria-hidden="true" tabindex="-1"></a>\log(p(y|\theta)) &amp;= \log {n\choose y} + y\log \theta + (n-y)\log (1-\theta)<span class="sc">\\</span></span>
<span id="cb28-792"><a href="#cb28-792" aria-hidden="true" tabindex="-1"></a>\frac{\partial \log(p(y|\theta))}{\partial \theta} &amp;= \frac{y}{\theta} - \frac{n-y}{1-\theta}<span class="sc">\\</span></span>
<span id="cb28-793"><a href="#cb28-793" aria-hidden="true" tabindex="-1"></a>\frac{\partial^2 \log(p(y|\theta))}{\partial^2 \theta} &amp;= \frac{-y}{\theta^2} - \frac{n-y}{(1-\theta)^2}<span class="sc">\\</span></span>
<span id="cb28-794"><a href="#cb28-794" aria-hidden="true" tabindex="-1"></a>E<span class="co">[</span><span class="ot">\frac{\partial^2 \log(p(y|\theta))}{\partial^2 \theta} |\theta</span><span class="co">]</span> &amp;= -\frac{n\theta}{\theta^2} - \frac{n-n\theta}{(1-\theta)^2}<span class="sc">\\</span></span>
<span id="cb28-795"><a href="#cb28-795" aria-hidden="true" tabindex="-1"></a>E<span class="co">[</span><span class="ot">\frac{\partial^2 \log(p(y|\theta))}{\partial^2 \theta} |\theta</span><span class="co">]</span> &amp;= \frac{-n}{\theta} - \frac{n}{1-\theta}<span class="sc">\\</span></span>
<span id="cb28-796"><a href="#cb28-796" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>E<span class="co">[</span><span class="ot">\frac{\partial^2 \log(p(y|\theta))}{\partial^2 \theta} |\theta</span><span class="co">]</span> &amp;= \frac{n}{\theta} + \frac{n}{1-\theta}<span class="sc">\\</span></span>
<span id="cb28-797"><a href="#cb28-797" aria-hidden="true" tabindex="-1"></a>I(\theta) &amp;= \frac{n}{\theta} + \frac{n}{1-\theta}<span class="sc">\\</span></span>
<span id="cb28-798"><a href="#cb28-798" aria-hidden="true" tabindex="-1"></a>          &amp;= \frac{n}{\theta(1-\theta)}<span class="sc">\\</span></span>
<span id="cb28-799"><a href="#cb28-799" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-800"><a href="#cb28-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-801"><a href="#cb28-801" aria-hidden="true" tabindex="-1"></a>$\because$ $p_J \propto \sqrt{I(\theta)}$</span>
<span id="cb28-802"><a href="#cb28-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-803"><a href="#cb28-803" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-804"><a href="#cb28-804" aria-hidden="true" tabindex="-1"></a>  p_J(\theta) \propto &amp;\sqrt{I(\theta)}<span class="sc">\\</span></span>
<span id="cb28-805"><a href="#cb28-805" aria-hidden="true" tabindex="-1"></a>                      &amp;= \sqrt{\frac{n}{\theta(1-\theta)}}<span class="sc">\\</span></span>
<span id="cb28-806"><a href="#cb28-806" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-807"><a href="#cb28-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-808"><a href="#cb28-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-809"><a href="#cb28-809" aria-hidden="true" tabindex="-1"></a>Let $c$ be the scalar. By the fact that $\frac{d}{dx}(\sin^{-1}x)=\frac{1}{\sqrt{1-x^2}}$,</span>
<span id="cb28-810"><a href="#cb28-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-811"><a href="#cb28-811" aria-hidden="true" tabindex="-1"></a>$$P_J (\theta) = c \times \sqrt{\frac{n}{\theta(1-\theta)}}$$ </span>
<span id="cb28-812"><a href="#cb28-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-813"><a href="#cb28-813" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-814"><a href="#cb28-814" aria-hidden="true" tabindex="-1"></a>1 &amp;= c \int^{1}_{0} \sqrt{\frac{n}{\theta(1-\theta)}} d\theta<span class="sc">\\</span></span>
<span id="cb28-815"><a href="#cb28-815" aria-hidden="true" tabindex="-1"></a>1 &amp;= nc \int^{1}_{0} \sqrt{\frac{1}{\theta(1-\theta)}} d\theta<span class="sc">\\</span></span>
<span id="cb28-816"><a href="#cb28-816" aria-hidden="true" tabindex="-1"></a>1 &amp;= nc \left<span class="co">[</span><span class="ot">  -2 \sin^{-1}(\sqrt{1-x})   \right</span><span class="co">]</span>^{1}_{0} <span class="sc">\\</span></span>
<span id="cb28-817"><a href="#cb28-817" aria-hidden="true" tabindex="-1"></a>1 &amp;= -2\times nc (\underbrace{\sin^{-1}(0)}_{=0} - \underbrace{\sin^{-1}(1)}_{=\frac{\pi}{2}})<span class="sc">\\</span> </span>
<span id="cb28-818"><a href="#cb28-818" aria-hidden="true" tabindex="-1"></a>1 &amp;= \pi n c <span class="sc">\\</span> </span>
<span id="cb28-819"><a href="#cb28-819" aria-hidden="true" tabindex="-1"></a>c &amp;= \frac{1}{\pi n}<span class="sc">\\</span></span>
<span id="cb28-820"><a href="#cb28-820" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-821"><a href="#cb28-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-822"><a href="#cb28-822" aria-hidden="true" tabindex="-1"></a>Thus,</span>
<span id="cb28-823"><a href="#cb28-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-824"><a href="#cb28-824" aria-hidden="true" tabindex="-1"></a>$$p_J(\theta) = \frac{1}{\pi n} \sqrt{\frac{n}{\theta(1-\theta)}}$$</span>
<span id="cb28-825"><a href="#cb28-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-826"><a href="#cb28-826" aria-hidden="true" tabindex="-1"></a>$$p_J(\theta) = \underline{\frac{1}{\pi \sqrt{n}}\frac{1}{\sqrt{\theta(1-\theta)}}}$$ {#eq-3-12-prior}</span>
<span id="cb28-827"><a href="#cb28-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-828"><a href="#cb28-828" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b) {#sec-3-12-b}</span></span>
<span id="cb28-829"><a href="#cb28-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-830"><a href="#cb28-830" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Reparameterize the binomial sampling model with $\psi = \log \theta / (1-\theta)$, so that $p(y|\psi) = {n\choose y} e^{\psi y} (1+e^{\psi})^{-n}$. Obtain Jefferys' prior distribution $p_J (\psi)$ for this model.</span></span>
<span id="cb28-831"><a href="#cb28-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-832"><a href="#cb28-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-833"><a href="#cb28-833" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-834"><a href="#cb28-834" aria-hidden="true" tabindex="-1"></a>  p(y|\psi) &amp;= {n\choose y} e^{\psi y} (1+e^{\psi})^{-n}<span class="sc">\\</span></span>
<span id="cb28-835"><a href="#cb28-835" aria-hidden="true" tabindex="-1"></a>  \log(p(y|\psi)) &amp;= {n\choose y} + \psi y \underbrace{\log(e)}_{=1} - n\log(1+e^{\psi})<span class="sc">\\</span></span>
<span id="cb28-836"><a href="#cb28-836" aria-hidden="true" tabindex="-1"></a>  \log(p(y|\psi)) &amp;= {n\choose y} + \psi y  - n\log(1+e^{\psi})<span class="sc">\\</span></span>
<span id="cb28-837"><a href="#cb28-837" aria-hidden="true" tabindex="-1"></a>  \frac{\partial \log p(y|\psi)}{\partial \psi} &amp;= y - n\frac{e^{\psi}}{1+e^{\psi}}<span class="sc">\\</span> </span>
<span id="cb28-838"><a href="#cb28-838" aria-hidden="true" tabindex="-1"></a>  \frac{\partial^2 \log p(y|\psi)}{\partial^2 \psi} &amp;= -n \frac{e^{\psi}}{(1+e^{\psi})^2}<span class="sc">\\</span></span>
<span id="cb28-839"><a href="#cb28-839" aria-hidden="true" tabindex="-1"></a>  E<span class="co">[</span><span class="ot"> \frac{\partial^2 \log p(y|\psi)}{\partial^2 \psi} | \psi</span><span class="co">]</span> &amp;= -n \frac{e^{\psi}}{(1+e^{\psi})^2}<span class="sc">\\</span></span>
<span id="cb28-840"><a href="#cb28-840" aria-hidden="true" tabindex="-1"></a>  I(\psi) = -E<span class="co">[</span><span class="ot"> \frac{\partial^2 \log p(y|\psi)}{\partial^2 \psi} | \psi</span><span class="co">]</span> &amp;= n \frac{e^{\psi}}{(1+e^{\psi})^2}<span class="sc">\\</span> </span>
<span id="cb28-841"><a href="#cb28-841" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-842"><a href="#cb28-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-843"><a href="#cb28-843" aria-hidden="true" tabindex="-1"></a>$\therefore$ $p_{J}(\psi) \propto \sqrt{I(\psi)} = \sqrt{\frac{n e^{\psi}}{(1+e^{\psi})^2}}$</span>
<span id="cb28-844"><a href="#cb28-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-845"><a href="#cb28-845" aria-hidden="true" tabindex="-1"></a>$$p_{J}(\psi) \propto \frac{\sqrt{n e^{\psi}}}{1+e^{\psi}}$$</span>
<span id="cb28-846"><a href="#cb28-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-847"><a href="#cb28-847" aria-hidden="true" tabindex="-1"></a><span class="fu">### (c) </span></span>
<span id="cb28-848"><a href="#cb28-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-849"><a href="#cb28-849" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Take the prior distribution from </span><span class="co">[</span><span class="ot">(a)</span><span class="co">](#sec-3-12-a)</span><span class="at"> and apply the change of variables formula from Exercise 3.10 to obtain the induced prior density on $\psi$. </span></span>
<span id="cb28-850"><a href="#cb28-850" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb28-851"><a href="#cb28-851" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This density should be the same as the one derived in part </span><span class="co">[</span><span class="ot">(b)</span><span class="co">](#sec-3-12-b)</span><span class="at"> of this exercise. This consistency under reparameterization is the defining characteristic of Jeffrey's' prior.</span></span>
<span id="cb28-852"><a href="#cb28-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-853"><a href="#cb28-853" aria-hidden="true" tabindex="-1"></a>$$\psi = g(\theta) = \log<span class="co">[</span><span class="ot">\frac{\theta}{1-\theta}</span><span class="co">]</span>$$</span>
<span id="cb28-854"><a href="#cb28-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-855"><a href="#cb28-855" aria-hidden="true" tabindex="-1"></a>$$\theta = h(\psi) = \frac{e^{\psi}}{1+e^{\psi}}$$</span>
<span id="cb28-856"><a href="#cb28-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-857"><a href="#cb28-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-858"><a href="#cb28-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-859"><a href="#cb28-859" aria-hidden="true" tabindex="-1"></a>From @eq-3-12-prior, $p_{\theta}(\theta) = \frac{1}{\pi \sqrt{n}}\frac{1}{\sqrt{\theta(1-\theta)}}$, </span>
<span id="cb28-860"><a href="#cb28-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-861"><a href="#cb28-861" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb28-862"><a href="#cb28-862" aria-hidden="true" tabindex="-1"></a>  p_{\psi}(\psi) &amp;= \frac{1}{\pi \sqrt{n}} p_{\theta}(h(\psi)) \times |\frac{dh}{d\psi}|<span class="sc">\\</span></span>
<span id="cb28-863"><a href="#cb28-863" aria-hidden="true" tabindex="-1"></a>                 &amp;=  \frac{1}{\pi \sqrt{n}} \frac{1+e^{\psi}}{\sqrt{e^{\psi}(1+e^{\psi}-e^{\psi})}}\times \frac{e^{\psi}}{(1+e^{\psi})^2}<span class="sc">\\</span></span>
<span id="cb28-864"><a href="#cb28-864" aria-hidden="true" tabindex="-1"></a>                 &amp;= \frac{1}{\pi \sqrt{n}} \frac{1+e^{\psi}}{\sqrt{e^{\psi}}}\times \frac{e^{\psi}}{(1+e^{\psi})^2}<span class="sc">\\</span></span>
<span id="cb28-865"><a href="#cb28-865" aria-hidden="true" tabindex="-1"></a>                 &amp;= \frac{1}{\pi\sqrt{n}} \frac{\sqrt{e^{\psi}}}{1+e^{\psi}}<span class="sc">\\</span></span>
<span id="cb28-866"><a href="#cb28-866" aria-hidden="true" tabindex="-1"></a>                 &amp;\propto \frac{\sqrt{e^{\psi}}}{1+e^{\psi}}<span class="sc">\\</span></span>
<span id="cb28-867"><a href="#cb28-867" aria-hidden="true" tabindex="-1"></a>                 &amp;\propto p_{J}(\psi)</span>
<span id="cb28-868"><a href="#cb28-868" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb28-869"><a href="#cb28-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-870"><a href="#cb28-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-871"><a href="#cb28-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-872"><a href="#cb28-872" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden when-format="html"}</span>
<span id="cb28-873"><a href="#cb28-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-874"><a href="#cb28-874" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb28-875"><a href="#cb28-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-876"><a href="#cb28-876" aria-hidden="true" tabindex="-1"></a>:::</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>