---
title: Homework 2
author: 
  - name: Shao-Ting Chiu (UIN:433002162)
    url: stchiu@email.tamu.edu
    affiliation: Department of Electrical and Computer Engineering, Texas A\&M University
date: today
bibliography: ../ref.bib
jupyter: python3  
execute: 
    echo: true
    freeze: auto
---

## Homework Description


> Read Chapter 3 in the Hoff book.
>
> Then, do the following exercises in Hoff: 3.1, 3.3, 3.4, 3.7, 3.12.
>
> For problems that require a computer, please do and derive as much as possible "on paper," and include these derivations in your submission file. Then, for the parts that do require the use of a computer (e.g., creating plots), you are free to use any software (R, Python, ...) of your choosing; no need to include your code in your write-up. Please make sure you create a single file for submission here on Canvas.
> 
> For computations involving gamma functions (e.g., 3.7), it is often helpful to work with log-gamma functions instead, to avoid numbers that are too large to be represented by a computer. In R, the functions lbeta() and lgamma() compute the (natural) log of the beta and gamma functions, respectively. See more here: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Special.html

- [PDF version](https://github.com/stevengogogo/STAT638_Applied-Bayes-Methods/blob/hw/hw2.pdf)
- Deadline: `Sep 20 by 12:01pm`

---

## Computational Enviromnent Setup

### Third-party libraries
``` {python}
%matplotlib inline
import sys # system information
import matplotlib # plotting
import scipy # scientific computing
import pandas as pd # data managing
from scipy.special import comb
from scipy import stats as st
from scipy.special import gamma
import numpy as np
import matplotlib.pyplot as plt
# Matplotlib setting
plt.rcParams['text.usetex'] = True
matplotlib.rcParams['figure.dpi']= 300
```

### Version
``` {python}
print(sys.version)
print(matplotlib.__version__)
print(scipy.__version__)
print(np.__version__)
print(pd.__version__)
```

---

## Problem 3.1

> Sample survey: Suppose we are going to sample 100 individuals from a county (of size much larger than 100) and ask each sampled person whether they support policy $Z$ or not. Let $Y_i = 1$ if person $i$ in the sample supports the policy, and $Y_i = 0$ otherwise.


### (a) 

> Assume $Y_1,\dots, Y_{100}$ are, conditional on $\theta$, i.i.d. binary random variables with expectation $\theta$. Write down the joint distribution of $Pr(Y_1 =y1,\dots, Y_{100} = y_{100}|\theta)$ in a compact form. Also write down the form of $Pr(\sum Y_i = y|\theta)$.

$$Pr(Y_1 = y_1,\dots, Y_{100}=y_{100}|\theta) = \underline{\theta^{\sum_{u=1}^{100}}(1-\theta)^{100-\sum_{u=1}^{100}}}$$

$$Pr(\sum_{i=1}^{100} Y_i = y |\theta)= \underline{{100 \choose y}\theta^{y}(1-\theta)^{100-y}}$$ {#eq-sum-prob}

### (b) 

> For the moment, suppose you believed that $\theta\in\{0.0,0.1,\dots,0.9,1.0\}$. Given that the results of the survey were $\sum^{100}_{i=1} Y_i = 57$, compute $Pr(\sum Y_{i} = 57|\theta)$ for each of these 11 values of $\theta$ and plot these probabilities as a function of $\theta$

From @eq-sum-prob, the sum of supports ($y$) is on the power term. Thus, directly computation is problematic with limited range of floating number. Converting probability to log scale is a way to bypass this problem.Another way is to use `scipy.stats.binom` function[^binom]

The distribution of $Pr(\sum_{i=1}^{100} Y_i = y |\theta)$ along with $\theta\in\{0.0,0.1,\dots,0.9,1.0\}$ is shown in @tbl-binom. The plot of distribution is shown in @fig-binom.


``` {python}
#| label: tbl-binom
#| tbl-cap: "Probabilities along with priors"


thetas = np.linspace(0.0,1.0,11)
tot = 100
probs = np.zeros(len(thetas))
count = 57

for (i, theta) in enumerate(thetas):
  probs[i] = st.binom.pmf(count, tot, theta)

# list of probabilities
pd.DataFrame({"Theta": thetas, "posteriori": probs})
```



``` {python}
#| label: fig-binom
#| fig-cap: "Probabilities along with priors"

plt.plot(thetas, probs, 'ko');
plt.xlabel(r"$\theta$");
plt.ylabel(r"$Pr(\sum Y_i = 57 |\theta)$");
```

[^binom]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html

### (c) {#sec-3-1-c}

> Now suppose you originally had no prior information to believe one of these $\theta$-values over another, and so $Pr(\theta=0.0)=Pr(\theta=0.1)=\dots=Pr(\theta=0.9)=Pr(\theta=1.0)$. Use Bayes' rule to compute $p(\theta|\sum^{n}_{i=1} Y_i = 57)$ for each $\theta$-value. Make a plot of this posterior distribtution as a function of $\theta$.

\begin{align}
  p(\theta_i |\sum^{n}_{i=1} Y_i = 57) &= \frac{p(\sum^{n}_{i=1} Y_i = 57|\theta)p(\theta_i)}{p(\sum^{n}_{i=1} Y_i = 57)}\\
  &= \frac{p(\sum^{n}_{i=1} Y_i = 57|\theta)p(\theta_i)}{\sum_{\theta\in\Theta}p(\sum^{n}_{i=1} Y_i = 57 | \theta)p(\theta)}
\end{align}

The following is the calculation of the posterior distribution (shown in @tbl-post-binom), and the result is shown in @fig-post-binom.

``` {python}
#| label: tbl-post-binom
#| tbl-cap: "Posterior distribution depends on discrete uniform distribution of theta."

p_theta = 1.0/len(thetas)

p_y = np.sum( probs*p_theta)
post_theta = np.zeros(len(thetas))

for (i, theta) in enumerate(thetas):
  post_theta[i] = probs[i]*p_theta/p_y

# list of probabilities
pd.DataFrame({"Theta": thetas, "posteriori":post_theta})
```


``` {python}
#| label: fig-post-binom
#| fig-cap: "Posterior distribution as a function of theta."

plt.plot(thetas, post_theta, 'ko');
plt.xlabel(r"$\theta$");
plt.ylabel(r"$p(\theta_i |\sum^{n}_{i=1} Y_i = 57)$");
```

### (d) 

> Now suppose you allow $\theta$ to be any value in the interval $[0,1]$. Using the uniform prior density for $\theta$, so that $p(\theta) = 1$, plot the posterior density $p(\theta) \times Pr(\sum^{n}_{i=1} Y_i = 57 |\theta)$ as a function of $\theta$.

As shown in @fig-post-binom-cont.

``` {python}
#| label: fig-post-binom-cont
#| fig-cap: "Posterior distribution with continouous uniform prior."

thetas = np.linspace(0,1, 1000)
p_theta = 1.0/len(thetas)
probs = np.zeros(len(thetas))
post_theta = np.zeros(len(thetas))
count = 57
for (i, theta) in enumerate(thetas):
  probs[i] = st.binom.pmf(count, tot, theta)
  post_theta[i] = probs[i]

# Plotting  
plt.plot(thetas, post_theta, 'k-');
plt.xlabel(r"$\theta$");
plt.ylabel(r"$p(\theta_i |\sum^{n}_{i=1} Y_i = 57)$");
```


### (e) 

> As discussed in this chapter, the posterior distribution of $\theta$ is $beta(1+57, 1+100-57)$. Plot the posterior density as a function of $\theta$. Discuss the relationships among all of the plots you have made for this exercise.


The $\theta$ with beta distribution is plotted in @fig-post-beta-cont.

@fig-post-binom is the normalized probability via Bayes' rule (@sec-3-1-c). On the other hand, @fig-binom is not normalized.

@fig-post-binom-cont and @fig-post-beta-cont has similar distribution, which means the prior $\theta$ has little influcence on the posterior distribution. This is because the sample number is large ($n=57$), and decrease the importance of the prior.


``` {python}
#| label: fig-post-beta-cont
#| fig-cap: "Posterior distribution with continouous beta prior."

grid = np.linspace(0,1, 3000)
thetas_rv = st.beta(1+57, 1+100-57)
thetas = [thetas_rv.pdf(x) for x in grid]

p_theta = 1.0/len(thetas)
probs = np.zeros(len(thetas))
post_theta = np.zeros(len(thetas))
count = 57
for (i, theta) in enumerate(thetas):
  probs[i] = st.binom.pmf(count, tot, theta)
  post_theta[i] = probs[i]

# Plotting  
plt.plot(thetas, post_theta, 'k-');
plt.xlabel(r"$\theta\sim beta$");
plt.ylabel(r"$p(\theta_i |\sum^{n}_{i=1} Y_i = 57)$");
```


## Problem 3.3

> Tumor counts: A cancer laboratory is estimating the rate of tumorigenesis in two strains of mice, A and B. They have tumor count data for 10 mice in strain A and 13 mice in strain B. Type A mice have been well studied, and information from other laboratories suggests that type A mice have tumor counts that are approximately Poisson-distributed with a mean of 12. Tumor count rates for type B mice are unknown, but type B mice are related to type A mice. The observed tumor counts for the two populations are
> $$\mathcal{y}_{A} = (12, 9, 12, 14, 13, 13, 15, 8, 15, 6);$$
> $$\mathcal{y}_{B} = (11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7).$$

### (a) 

> Find the posterior distributions, means, variances and $95\%$ quantile-based confidence intervals for $\theta_A$ and $\theta_B$, assuming a Poisson sampling distribution for each group and the following prior distribution:
> $\theta_A \sim gamma(120,10), \theta_B \sim gamma(12,1), p(\theta_A, \theta_B) = p(\theta_A) \times p(\theta_B)$

According to @hoff2009first [pp. 46-47],


$$E[\theta_{*} | y_1,\dots, y_{n_{*}}] = \frac{a_* + \sum_{i=1}^{n_*} y_{i}}{b_* + n_*}$$

where $*\in \{A, B\}$. Given


$\begin{cases}
    \theta_{*} &\sim gamma(a_*,b_*)\\
    Y_1,\dots, Y_{n_*}|\theta_{*} &\sim Poisson(\theta_{*})
\end{cases}$


$$\Rightarrow\{\theta_{i}|Y_1,\dots,Y_{n_*}\}\sim gamma(a + \sum^{n_*}_{i=1} Y_i, b_* + n_*)$$ {#eq-gamma-conj}

The properties of Gamma distribution [@hoff2009first pp. 45-46],

$$p(\theta) = \frac{b^a}{\Gamma(a)}\theta^{a-1}e^{-b\theta}, \quad \theta,a,b > 0$$ 


$$E[\theta] = \frac{a}{b}$$ {#eq-gamma-mean}
$$Var[\theta] = \frac{a}{b^2}$$ {#eq-gamma-var}


**Type A Mice**

|Parameter|Value|
|---|---|
|$a_A$|120|
|$b_A$|10|
|$n_A$|10|
|$\sum_{i=1}^{n_{A}y_{i}}$|$12+9+12+14+13+13+15+8+15+6=117$|
: Parameters of type A mice {#tbl-type-a-mice}

The posterior distribution of mice A:

$$\{\theta_A|Y_1,\dots,Y_{n_A} \sim gamma(120 + 117, 10+10)= gamma(237,20)\} $$

- $E[\theta_{A}|\sum_{i=1}^{n_{A}} Y_{i}] = \frac{237}{20}= \underline{11.85}$
- $Var[\theta_{A}|\sum_{i=1}^{n_{A}} Y_{i}] = \frac{237}{20^2}\approx \underline{0.59}$

- $95\%$ quantile-based confidence intervals is shown in @tbl-33a

``` {python}
#| label: tbl-33a
#| tbl-cap: "95% quantile-based confidence intervals of mice A."

def interval_gamma_95(a,b):
  rvA = st.gamma(a, scale=1/b)
  ints = rvA.interval(0.95)
  return pd.DataFrame({"Left bound":[ints[0]], "Right bound":[ints[1]]})

aA = 237
bA = 20
interval_gamma_95(aA,bA)
```


**Type B Mice**

similarly,

|Parameter|Value|
|---|---|
|$a_B$|12|
|$b_B$|1|
|$n_B$|13|
|$\sum_{i=1}^{n_{B}y_{i}}$|$11+11+10+9+9+8+7+10+6+8+8+9+7=113$|
: Parameters of type B mice {#tbl-type-b-mice}

The posterior distribution of mice B:

$$\{\theta_B|Y_1,\dots,Y_{n_B} \sim gamma(12+113, 1+13)= gamma(125, 14)\} $$

- $E[\theta_{B}|\sum_{i=1}^{n_{B}} Y_{i}] = \frac{125}{14} \approx \underline{8.93}$
- $Var[\theta_{B}|\sum_{i=1}^{n_{B}} Y_{i}] = \frac{125}{14^2}\approx \underline{0.64}$
- $95\%$ quantile-based confidence intervals is shown in @tbl-33b

``` {python}
#| label: tbl-33b
#| tbl-cap: "95% quantile-based confidence intervals of mice B."

aB = 125
bB = 14
interval_gamma_95(aB,bB)
```


### (b)

> Computing and plot the posterior expectation of $\theta_B$ under the prior distribution $\theta_B \sim gamma(12\times n_0, n_0)$ for each value of $n_0\in \{1,2,\dots, 50\}$.
> Descirbe what sort of prior beliefs about $\theta_B$ to be close to that of $\theta_A$.

The posterior distribution can be derived from @eq-gamma-conj. As shown in @fig-3-3-b, the mean value of $\theta_B$ with $n_0$ close to $50$ is necessary to have the similar posterior mean as $\theta_A$.

``` {python}
#| label: fig-3-3-b
#| fig-cap: "Mean of Posterior distribution of mice B with given n0s."

def post_gamma(a,b, sumY, n):
  return st.gamma(a+sumY, scale=1/(b + n))

n0s = np.arange(1, 50, 1)
sumYB = 11+11+10+9+9+8+7+10+6+8+8+9+7
nB = 13
post_theta_rvBs = [post_gamma(12*n0, n0, sumYB, nB) for n0 in n0s]

meanBs = [post_theta_rvBs[i].mean() for i in range(0, len(n0s))]


# Plotting
plt.plot(n0s, meanBs, "ko")
plt.xlabel("$n_0$")
plt.ylabel("$E[Pr(\\theta_{B}|y_{B})]$");
```

### (c)

> Should knowledge about population $A$ tell us anything about population $B$? Discuss whether or not it makes sense to have $p(\theta_A,\theta_B)=p(\theta_A)\times p(\theta_B)$.


The understanding of mice $A$ is well known. Though mice $B$ is related to mice $A$, there is possibility that mice $B$ is different from the distribution of $A$. Thus, viewing mice $A$ and mice $B$ with independent prior distribution makes sense.
 
## Problem 3.4

> Mixtures of beta priors: Estimate the probability $\theta$ of teen recidivism based on a study in which there were $n=43$ individuals released from incarceration and $y=15$ re-offenders within $36$ months.



### (a) {#sec-3-4-a}
> Using a $beta(2,8)$ prior for $\theta$, plot $p(\theta)$, $p(y|\theta)$ and $p(\theta|y)$ as functions of $\theta$. Find the posterior mean, mode, and standard deviation of $\theta$.
> Find a $95\%$ quantile-based condifence interval.

- $p(\theta) \sim beta(2,8)$
  - Plotted in @fig-3-4-prior 

According to @hoff2009first pp. 37-38, the conjugate posterior ($\{\theta|Y=y\}$) given beta as prior is a beta distribution, and $Y\sim binomial(n,\theta)$

- $p(y|\theta) = {n\choose y}\theta^{y}(1-\theta)^{(n-y)}$
  - Plotted in @fig-3-4-likelihood 

- $p(\theta | y) \sim beta(a+y, b + n-y) = beta(2+15, 8 + 43 - 15) = beta(17, 36)$
  - Plotted in @fig-3-4-posterior 
  - $E[p(\theta | y)] = \frac{a}{a+b} = \frac{17}{17+36} \approx 0.32$
  - $Mode(p(\theta | y)) = \frac{a-1}{a+b-2} \approx 0.31$
  - $std(p(\theta | y)) = \sqrt{var[p(\theta | y)]} = \sqrt{\frac{ab}{(a+b)^2 (a+b+1)}} =  \sqrt{\frac{17\times 36}{(17+36)^2 (17+36+1)}}\approx 0.06$
  - Properties are shown in @tbl-3-4-posterior.


``` {python}
#| label: fig-3-4-prior
#| fig-cap: "Prior distribution."

thetas = np.linspace(0,1,1000)
rv_theta = st.beta(2,8)

# Plotting
plt.plot(thetas, rv_theta.pdf(thetas), 'k-')
plt.xlabel("$\\theta$");
plt.ylabel("$Pr(\\theta)$");
```



``` {python}
#| label: fig-3-4-likelihood
#| fig-cap: "Likelihood"

thetas = np.linspace(0,1,1000)
n = 43
y = 15

pr_like = [st.binom.pmf(y, n, theta) for theta in thetas]

# Plotting
plt.plot(thetas, pr_like, 'k-')
plt.xlabel("$\\theta$");
plt.ylabel("$Pr(y|\\theta)$");
```

``` {python}
#| label: fig-3-4-posterior
#| fig-cap: "Posterior distribution"

thetas = np.linspace(0,1,1000)
rv_theta = st.beta(2+15, 8 + 43 - 15)

# Plotting
plt.plot(thetas, rv_theta.pdf(thetas), 'k-')
plt.xlabel("$\\theta$");
plt.ylabel("$Pr(\\theta | y)$");
```

``` {python}
#| label: tbl-3-4-posterior
#| tbl-cap: "Properties of posterior distribution."


ints = rv_theta.interval(0.95)

pd.DataFrame({"Properties": ["Left bound (CI)", "Right bound (CI)", "mean", "mode", "standard deviation"], "Values": [ints[0], ints[1], rv_theta.mean(), (17-1)/(17+36-2), rv_theta.std()]})
```

### (b) {#sec-3-4-b}

> Repeat [(a)](#sec-3-4-a), but using a $beta(8,2)$ prior for $\theta$.




- $p(\theta) \sim beta(8,2)$
  - Plotted in @fig-3-4-prior-2 

- $p(y|\theta) = {n\choose y}\theta^{y}(1-\theta)^{(n-y)}$
  - Plotted in @fig-3-4-likelihood-2

- $p(\theta | y) \sim beta(a+y, b + n-y) = beta(8+15, 2 + 43 - 15) = beta(23, 30)$
  - Plotted in @fig-3-4-posterior-2 
  - $E[p(\theta | y)] = \frac{a}{a+b} = \frac{23}{23+30} \approx 0.434$
  - $Mode(p(\theta | y)) = \frac{a-1}{a+b-2} \approx 0.431$
  - $std(p(\theta | y)) = \sqrt{var[p(\theta | y)]} = \sqrt{\frac{ab}{(a+b)^2 (a+b+1)}} =  \sqrt{\frac{23\times 30}{(23+30)^2 (23+30+1)}}\approx 0.07$
  - Properties are shown in @tbl-3-4-posterior-2.


``` {python}
#| label: fig-3-4-prior-2
#| fig-cap: "Prior distribution."

thetas = np.linspace(0,1,1000)
rv_theta = st.beta(8,2)

# Plotting
plt.plot(thetas, rv_theta.pdf(thetas), 'k-')
plt.xlabel("$\\theta$");
plt.ylabel("$Pr(\\theta)$");
```



``` {python}
#| label: fig-3-4-likelihood-2
#| fig-cap: "Likelihood"

thetas = np.linspace(0,1,1000)
n = 43
y = 15

pr_like = [st.binom.pmf(y, n, theta) for theta in thetas]

# Plotting
plt.plot(thetas, pr_like, 'k-')
plt.xlabel("$\\theta$");
plt.ylabel("$Pr(y|\\theta)$");
```

``` {python}
#| label: fig-3-4-posterior-2
#| fig-cap: "Posterior distribution"

thetas = np.linspace(0,1,1000)
rv_theta = st.beta(8+15, 2 + 43 - 15)

# Plotting
plt.plot(thetas, rv_theta.pdf(thetas), 'k-')
plt.xlabel("$\\theta$");
plt.ylabel("$Pr(\\theta | y)$");
```

``` {python}
#| label: tbl-3-4-posterior-2
#| tbl-cap: "Properties of posterior distribution."


ints = rv_theta.interval(0.95)

pd.DataFrame({"Properties": ["Left bound (CI)", "Right bound (CI)", "mean", "mode", "standard deviation"], "Values": [ints[0], ints[1], rv_theta.mean(), (23-1)/(23+30-2), rv_theta.std()]})
```






### (c) {#sec-3-4-c}
> Consider the following prior distribution for $\theta$: 
> $$p(\theta) = \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}[3\theta(1-\theta)^7+\theta^7(1-\theta)]$$
> which is a $75-25\%$ mixture of a $beta(2,8)$ and a $beta(8,2)$ prior distribution. Plot this prior distribution and compare it to the priors in [(a)](#sec-3-4-a) and [(b)](#sec-3-4-b). Describe what sort of prior opinion this may represent.


The mixture of beta distribution is plotted in @fig-3-4-mixbeta. This opinion merges two opposite suggestions with different weights:

1. $\theta$ is low ([@fig-3-4-prior]).
2. $\theta$ is high ([@fig-3-4-prior-2]).

``` {python}
#| label: fig-3-4-mixbeta
#| fig-cap: "Mixture beta distribution"

def mixBeta(th):
  return 0.25*gamma(10)/(gamma(2)*gamma(8))*( 3*th*((1-th)**7) + (th**7)*(1-th) )

thetas = np.linspace(0,1, 1000)
prs = [mixBeta(theta) for theta in thetas]

# Plotting
plt.plot(thetas, prs, "k-")
plt.xlabel("$\\theta$")
plt.ylabel("$p(\\theta)$");
```

### (d) {#sec-3-4-d}

> For the prior in [(c)](#sec-3-4-c):
>
> 1. Write out mathematically $p(\theta)\times p(y|\theta)$ and simplify as much as possible.
>
> 2. The posterior distribution is a mixture of two distributions you know. Identify these distributions.
>
> 3. On a computer, calculate and plot $p(\theta) \times p(y|\theta)$ for a variety of $\theta$ values. Also find (approximately) the posterior mode, and discuss its relation to the modes in [(a)](#sec-3-4-a) and [(b)](#sec-3-4-b).


**Part 1.**

Noted that ${43 \choose 15} = \frac{43!}{15!28!}=\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}$

\begin{align}
  p(\theta)\times p(y|\theta) &= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}[3\theta(1-\theta)^7+\theta^7(1-\theta)] \times {43\choose 15}\theta^{15}(1-\theta)^{(43-15)}\\
  &= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}\underbrace{{43 \choose 15}}_{\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}}(\theta^{22}(1-\theta)^{29} + 3\theta^{16}(1-\theta)^{35})\\
  &= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}(\underline{\theta^{22}(1-\theta)^{29}} + \underline{3\theta^{16}(1-\theta)^{35}})\\
\end{align}


*The simplification is by the aid of walfram-alpha[^wolfram].*

[^wolfram]: https://www.wolframalpha.com/input?i=%283*x*%281-x%29%5E7+%2B+x%5E7+*+%281-x%29%29+*x%5E15+*+%281-x%29%5E28

**Part 2.**

The distribution is the mixture of $Beta(23,30)$ and $Beta(17,36)$ with certain weights.

**Part 3.**

- The mode of $p(\theta) \times p(y|\theta)$  is $0.314$ (See [@fig-3-4-3-mixbeta]).
- The mode in [(a)](#sec-3-4-a): $0.313725$
- The mode in [(b)](#sec-3-4-b): $0.431373$

Thus, the posterior distribution has the mode between $Beta(2,8)$ ([(a)](#sec-3-4-a)) and $Beta(8,2)$([(b)](#sec-3-4-b)), and more close to $Beta(2.8)$

``` {python}
#| label: fig-3-4-3-mixbeta
#| fig-cap: "Posterior distribution with mixture of two beta distributions."

def mixture_post(th):
  scale = 0.25 * gamma(10)/(gamma(2)*gamma(8)) * gamma(44)/(gamma(16)*gamma(29))
  beta = (th**22)*(1-th)**29 + 3*(th**16)*(1-th)**35
  return scale*beta

prs = [mixture_post(theta) for theta in thetas]

maxTh = thetas[np.argmax(prs)]

plt.plot(thetas, prs, 'k-')
plt.xlabel("$\\theta$")
plt.ylabel("$p(\\theta)\\times p(y|\\theta)$");
plt.axvline(x=maxTh, linestyle='--', color='k', label= "Mode={}".format(maxTh));
plt.legend();
```


### (e)
> Find a general formula for the weights of the mixture distribution in [(d) 2.](#sec-3-4-d), and provide an interpretation for their values.

Let $c_1 =  \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}$

\begin{align}
  p(\theta)\times p(y|\theta) &= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}\frac{\Gamma(44)}{\Gamma(16)\Gamma(29)}(\underline{\theta^{22}(1-\theta)^{29}} + \underline{3\theta^{16}(1-\theta)^{35}})\\ 
                              &= c_1 (\theta^{22}(1-\theta)^{29} + 3\theta^{16}(1-\theta)^{35})\\
                              &= c_1 \theta^{22}(1-\theta)^{29} + 3c_1 \theta^{16}(1-\theta)^{35})\\
                              &= c_1 \frac{\Gamma(23)\Gamma(30)}{\Gamma(53)} Beta(\theta, 23,30) + 3 c_1 \frac{\Gamma(17)\Gamma(36)}{\Gamma(51)} Beta(\theta, 17,36)\\ 
                              &= 0.0003 \times Beta(\theta, 23,30) + 58.16 \times Beta(\theta, 17,36)\\
                              &= \omega_1 \cdot Beta(\theta, 23,30) + \omega_2 \cdot Beta(\theta, 17,36)
\end{align}

That means $Beta(17,36)$ is preferred to $Beta(23,30)$. The updated posterior information is more close to the [(a)](#sec-3-4-d). That is because the mixture of priors has more weights ($75\%$) on the prior of $Beta(2,8)$.

## Problem 3.7

> Posterior prediction: Consider a pilot study in which $n_1 = 15$ children enrolled in special education classes were randomly selected and tested for a certain type of learning disability. In the pilot study, $y_1 = 2$ children tested positive for the disability.

### (a)
> Using a uniform prior distribution, find the posterior distribution of $\theta$, the fraction of students in special education classes who have the disability. Find the posterior mean, mode and standard deviation of $\theta$, and plot the posterior density.

$$\theta \sim beta(1,1) (uniform)$$
$$Y\sim binomial(n_1,\theta)$$

\begin{align}
  \theta|Y=y &\sim beta(1+y_1, 1+n_1 - y_1)\\ 
             &= beta(1 + 2, 1+15-2)\\ 
             &= beta(3, 14)\\ 
             &= beta(a_p, b_p)\\ 
\end{align}

- The distribution is plotted in @fig-3-7-a
- $E[\theta|Y] = \frac{a_p}{a_p+b_p} = \frac{3}{3+14} \approx 0.1764$
- $Mode[\theta|Y] = \frac{(a_p - 1)}{a_p - 1 + b_p - 1} = \frac{(3 - 1)}{3 - 1 + 14 - 1} \approx 0.1333$
- $Std[\theta|Y] = \sqrt{ \frac{a_p b_p}{(a_p+b_p+1)(a_p+b_p)^2} } = \sqrt{ \frac{3\cdot 14}{(3+14+1)(3+14)^2}} \approx 0.0899$

``` {python}
#| label: fig-3-7-a
#| fig-cap: "Posterior distribution"

thetas = np.linspace(0,1,1000)
pos = st.beta(3, 14)
pr_pos = [pos.pdf(theta) for theta in thetas]

plt.plot(thetas, pr_pos, "k-")
plt.xlabel("$\\theta$")
plt.ylabel("$Pr(\\theta|Y)$");
```

> Researchers would like to recruit students with the disability to participate in a long-term study, but first they need to make sure they can recruit enough students. Let $n_2 = 278$ be the number of children in special education classes in this particular school district, and let $Y_2$ be the number of students with the disability.

### (b) 

> Find $Pr(Y_2=y_2|Y_1 =2)$, the posterior predictive distribution of $Y_2$, as follows:
>
> 1. Discuss what assumptions are needed about the joint distribution of $(Y_1, Y_2)$ such that the fololowing is true:
> $$Pr(Y_2=y_2 |Y_1=2) = \int^{1}_{0} Pr(Y_2=y_2|\theta)p(\theta|Y_1=2)d\theta$$
> 
> 2. Now plug in the forms of $Pr(Y_2=y_2|\theta)$ and $p(\theta|Y_1 =2)$ in the above integral.
>
> 3. Figure out what the above integral must be by using the calculus result discussed in Section 3.1.

**Part 1**


**Part 2**


**Part 3**

### (c) {#sec-3-7-c}
> Plot the function $Pr(Y_2 = y_2 | Y_1 =2)$ as a function of $y_2$. Obtain the mean and standard deviation of $Y_2$, given $Y_1 = 2$.


### (d)
> The posterior mode and the MLE (maximum likelihood estimate) of $\theta$, based on data from the pilot study, are both $\hat{\theta} = \frac{2}{15}$. Plot the distribution $Pr(Y_2 = y_2|\theta=\hat{\theta})$, and find the mean and standard deviation of $Y_2$ given $\theta=\hat{\theta}$. Compare these results to the plots and calculation in [(c)](#sec-3-7-c) and discuss any differences. Which distribution for $Y_2$ would you used to make predictions, and why?




## Problem 3.12

> Jeffrey's prior: Jeffreys (1961) suggested a default rule for gnerating a prior distribution of a parameter $\theta$ in a sampling model $p(y|\theta)$. Jeffreys' prior is given by $p_{J}\propto \sqrt{I(\theta)}$, where $I(\theta) = - E[\frac{\partial^{2} \log p(Y|\theta)}{\partial\theta^2} | \theta]$ is the *Fisher information*.

### (a) {#sec-3-12-a}
> Let $Y\sim binomial(n,\theta)$. Obtain Jeffreys' prior  distribution $p_J(\theta)$ for this model.

$\because$ $Y\sim binomial(n,\theta)$ $\therefore$ $E[Y]=n\theta$

\begin{align}
p(y|\theta) &= {n\choose y}\theta^{y}(1-\theta)^{n-y}\\ 
\log(p(y|\theta)) &= \log {n\choose y} + y\log \theta + (n-y)\log (1-\theta)\\
\frac{\partial \log(p(y|\theta))}{\partial \theta} &= \frac{y}{\theta} - \frac{n-y}{1-\theta}\\
\frac{\partial^2 \log(p(y|\theta))}{\partial^2 \theta} &= \frac{-y}{\theta^2} - \frac{n-y}{(1-\theta)^2}\\
E[\frac{\partial^2 \log(p(y|\theta))}{\partial^2 \theta} |\theta] &= -\frac{n\theta}{\theta^2} - \frac{n-n\theta}{(1-\theta)^2}\\
E[\frac{\partial^2 \log(p(y|\theta))}{\partial^2 \theta} |\theta] &= \frac{-n}{\theta} - \frac{n}{1-\theta}\\
- E[\frac{\partial^2 \log(p(y|\theta))}{\partial^2 \theta} |\theta] &= \frac{n}{\theta} + \frac{n}{1-\theta}\\
I(\theta) &= \frac{n}{\theta} + \frac{n}{1-\theta}\\
          &= \frac{n}{\theta(1-\theta)}\\
\end{align}

$\because$ $p_J \propto \sqrt{I(\theta)}$

\begin{align}
  p_J(\theta) \propto &\sqrt{I(\theta)}\\
                      &= \sqrt{\frac{n}{\theta(1-\theta)}}\\
\end{align}


Let $c$ be the scalar. By the fact that $\frac{d}{dx}(\sin^{-1}x)=\frac{1}{\sqrt{1-x^2}}$,

$$P_J (\theta) = c \times \sqrt{\frac{n}{\theta(1-\theta)}}$$ 

\begin{align}
1 &= c \int^{1}_{0} \sqrt{\frac{n}{\theta(1-\theta)}} d\theta\\
1 &= nc \int^{1}_{0} \sqrt{\frac{1}{\theta(1-\theta)}} d\theta\\
1 &= nc \left[  -2 \sin^{-1}(\sqrt{1-x})   \right]^{1}_{0} \\
1 &= -2\times nc (\underbrace{\sin^{-1}(0)}_{=0} - \underbrace{\sin^{-1}(1)}_{=\frac{\pi}{2}})\\ 
1 &= \pi n c \\ 
c &= \frac{1}{\pi n}\\
\end{align}

Thus,

$$p_J(\theta) = \frac{1}{\pi n} \sqrt{\frac{n}{\theta(1-\theta)}}$$

$$p_J(\theta) = \underline{\frac{1}{\pi \sqrt{n}}\frac{1}{\sqrt{\theta(1-\theta)}}}$$ {#eq-3-12-prior}

### (b) {#sec-3-12-b}

> Reparameterize the binomial sampling model with $\psi = \log \theta / (1-\theta)$, so that $p(y|\psi) = {n\choose y} e^{\psi y} (1+e^{\psi})^{-n}$. Obtain Jefferys' prior distribution $p_J (\psi)$ for this model.


\begin{align}
  p(y|\psi) &= {n\choose y} e^{\psi y} (1+e^{\psi})^{-n}\\
  \log(p(y|\psi)) &= {n\choose y} + \psi y \underbrace{\log(e)}_{=1} - n\log(1+e^{\psi})\\
  \log(p(y|\psi)) &= {n\choose y} + \psi y  - n\log(1+e^{\psi})\\
  \frac{\partial \log p(y|\psi)}{\partial \psi} &= y - n\frac{e^{\psi}}{1+e^{\psi}}\\ 
  \frac{\partial^2 \log p(y|\psi)}{\partial^2 \psi} &= -n \frac{e^{\psi}}{(1+e^{\psi})^2}\\
  E[ \frac{\partial^2 \log p(y|\psi)}{\partial^2 \psi} | \psi] &= -n \frac{e^{\psi}}{(1+e^{\psi})^2}\\
  I(\psi) = -E[ \frac{\partial^2 \log p(y|\psi)}{\partial^2 \psi} | \psi] &= n \frac{e^{\psi}}{(1+e^{\psi})^2}\\ 
\end{align}

$\therefore$ $p_{J}(\psi) \propto \sqrt{I(\psi)} = \sqrt{\frac{n e^{\psi}}{(1+e^{\psi})^2}}$

$$p_{J}(\psi) \propto \frac{\sqrt{n e^{\psi}}}{1+e^{\psi}}$$

### (c) 

> Take the prior distribution from [(a)](#sec-3-12-a) and apply the change of variables formula from Exercise 3.10 to obtain the induced prior density on $\psi$. 
> 
> This density should be the same as the one derived in part [(b)](#sec-3-12-b) of this exercise. This consistency under reparameterization is the defining characteristic of Jeffrey's' prior.

$$\psi = g(\theta) = \log[\frac{\theta}{1-\theta}]$$

$$\theta = h(\psi) = \frac{e^{\psi}}{1+e^{\psi}}$$



From @eq-3-12-prior, $p_{\theta}(\theta) = \frac{1}{\pi \sqrt{n}}\frac{1}{\sqrt{\theta(1-\theta)}}$, 

\begin{align}
  p_{\psi}(\psi) &= \frac{1}{\pi \sqrt{n}} p_{\theta}(h(\psi)) \times |\frac{dh}{d\psi}|\\
                 &=  \frac{1}{\pi \sqrt{n}} \frac{1+e^{\psi}}{\sqrt{e^{\psi}(1+e^{\psi}-e^{\psi})}}\times \frac{e^{\psi}}{(1+e^{\psi})^2}\\
                 &= \frac{1}{\pi \sqrt{n}} \frac{1+e^{\psi}}{\sqrt{e^{\psi}}}\times \frac{e^{\psi}}{(1+e^{\psi})^2}\\
                 &= \frac{1}{\pi\sqrt{n}} \frac{\sqrt{e^{\psi}}}{1+e^{\psi}}\\
                 &\propto \frac{\sqrt{e^{\psi}}}{1+e^{\psi}}\\
                 &\propto p_{J}(\psi)
\end{align}



::: {.content-hidden when-format="html"}

## References

:::