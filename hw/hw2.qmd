---
title: Homework 2
author: 
  - name: Shao-Ting Chiu (UIN:433002162)
    url: stchiu@email.tamu.edu
    affiliation: Department of Electrical and Computer Engineering, Texas A\&M University
date: today
bibliography: ../ref.bib
jupyter: python3  
execute: 
    echo: true
    freeze: auto
---

## Homework Description


> Read Chapter 3 in the Hoff book.
>
> Then, do the following exercises in Hoff: 3.1, 3.3, 3.4, 3.7, 3.12.
>
> For problems that require a computer, please do and derive as much as possible "on paper," and include these derivations in your submission file. Then, for the parts that do require the use of a computer (e.g., creating plots), you are free to use any software (R, Python, ...) of your choosing; no need to include your code in your write-up. Please make sure you create a single file for submission here on Canvas.
> 
> For computations involving gamma functions (e.g., 3.7), it is often helpful to work with log-gamma functions instead, to avoid numbers that are too large to be represented by a computer. In R, the functions lbeta() and lgamma() compute the (natural) log of the beta and gamma functions, respectively. See more here: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Special.html

- [PDF version](https://github.com/stevengogogo/STAT638_Applied-Bayes-Methods/blob/hw/hw2.pdf)
- Deadline: `Sep 20 by 12:01pm`

---

## Computational Enviromnent Setup

### Third-party libraries
``` {python}
%matplotlib inline
import sys # system information
import matplotlib # plotting
import scipy # scientific computing
import pandas as pd # data managing
from scipy.special import comb
from scipy import stats as st
import numpy as np
import matplotlib.pyplot as plt
# Matplotlib setting
plt.rcParams['text.usetex'] = True
matplotlib.rcParams['figure.dpi']= 300
```

### Version
``` {python}
print(sys.version)
print(matplotlib.__version__)
print(scipy.__version__)
print(np.__version__)
print(pd.__version__)
```

---

## Problem 3.1

> Sample survey: Suppose we are going to sample 100 individuals from a county (of size much larger than 100) and ask each sampled person whether they support policy $Z$ or not. Let $Y_i = 1$ if person $i$ in the sample supports the policy, and $Y_i = 0$ otherwise.


### (a) 

> Assume $Y_1,\dots, Y_{100}$ are, conditional on $\theta$, i.i.d. binary random variables with expectation $\theta$. Write down the joint distribution of $Pr(Y_1 =y1,\dots, Y_{100} = y_{100}|\theta)$ in a compact form. Also write down the form of $Pr(\sum Y_i = y|\theta)$.

$$Pr(Y_1 = y_1,\dots, Y_{100}=y_{100}|\theta) = \underline{\theta^{\sum_{u=1}^{100}}(1-\theta)^{100-\sum_{u=1}^{100}}}$$

$$Pr(\sum_{i=1}^{100} Y_i = y |\theta)= \underline{{100 \choose y}\theta^{y}(1-\theta)^{100-y}}$$ {#eq-sum-prob}

### (b) 

> For the moment, suppose you believed that $\theta\in\{0.0,0.1,\dots,0.9,1.0\}$. Given that the results of the survey were $\sum^{100}_{i=1} Y_i = 57$, compute $Pr(\sum Y_{i} = 57|\theta)$ for each of these 11 values of $\theta$ and plot these probabilities as a function of $\theta$

From @eq-sum-prob, the sum of supports ($y$) is on the power term. Thus, directly computation is problematic with limited range of floating number. Converting probability to log scale is a way to bypass this problem.Another way is to use `scipy.stats.binom` function[^binom]

The distribution of $Pr(\sum_{i=1}^{100} Y_i = y |\theta)$ along with $\theta\in\{0.0,0.1,\dots,0.9,1.0\}$ is shown in @tbl-binom. The plot of distribution is shown in @fig-binom.


``` {python}
#| label: tbl-binom
#| tbl-cap: "Probabilities along with priors"


thetas = np.linspace(0.0,1.0,11)
tot = 100
probs = np.zeros(len(thetas))
count = 57

for (i, theta) in enumerate(thetas):
  probs[i] = st.binom.pmf(count, tot, theta)

# list of probabilities
pd.DataFrame({"Theta": thetas, "posteriori": probs})
```



``` {python}
#| label: fig-binom
#| fig-cap: "Probabilities along with priors"

plt.plot(thetas, probs, 'ko');
plt.xlabel(r"$\theta$");
plt.ylabel(r"$Pr(\sum Y_i = 57 |\theta)$");
```

[^binom]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html

### (c) {#sec-3-1-c}

> Now suppose you originally had no prior information to believe one of these $\theta$-values over another, and so $Pr(\theta=0.0)=Pr(\theta=0.1)=\dots=Pr(\theta=0.9)=Pr(\theta=1.0)$. Use Bayes' rule to compute $p(\theta|\sum^{n}_{i=1} Y_i = 57)$ for each $\theta$-value. Make a plot of this posterior distribtution as a function of $\theta$.

\begin{align}
  p(\theta_i |\sum^{n}_{i=1} Y_i = 57) &= \frac{p(\sum^{n}_{i=1} Y_i = 57|\theta)p(\theta_i)}{p(\sum^{n}_{i=1} Y_i = 57)}\\
  &= \frac{p(\sum^{n}_{i=1} Y_i = 57|\theta)p(\theta_i)}{\sum_{\theta\in\Theta}p(\sum^{n}_{i=1} Y_i = 57 | \theta)p(\theta)}
\end{align}

The following is the calculation of the posterior distribution (shown in @tbl-post-binom), and the result is shown in @fig-post-binom.

``` {python}
#| label: tbl-post-binom
#| tbl-cap: "Posterior distribution depends on discrete uniform distribution of theta."

p_theta = 1.0/len(thetas)

p_y = np.sum( probs*p_theta)
post_theta = np.zeros(len(thetas))

for (i, theta) in enumerate(thetas):
  post_theta[i] = probs[i]*p_theta/p_y

# list of probabilities
pd.DataFrame({"Theta": thetas, "posteriori":post_theta})
```


``` {python}
#| label: fig-post-binom
#| fig-cap: "Posterior distribution as a function of theta."

plt.plot(thetas, post_theta, 'ko');
plt.xlabel(r"$\theta$");
plt.ylabel(r"$p(\theta_i |\sum^{n}_{i=1} Y_i = 57)$");
```

### (d) 

> Now suppose you allow $\theta$ to be any value in the interval $[0,1]$. Using the uniform prior density for $\theta$, so that $p(\theta) = 1$, plot the posterior density $p(\theta) \times Pr(\sum^{n}_{i=1} Y_i = 57 |\theta)$ as a function of $\theta$.

As shown in @fig-post-binom-cont.

``` {python}
#| label: fig-post-binom-cont
#| fig-cap: "Posterior distribution with continouous uniform prior."

thetas = np.linspace(0,1, 1000)
p_theta = 1.0/len(thetas)
probs = np.zeros(len(thetas))
post_theta = np.zeros(len(thetas))
count = 57
for (i, theta) in enumerate(thetas):
  probs[i] = st.binom.pmf(count, tot, theta)
  post_theta[i] = probs[i]

# Plotting  
plt.plot(thetas, post_theta, 'k-');
plt.xlabel(r"$\theta$");
plt.ylabel(r"$p(\theta_i |\sum^{n}_{i=1} Y_i = 57)$");
```


### (e) 

> As discussed in this chapter, the posterior distribution of $\theta$ is $beta(1+57, 1+100-57)$. Plot the posterior density as a function of $\theta$. Discuss the relationships among all of the plots you have made for this exercise.


The $\theta$ with beta distribution is plotted in @fig-post-beta-cont.

@fig-post-binom is the normalized probability via Bayes' rule (@sec-3-1-c). On the other hand, @fig-binom is not normalized.

@fig-post-binom-cont and @fig-post-beta-cont has similar distribution, which means the prior $\theta$ has little influcence on the posterior distribution. This is because the sample number is large ($n=57$), and decrease the importance of the prior.


``` {python}
#| label: fig-post-beta-cont
#| fig-cap: "Posterior distribution with continouous beta prior."

grid = np.linspace(0,1, 3000)
thetas_rv = st.beta(1+57, 1+100-57)
thetas = [thetas_rv.pdf(x) for x in grid]

p_theta = 1.0/len(thetas)
probs = np.zeros(len(thetas))
post_theta = np.zeros(len(thetas))
count = 57
for (i, theta) in enumerate(thetas):
  probs[i] = st.binom.pmf(count, tot, theta)
  post_theta[i] = probs[i]

# Plotting  
plt.plot(thetas, post_theta, 'k-');
plt.xlabel(r"$\theta\sim beta$");
plt.ylabel(r"$p(\theta_i |\sum^{n}_{i=1} Y_i = 57)$");
```


## Problem 3.3

> Tumor counts: A cancer laboratory is estimating the rate of tumorigenesis in two strains of mice, A and B. They have tumor count data for 10 mice in strain A and 13 mice in strain B. Type A mice have been well studied, and information from other laboratories suggests that type A mice have tumor counts that are approximately Poisson-distributed with a mean of 12. Tumor count rates for type B mice are unknown, but type B mice are related to type A mice. The observed tumor counts for the two populations are
> $$\mathcal{y}_{A} = (12, 9, 12, 14, 13, 13, 15, 8, 15, 6);$$
> $$\mathcal{y}_{B} = (11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7).$$

### (a) 

> Find the posterior distributions, means, variances and $95\%$ quantile-based confidence intervals for $\theta_A$ and $\theta_B$, assuming a Poisson sampling distribution for each group and the following prior distribution:
> $\theta_A \sim gamma(120,10), \theta_B \sim gamma(12,1), p(\theta_A, \theta_B) = p(\theta_A) \times p(\theta_B)$

According to @hoff2009first [pp. 46-47],


$$E[\theta_{*} | y_1,\dots, y_{n_{*}}] = \frac{a_* + \sum_{i=1}^{n_*} y_{i}}{b_* + n_*}$$

where $*\in \{A, B\}$. Given


$\begin{cases}
    \theta_{*} &\sim gamma(a_*,b_*)\\
    Y_1,\dots, Y_{n_*}|\theta_{*} &\sim Poisson(\theta_{*})
\end{cases}$


$$\Rightarrow\{\theta_{i}|Y_1,\dots,Y_{n_*}\}\sim gamma(a + \sum^{n_*}_{i=1} Y_i, b_* + n_*)$$ {#eq-gamma-conj}

The properties of Gamma distribution [@hoff2009first pp. 45-46],

$$p(\theta) = \frac{b^a}{\Gamma(a)}\theta^{a-1}e^{-b\theta}, \quad \theta,a,b > 0$$ 


$$E[\theta] = \frac{a}{b}$$ {#eq-gamma-mean}
$$Var[\theta] = \frac{a}{b^2}$$ {#eq-gamma-var}


**Type A Mice**

|Parameter|Value|
|---|---|
|$a_A$|120|
|$b_A$|10|
|$n_A$|10|
|$\sum_{i=1}^{n_{A}y_{i}}$|$12+9+12+14+13+13+15+8+15+6=117$|
: Parameters of type A mice {#tbl-type-a-mice}

The posterior distribution of mice A:

$$\{\theta_A|Y_1,\dots,Y_{n_A} \sim gamma(120 + 117, 10+10)= gamma(237,20)\} $$

- $E[\theta_{A}|\sum_{i=1}^{n_{A}} Y_{i}] = \frac{237}{20}= \underline{11.85}$
- $Var[\theta_{A}|\sum_{i=1}^{n_{A}} Y_{i}] = \frac{237}{20^2}\approx \underline{0.59}$

- $95\%$ quantile-based confidence intervals is shown in @tbl-33a

``` {python}
#| label: tbl-33a
#| tbl-cap: "95% quantile-based confidence intervals of mice A."

def interval_gamma_95(a,b):
  rvA = st.gamma(a, scale=1/b)
  ints = rvA.interval(0.95)
  return pd.DataFrame({"Left bound":[ints[0]], "Right bound":[ints[1]]})

aA = 237
bA = 20
interval_gamma_95(aA,bA)
```


**Type B Mice**

similarly,

|Parameter|Value|
|---|---|
|$a_B$|12|
|$b_B$|1|
|$n_B$|13|
|$\sum_{i=1}^{n_{B}y_{i}}$|$11+11+10+9+9+8+7+10+6+8+8+9+7=113$|
: Parameters of type B mice {#tbl-type-b-mice}

The posterior distribution of mice B:

$$\{\theta_B|Y_1,\dots,Y_{n_B} \sim gamma(12+113, 1+13)= gamma(125, 14)\} $$

- $E[\theta_{B}|\sum_{i=1}^{n_{B}} Y_{i}] = \frac{125}{14} \approx \underline{8.93}$
- $Var[\theta_{B}|\sum_{i=1}^{n_{B}} Y_{i}] = \frac{125}{14^2}\approx \underline{0.64}$
- $95\%$ quantile-based confidence intervals is shown in @tbl-33b

``` {python}
#| label: tbl-33b
#| tbl-cap: "95% quantile-based confidence intervals of mice B."

aB = 125
bB = 14
interval_gamma_95(aB,bB)
```


### (b)

> Computing and plot the posterior expectation of $\theta_B$ under the prior distribution $\theta_B \sim gamma(12\times n_0, n_0)$ for each value of $n_0\in \{1,2,\dots, 50\}$.
> Descirbe what sort of prior beliefs about $\theta_B$ to be close to that of $\theta_A$.



### (c)

> Should knowledge about population $A$ tell us anything about population $B$? Discuss whether or not it makes sense to have $p(\theta_A,\theta_B)=p(\theta_A)\times p(\theta_B)$.
 
## Problem 3.4

> Mixtures of beta priors: Estimate the probability $\theta$ of teen recidivism based on a study in which there were $n=43$ individuals released from incarceration and $y=15$ re-offenders within $36$.


### (a) {#sec-3-4-a}
> Using a $beta(2,8)$ prior for $\theta$, plot $p(\theta)$, $p(y|\theta)$ and $p(\theta|y)$ as functions of $\theta$. Find the posterior mean, mode, and standard deviation of $\theta$.
> Find a $95\%$ quantile-based condifence interval.


### (b) {#sec-3-4-b}

> Repeat [(a)](#sec-3-4-a), but using a $beta(8,2)$ prior for $\theta$.

### (c) {#sec-3-4-c}
> Consider the following prior distribution for $\theta$: 
> $$p(\theta) = \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2)\Gamma(8)}[3\theta(1-\theta)^7+\theta^7(1-\theta)]$$
> which is a $75-25\%$ mixture of a $beta(2,8)$ and a $beta(8,2)$ prior distribution. Plot this prior distribution and compare it to the priors in [(a)](#sec-3-4-a) and [(b)](#sec-3-4-b). Describe what sort of prior opinion this may represent.

### (d) {#sec-3-4-d}

> For the prior in [(c)](#sec-3-4-c):
>
> 1. Write out mathematically $p(\theta)\times p(y|\theta)$ and simplify as much as possible.
>
> 2. The posterior distribution is a mixture of two distributions you know. Identify these distributions.
>
> 3. On a computer, calculate and plot $p(\theta) \times p(y|\theta)$ for a variety of $\theta$ values. Also find (approximately) the posterior mode, and discuss its relation to the modes in [(a)](#sec-3-4-a) and [(b)](#sec-3-4-b).


### (e)
> Find a general formula for the weights of the mixture distribution in [(d) 2.](#sec-3-4-d), and provide an interpretation for their values.

## Problem 3.7

> Posterior prediction: Consider a pilot study in which $n_1 = 15$ children enrolled in special education classes were randomly selected and tested for a certain type of learning disability. In the pilot study, $y_1 = 2$ children tested positive for the disability.

### (a)
> Using a uniform prior distribution, find the posterior distribution of $\theta$, the fraction of students in special education classes who have the disability. Find the posterior mean, mode and standard deviation of $\theta$, and plot the posterior density.




> Researchers would like to recruit students with the disability to participate in a long-term study, but first they need to make sure they can recruit enough students. Let $n_2 = 278$ be the number of children in special education classes in this particular school district, and let $Y_2$ be the number of students with the disability.
### (b) 
> Find $Pr(Y_2=y_2|Y_1 =2), the posterior predictive distribution of $Y_2$, as follows:
> 1. Discuss what assumptions are needed about the joint distribution of $(Y_1, Y_2)$ such that the fololowing is true:
> $$Pr(Y_2=y_2 |Y_1=2) = \int^{1}_{0} Pr(Y_2=y_2|\theta)p(\theta|Y_1=2)d\theta$$
> 
> 2. Now plug in the forms of $Pr(Y_2=y_2|\theta)$ and $p(\theta|Y_1 =2)$ in the above integral.
>
> 3. Figure out what the above integral must be by using the calculus result discussed in Section 3.1.

### (c) {#sec-3-7-c}
> Plot the function $Pr(Y_2 = y_2 | Y_1 =2)$ as a function of $y_2$. Obtain the mean and standard deviation of $Y_2$, given $Y_1 = 2$.


### (d)
> The posterior mode and the MLE (maximum likelihood estimate) of $\theta$, based on data from the pilot study, are both $\hat{\theta} = \frac{2}{15}$. Plot the distribution $Pr(Y_2 = y_2|\theta=\hat{\theta})$, and find the mean and standard deviation of $Y_2$ given $\theta=\hat{\theta}$. Compare these results to the plots and calculation in [(c)](#sec-3-7-c) and discuss any differences. Which distribution for $Y_2$ would you used to make predictions, and why?

``` {python}
#| label: fig-polar
#| fig-cap: "A line plot on a polar axis"


r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
fig, ax = plt.subplots(
  subplot_kw = {'projection': 'polar'} 
)
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```


## Problem 3.12

> Jeffrey's prior: Jeffreys (1961) suggested a default rule for gnerating a prior distribution of a parameter $\theta$ in a sampling model $p(y|\theta)$. Jeffreys' prior is given by $p_{J}\propto \sqrt{I(\theta)}$, where $I(\theta) = - E[\frac{\partial^{2} \log p(Y|\theta)}{\partial\theta^2} | \theta]$ is the *Fisher information*.

### (a) {#sec-3-12-a}
> Let $Y\sim binomial(n,\theta)$. Obtain Jeffreys' prior  distribution $p_J(\theta)$ for this model.


### (b) {#sec-3-12-b}

> Reparameterize the binomial sampling model with $\psi = \log \theta / (1-\theta)$, so that $p(y|\psi) = {n\choose y} e^{\psi y} (1+e^{\psi})^{-n}$. Obtain Jefferys' prior distribution $p_J (\psi)$ for this model.


### (c) 

> Take the prior distribution from [(a)](#sec-3-12-a) and apply the change of variables formula from Exercise 3.10 to obtain the induced prior density on $psi$. 
> 
> This density should be the same as the one derived in part [(b)](#sec-3-12-b) of this exercise. This consistency under reparameterization is the defining characteristic of Jeffrey's' prior.

::: {.content-hidden when-format="html"}

## References

:::