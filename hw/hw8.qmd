---
title: "Homework 8"
author: 
  - name: Shao-Ting Chiu (UIN:433002162)
    url: stchiu@email.tamu.edu
    affiliation: Department of Electrical and Computer Engineering, Texas A\&M University
date: today
bibliography: ../ref.bib
format:
  html:
    table-of-contents: true
    keep-ipynb: true
  pdf:
    table-of-contents: true
jupyter: julia-1.8
execute: 
    echo: true
    freeze: auto
---

## Description

- Course: STAT638, 2022 Fall

> Read Chapter 8 in the Hoff book. Then do the following exercises in Hoff: 8.1 and 8.3.
> 
> Please note some typos in 8.1: All $\theta_i$'s should be $\theta_j$'s.
> 
> For 8.1(c), you may find [the law of total (co-)variance](https://en.wikipedia.org/wiki/Law_of_total_covariance) useful. In addition, remember that all of these laws also hold for conditional distributions (e.g., when conditioning on additional quantities such as $\mu$ and $\tau^2$ in all terms on the left- and right-hand side of the equation).

## Computational Enviromnent Setup

### Third-party libraries
``` {julia}
using Pkg
Pkg.activate("hw8")
using Distributions
using DataFrames
using Turing
using Plots
using DelimitedFiles
using LinearAlgebra
using Statistics
using Turing
```

### Version
``` {julia}
Pkg.status()
VERSION
```

---

### Problem 8.1

> Components of variance: Consider the hierarchical model where
> 
> $$\theta_1, \dots, \theta_m | \mu, \tau^2 \sim i.i.d. \text{normal}(\mu, \tau^2)$$
> 
> $$y_{1,j}, \dots, y_{n_j, j} |\theta_j, \sigma^2 \sim i.i.d. \text{normal}(\theta_j, \sigma^2)$$
> For this problem, we will eventually compute the following:
>
> - $Var[y_{i,j}|\theta_i, \sigma^2]$, $Var[\bar{y}_{\cdot,j}|\theta_i, \sigma^2]$, $Cov[y_{i_1,j}, y_{i_2, j}|\theta_j, \sigma^2]$
> - $Var[y_{i,j}|\mu, \tau^2]$, $Var[\bar{y}_{\cdot,j}|\mu, \tau^2]$, $Cov[y_{i_1,j}, y_{i_2, j}|\mu, \tau^2]$
> First, lets use our intuition to guess at the answers:


### (a)

> Which do you think is bigger, $Var[y_{i,j}|\theta_i, \sigma^2]$ or $Var[y_{i,j}|\mu, \tau^2]$? To guide your intuition, you can interpret the first as the variability of the $Y$'s when sampling from a fixed group, and the second as the variability in first sampling a group, then sampling a unit from within the group.

- $Var[y_{i,j} | \mu, \tau^2]$ because $\theta_j$ is uncertain and the between-group varibability create additional uncertainty.

### (b)

> Do you think $Cov[y_{i_1,j}, y_{i_2, j}|\theta_j, \sigma^2]$ is negative, positive, or zero? Answer the same for $Cov[y_{i_1,j}, y_{i_2, j}|\mu, \tau^2]$. You may want to think about what $y_{i_2, j}$ tells you about $y_{i_1, j}$ if $\theta_j$ is known, and what it tells you when $\theta_j$ is unknown.


$Cov[y_{i_1,j}, y_{i_2, j}|\theta_j, \sigma^2]$

Because $y_{i_1, j}$ and $y_{i_2, j}$ is i.i.d. sampled, I expect $Cov[y_{i_1,j}, y_{i_2, j}|\theta_j, \sigma^2]$ to be zero.

$Cov[y_{i_1,j}, y_{i_2, j}|\mu, \tau^2]$

$y_{1,j}$ does tell information about $y_{2,j}$. The covariance $Cov[y_{i_1,j}, y_{i_2, j}|\mu, \tau^2]$ is likely to be positive because values from same $\theta_j$ tend to be close together.

### (c)

> Now compute each of the six quantities above and compare to your answers in (a) and (b). [^tot-var]

\begin{align}
    Var[y_{i,j}|\theta_i, \sigma^2]%
    &= \sigma^2
\end{align}


\begin{align}
    Var[\bar{y}_{\cdot,j}|\theta_i, \sigma^2]%
    &= Var[\sum_{i'=1}^{n_j}y_{i',j}/n |\theta_i, \sigma^2]\\ 
    &= \frac{1}{n^2}Var[\sum_{i'=1}^{n_j}y_{i',j} |\theta_i, \sigma^2]\\ 
    &= \frac{1}{n^2} \sum_{i'=1}^{n_j} Var[y_{i',j} |\theta_i, \sigma^2]\\ 
    &= \frac{1}{n} Var[y_{i',j} |\theta_i, \sigma^2]\\
    &= \frac{\sigma^2}{n} 
\end{align}


\begin{align}
    Cov[y_{i_1,j}, y_{i_2, j}|\theta_j, \sigma^2]% 
    &= E[y_{i_1, j} y_{i_2, j}] - E[y_{i_1, j}]E[y_{i_2, j}]\\ 
    &= E[y_{i_1, j}]E[y_{i_2, j}] - E[y_{i_1, j}]E[y_{i_2, j}]\\ 
    &= 0
\end{align}

\begin{align}
    Var[y_{i,j}|\mu, \tau^2]%
    &= E(Var[y_{i,j}|\mu, \tau^2, \theta, \sigma^2]|\mu, \tau^2) + Var(E[y_{i,j}|\mu, \tau^2, \theta, \sigma^2]|\mu, \tau^2)\\ 
    &= E(\sigma^2 | \mu, \tau^2) + Var(\theta | \mu, \tau^2)\\ 
    &= \sigma^2 + \tau^2
\end{align}


\begin{align}
    Var[\bar{y}_{\cdot,j}|\mu, \tau^2]%
    &= E(Var[\bar{y}_{\cdot,j}|\mu, \tau^2, \theta, \sigma^2]|\mu, \tau^2) + Var(E[\bar{y}_{\cdot,j}|\mu, \tau^2, \theta, \sigma^2]|\mu, \tau^2)\\ 
    &= E(\frac{\sigma^2}{n}|\mu,\tau^2) + Var(\theta | \mu, \tau^2)\\ 
    &= \frac{\sigma^2}{n} + \tau^2
\end{align}

\begin{align}
    Cov[y_{i_1, j}, y_{i_2, j}|\mu, \tau^2]%
    &= E(Cov[y_{i_1, j}, y_{i_2, j} | \theta, \sigma^2, \mu, \tau^2]| \mu, \tau^2) \\ 
    &+ Cov(E[y_{i_1, j} | \theta, \sigma^2, \mu, \tau^2], E[y_{i_2, j} | \theta, \sigma^2, \mu, \tau^2] | \mu, \tau^2)\\ 
    &= 0 + Cov(\theta, \theta | \mu, \tau^2)\\ 
    &= E[\theta^2|\mu, \tau^2] - E[\theta|\mu, \tau^2]^2\\
    &= Var(\theta |\mu, \tau^2)\\ 
    &= \tau^2
\end{align}

[^tot-var]: $Var(Y) =  E[Var(Y|X)] + Var(E[Y|X])$

### (d)

> Now assume we have a prior $p(\mu)$ for $\mu$. Using Bayes' rule, show that 
> $$p(\mu|\theta_1, \dots, \theta_m, \sigma^2, \tau^2, y_1, \dots, y_m) = p(\mu|\theta_1, \dots, \theta_m, \tau^2)$$
> Interpret in words what this means.

\begin{align}
p(\mu|\theta_1, \dots, \theta_m, \sigma^2, \tau^2, y_1, \dots, y_m)%
&= \frac{p(\sigma^2, y_1, \dots, y_m | \mu, \theta_1, \dots, \theta_m, \tau^2) p(\mu |\theta_1, \dots, \theta_m, \tau^2)}{ p(\sigma^2, y_1, \dots, y_m | \theta_1, \dots, \theta_m, \tau^2) }\\ 
&= p(\mu|\theta_1, \dots, \theta_m, \tau^2)
\end{align}

where $p(\sigma^2, y_1, \dots, y_m | \mu, \theta_1, \dots, \theta_m, \tau^2) =  p(\sigma^2, y_1, \dots, y_m |  \theta_1, \dots, \theta_m, \tau^2)$ because knowing $\mu$ doesn't provide more information when $\theta_1, \dots, \theta_m$ are known.

## Problem 8.3

> Herarchical modeling: The files `school1.dat` through `school8.dat` give weekly hours spent on homework for students sampled from eight different schools. Obtain posterior distributions for the true means for the eight different schools using a herarchical normal model with the following prior parameters:
> $$\mu_0 = 7, \gamma^{2}_{0} = 5, \tau^{2}_{0}=10, \eta_0 = 2, \sigma^{2}_{0} = 15, \nu_0 = 2$$


``` {julia}

```

### (a)

> Run a Gibbs sampling algorithm to approximate the posterior distribution of $\{\theta, \sigma^2, \mu, tau^2\}$. Assess the convergence of the Markov chain, and find the effective sample size for $\{\sigma^2, \mu, \tau^2\}$. Run the chain long enough so that the effective sample sizes are all above $1000$.




### (b)

> Compute posterior means and $95\%$ confidence regions for $\{\sigma^2, \mu, \tau^2\}$. Also, compare the posterior densities to the prior densities, and discuss what was learned from the data.


### (c)
> Plot the posterior density of $R=\frac{\tau^2}{\sigma^2 + \tau^2}$ and compare it to a plot of the prior density of $R$. Describe the evidence for between-school variation.

### (d)

> Obtain the posterior probability that $\theta_7$ is smaller than $\theta_6$, as well as the posterior probability that $\theta_7$ is smaller than of all the $\theta$'s.


### (e)

> Plot the sample averages $\bar{y}_1, \dots, \bar{y}_8$ against the posterior expectations of $\theta_1, \dots, \theta_8$, and describe the relationship. Also compute the sample mean of all observations and compare it to the posterior mean of $\mu$.