[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT638: Applied Bayesian Methods",
    "section": "",
    "text": "Preface\nThis is the lecture notes for STAT638 Applied Bayesian Methods by Dr. Matthias Katzfuss."
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "STAT638: Applied Bayesian Methods",
    "section": "Course details",
    "text": "Course details\n\nCourse Number: STAT 638\nCourse Title: Introduction to Applied Bayesian Methods\nTime: TuTh 2:20 - 3:35 (Central time)\nLocation: Blocker 411\nTextbook: Hoff, (Links to an external site.)A First Course in Bayesian Statistical Methods (Links to an external site.) (electronic version available through TAMU library)\nPrerequisite: STAT 630 or STAT 650. (Also, familiarity with R or other statistical software, training in vector/matrix algebra, and some exposure to linear regression will be very helpful.)"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "STAT638: Applied Bayesian Methods",
    "section": "Schedule",
    "text": "Schedule\n\nIntroduction (Week 1)\nConditional distributions and Bayes rule (Weeks 1-2)\nOne-parameter models (Weeks 3-4)\nMonte Carlo approximation (Weeks 5-6)\nThe normal model (Weeks 6-8)\nGibbs sampling (Weeks 8-9)\nThe multivariate normal model (Weeks 9-11)\nGroup comparisons and hierarchical modeling (Weeks 11-12)\nLinear regression (Weeks 12-13)\nMarkov chain Monte Carlo (Weeks 13-14)\nMixed effects models (Week 14)"
  },
  {
    "objectID": "index.html#programming",
    "href": "index.html#programming",
    "title": "STAT638: Applied Bayesian Methods",
    "section": "Programming",
    "text": "Programming\n\nMajorly use R\nHomework will be R\nUsing Python is acceptable"
  },
  {
    "objectID": "index.html#exam",
    "href": "index.html#exam",
    "title": "STAT638: Applied Bayesian Methods",
    "section": "Exam",
    "text": "Exam\n\nExam 1: October 18th\nExam 2: December 1st"
  },
  {
    "objectID": "index.html#homework",
    "href": "index.html#homework",
    "title": "STAT638: Applied Bayesian Methods",
    "section": "Homework",
    "text": "Homework\n\nNo late homework is acceptable"
  },
  {
    "objectID": "index.html#solution-manual",
    "href": "index.html#solution-manual",
    "title": "STAT638: Applied Bayesian Methods",
    "section": "Solution manual",
    "text": "Solution manual\n\nhttps://github.com/jayelm/hoff-bayesian-statistics"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "STAT638: Applied Bayesian Methods",
    "section": "Resources",
    "text": "Resources\n\nNotes and solutions of Hoff book. [Github]"
  },
  {
    "objectID": "ch1.html#what-is-beseyian-methods",
    "href": "ch1.html#what-is-beseyian-methods",
    "title": "1  Chapter 1 Introduction and examples",
    "section": "1.1 What is Beseyian methods?",
    "text": "1.1 What is Beseyian methods?\n\nBayes’s rule provides a rational method for updating beliefs in light of new information."
  },
  {
    "objectID": "ch1.html#what-can-bayesian-methods-provide",
    "href": "ch1.html#what-can-bayesian-methods-provide",
    "title": "1  Chapter 1 Introduction and examples",
    "section": "1.2 What can Bayesian methods provide?",
    "text": "1.2 What can Bayesian methods provide?\n\nParameter estimates with good statistical properties\nParsimonious descriptions of observed data"
  },
  {
    "objectID": "ch1.html#contrast-between-frequentist-and-bayesian-statistics",
    "href": "ch1.html#contrast-between-frequentist-and-bayesian-statistics",
    "title": "1  Chapter 1 Introduction and examples",
    "section": "1.3 Contrast between Frequentist and Bayesian Statistics",
    "text": "1.3 Contrast between Frequentist and Bayesian Statistics\n\nFrequentist statistics\n\nUncertainty about the parameter estimates\n\nBayesian statistics\n\nUnvertainty is quantified by the oberservation of data."
  },
  {
    "objectID": "ch1.html#bayesian-learning",
    "href": "ch1.html#bayesian-learning",
    "title": "1  Chapter 1 Introduction and examples",
    "section": "1.4 Bayesian learning",
    "text": "1.4 Bayesian learning\n\nParameter — \\(\\theta\\)\n\nnumerical values of population characteristics\n\nDataset — \\(y\\)\n\nAfter a dataset \\(y\\) is obtained, the information it contains can be used to decrease our uncertainty about the population characteristics.\n\nBayesian inference\n\nQuantifying this change in uncertainty is the purpose of Bayesian inference\n\nSample space — \\(\\mathcal{Y}\\)\n\nThe set of all possible datasets.\nSingle dataset \\(y\\)\n\nParameter space — \\(\\Theta\\)\n\npossbile parameter values\nwe hope to identify the value that best represents the true population characteristics.\n\nBayesian learning begins with joint beliefs about \\(y\\) and \\(\\theta\\), in terms of distribution over \\(\\mathcal{Y}\\) and \\(\\Theta\\)\n\nPrior distribution — \\(p(\\theta)\\)\n\nOur belief that \\(\\theta\\) represents that true population characteristics.\n\nSampling model — \\(p(\\mathcal{y}|\\theta)\\)\n\ndescribes our belief that \\(\\mathcal{y}\\) would be the outcome of our study if we knew \\(\\theta\\) to be true.\n\nPosterior distribution — \\(p(\\theta|\\mathcal{y})\\)\n\nOur belief that \\(\\theta\\) is the true value, having observed dataset \\(\\mathcal{y}\\)\n\nBayesian Update (Equation 1.1) \\[p(\\theta|y) = \\frac{\\overbrace{p(y|\\theta)}^{\\text{Sampling model}}\\overbrace{p(\\theta)}^{\\text{Prior distribution}}}{\\int_{\\Theta}p(y|\\tilde{\\theta})p(\\tilde{\\theta})d\\tilde{\\theta}} \\tag{1.1}\\]\n\nBayes’s rule tells us how to change our belief after seeing new information."
  },
  {
    "objectID": "ch1.html#example",
    "href": "ch1.html#example",
    "title": "1  Chapter 1 Introduction and examples",
    "section": "1.5 Example",
    "text": "1.5 Example\n\n\n\n\n\n\nBeta distribution1\n\n\n\n\nNotation: \\(Beta(\\alpha, \\beta)\\)\nParameters:\n\n\\(\\alpha > 0\\)\n\\(\\beta > 0\\)\n\nSupport:\n\n\\(x\\in [0,1]\\)\n\nPDF \\[p(x) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\] where\n\n\\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\)\n\\(\\Gamma(\\alpha)\\) is a gamma function\n\n\\(\\Gamma(\\alpha) = (\\alpha-1)!\\), \\(\\alpha\\) is a positive interger2\n\n\nMean: \\(E[X] = \\frac{\\alpha}{\\alpha + \\beta}\\)\nVariancd: \\(var[X] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)\nBayesian inference\n\nThe use of Beta distribution in Bayesian inference provide a family of conjugate prior probability disbritions for binomial and geometric dictritutions.\n\n\n\n\n\n\n\n\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical Methods. Vol. 580. Springer."
  },
  {
    "objectID": "ch2.html#axioms-of-probability",
    "href": "ch2.html#axioms-of-probability",
    "title": "2  Chapter 2: Conditional distributions and Bayes rule",
    "section": "2.1 Axioms of probability",
    "text": "2.1 Axioms of probability\nLet \\(F\\), \\(G\\) and \\(H\\) be three possibly overlapping statements.\n\n0 = Pr(not H|H) \\(\\leq\\) Pr(F|H) \\(\\leq\\) Pr(H|H) = 1\nPr(F\\(\\cup\\)G|H) = Pr(F|H) + Pr(G|H) if \\(F\\cap G=\\emptyset\\)\n\\(Pr(F\\cap G|H)=Pr(G|H)Pr(F|G\\cap H)\\)"
  },
  {
    "objectID": "ch2.html#events-and-partition",
    "href": "ch2.html#events-and-partition",
    "title": "2  Chapter 2: Conditional distributions and Bayes rule",
    "section": "2.2 Events and partition",
    "text": "2.2 Events and partition\n\nSample space \\(S\\)\nPartition\n\na collection of sets \\(A_1,\\dots, A_m\\)\n\\(A_{i} \\cap A_j = \\emptyset\\)\n\nConditional probability\n\nLet \\(B\\) be an event, and \\(A_i,\\dots,A_m\\) be a partition of \\(S\\)\n\\(P(B|A_i) = \\frac{P(B\\cap A_i)}{P(A_i)}\\)\n\nBayes rule\n\n\\(P(A_j |B) = \\frac{P(B|A_j)P(A_j)}{P(B)} = \\frac{P(B|A_j)P(A_j)}{\\sum^{m}_{i=1}P(B|A_i)P(Ai)}\\)\n\n\n\n\n\n\n\n\nExample: COVID test\n\n\n\n\nA college with a Covid-19 prevalencde of 10% is using a test that is positive with probability 90% if an individual is infected, and positive with probability of 5% if the individual is not.\n\n\n\nFor a randomly selected student at the college, what is the probability that the test will be positive?\n\n\n\\[\\begin{align}\nP(+) &= P(+|C)P(C) + P(+|H)P(H)\\\\\n     &= 0.9*0.1 + 0.05*0.9\\\\\n     &= 0.135\n\\end{align}\\]\n\n\nGive that a student has tested positive, what is the probability the student is actually infected?\n\n\n\\[\\begin{align}\nP(C|+) &= \\frac{P(+|C)P(C)}{P(+)}\\\\\n       &= \\frac{0.9*0.1}{0.135}\\\\\n       &= 0.67\n\\end{align}\\]"
  },
  {
    "objectID": "ch2.html#random-variables-and-univariate-distributions",
    "href": "ch2.html#random-variables-and-univariate-distributions",
    "title": "2  Chapter 2: Conditional distributions and Bayes rule",
    "section": "2.3 Random variables and univariate distributions",
    "text": "2.3 Random variables and univariate distributions\n\nRandom variable is an unknown quantity characterized by a probability distribution\n\n\nRV\nDiscrete\nContinuous\n\n\n\n\nOutcome \\(y\\)\ncountable\nuncountable\n\n\nprop. of pdf\n\\(0\\leq p(y) \\leq1\\)\n\\(0\\leq p(y)\\)\n\n\n\n\\(\\sum_{y\\in Y}p(y)=1\\)\n\\(\\int_{Y}p(y)dy = 1\\)\n\n\ncdf \\(F(a)\\)\n\\(F(a) = \\sum_{y\\leq a}p(y)\\)\n\\(F(a)=\\int^{a}_{-\\infty}p(y)dy\\)\n\n\nmean\n\\(E(Y)=\\sum_{y\\in Y}p(y)\\)\n\\(E(Y)=\\int_{Y}y p(y)dy\\)\n\n\n\n\nCDF: \\(F(a) = P(Y\\leq a)\\)\nVariance: \\(Var(Y) = E(Y-E(Y))^2 = E(Y^2) - (E(Y))^2\\)\n\n\n\n\n\n\n\nBinomial distribution\n\n\n\n\\[p(Y=y|\\theta) = dbinom(y,n,\\theta) = {n\\choose y} \\theta^y (1-\\theta)^{n-y}\\]\n\n\n\n\n\n\n\n\nPoisson distribution\n\n\n\nLet \\(\\mathcal{Y} = \\{0,1,2,\\dots\\}\\). The uncertain quantity \\(Y\\in \\mathcal{Y}\\) has a poisson distribution with mean \\(\\theta\\) if\n\\[p(Y=y|\\theta) = dpois(y,\\theta) = \\theta^{y}\\frac{e^{-\\theta}}{y!}\\]"
  },
  {
    "objectID": "ch2.html#description-of-distributions",
    "href": "ch2.html#description-of-distributions",
    "title": "2  Chapter 2: Conditional distributions and Bayes rule",
    "section": "2.4 Description of distributions",
    "text": "2.4 Description of distributions\n\nExpection\n\n\\(E[Y] = \\sum_{y\\in\\mathcal{Y}yp(y)}\\) if \\(Y\\) is discrete.\n\\(E[Y] = \\int_{y\\in\\mathcal{Y}yp(y)}\\) if \\(Y\\) is discrete.\n\nMode\n\nThe most probable value of \\(Y\\)\n\nMedian\n\nThe value of \\(Y\\) in the middle of the distribution\n\nVariance \\[\\begin{align}\n     Var[Y] &= E[(Y-E[Y])^2]\\\\\n            &= E[Y^2-2YE[Y] + E[Y]^2]\\\\\n            &= E[Y^2] - 2E[Y]^2 + E[Y]^2\\\\\n            &= E[Y^2] - E[Y]^2\n\\end{align}\\]"
  },
  {
    "objectID": "ch2.html#joint-distribution",
    "href": "ch2.html#joint-distribution",
    "title": "2  Chapter 2: Conditional distributions and Bayes rule",
    "section": "2.5 Joint distribution",
    "text": "2.5 Joint distribution\n\nMarginal\n\n\n\n\n\n\nDiscrete\nContinuous\n\n\n\n\n\\(P_{y1}(y_1)=\\sum_{y_2\\in Y_2}p_{Y_1, Y_2}(y_1, y_2)\\)\n\\(p_{Y_1}(y_1) = \\int_{y_2}p_{Y_1,Y_2}(y_1,y_2)dy_2\\)\n\n\n\n\nConditional: \\(p_{Y_2|Y_1}(y_2|y_1) = \\frac{p_{Y_1,Y_2}(y_1,y_2)}{p_{Y_1}(y_1)}\\)"
  },
  {
    "objectID": "ch2.html#proportionality",
    "href": "ch2.html#proportionality",
    "title": "2  Chapter 2: Conditional distributions and Bayes rule",
    "section": "2.6 Proportionality",
    "text": "2.6 Proportionality\n\nA function \\(f(x)\\) is proportional to \\(g(x)\\), denoted by \\(f(x) \\propto g(x)\\)\n\n\\[f(x) = cg(x)\\]"
  },
  {
    "objectID": "ch2.html#a-bayesian-model",
    "href": "ch2.html#a-bayesian-model",
    "title": "2  Chapter 2: Conditional distributions and Bayes rule",
    "section": "2.7 A Bayesian model",
    "text": "2.7 A Bayesian model\n\nRandom vector of data — \\(Y\\)\nProbability distribution of \\(Y\\) — \\(p(y|\\theta)\\)\n\n\n\n\n\n\n\nBayes theorem applied to statistical model\n\n\n\n\\[p(\\theta|y) = \\frac{p(y,\\theta)}{m(y)} = \\frac{p(y|\\theta)p(\\theta)}{\\int_{\\Theta}p(y|\\theta)p(\\theta)d\\theta}\\]\n\n\\(p(\\theta)\\): the prior distribution\n\\(\\Theta\\): parameter space\n\\(p(y|\\theta)\\): the likelihood function.\n\\(p(\\theta|y)\\): the posterior distribution\n\\(m\\): marginal distribution of \\(Y\\)\n\nThe posterior distribution expresses the experimenter’s updated beliefs about \\(\\theta\\) in light of the observed data \\(y\\).\n\n\n\n\\(m(y)\\): doesn’t depend on \\(\\theta\\)."
  },
  {
    "objectID": "ch2.html#conditional-independence-and-exhangeability",
    "href": "ch2.html#conditional-independence-and-exhangeability",
    "title": "2  Chapter 2: Conditional distributions and Bayes rule",
    "section": "2.8 Conditional independence and Exhangeability",
    "text": "2.8 Conditional independence and Exhangeability\n\n\n\n\n\n\nConditional independence\n\n\n\n\\[P(A\\cap B|C) = P(A|C)P(B|C)\\]\n\nTwo events \\(A\\) and \\(B\\) are conditionally independent given event \\(C\\)\n\n\\(A\\perp B|C\\)\n\n\n\nKnowing \\(C\\) and \\(B\\) gives no more information about \\(A\\) thatn does \\(C\\) by itself.\n\n\n\\[P(A|C\\perp B) = P(A|C)\\]\n\n\n\n\nConditional independence\n\nLet \\(Y_1,\\dots,Y_n\\) are conditionally indep. given \\(\\theta\\). for every collection \\(A_1,\\dots,A_n\\) of sets:\n\\[P(Y_1\\in A_1,\\dots, Y_n\\in A_n |\\theta) = \\amalg^{n}_{i=1} P(Y_i\\in A_i |\\theta)\\]\n\n\n\n2.8.1 Exchangeability\n\\[p(y_1,\\dots,y_n) = p(y_{\\pi_1},\\dots,y_{\\pi_n})\\]\nfor all permutations \\(\\pi\\) of \\(\\{1,\\dots,n\\}\\)\n\nIf we think of \\(Y_1,\\dots,Y_n\\) as data, exchangeability says that the ordering of the data conveys no extra information than that in the observations themselves.\n\n\nFor example: time-series of weather is not exhangeable\ni.i.d. data is exchangeable.\nexchangeable does not imply unconditional independence"
  },
  {
    "objectID": "ch3.html#key-messages",
    "href": "ch3.html#key-messages",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.1 Key messages",
    "text": "3.1 Key messages\n\nOne-parameter models\n\nBinomial model\nPoisson model\n\nBayesian data analysis\n\nConjugate prior distribution\nPredictive distribution\nConfidence regions"
  },
  {
    "objectID": "ch3.html#the-binomial-model",
    "href": "ch3.html#the-binomial-model",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.2 The binomial model",
    "text": "3.2 The binomial model\n\\[p(\\theta|y) \\propto p(y|\\theta)\\]\n\n\n\n\n\n\nCalculus\n\n\n\n\\[\\int^{1}_{0}\\theta^{a-1}(1-\\theta)^{b-1}d\\theta = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\]\nwhere \\(\\Gamma(n) = (n-1)!\\)."
  },
  {
    "objectID": "ch3.html#the-beta-distribution",
    "href": "ch3.html#the-beta-distribution",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.3 The beta distribution",
    "text": "3.3 The beta distribution\n\\[p(\\theta) = dbeta(\\theta, a, b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}(1-\\theta)^{b-1}\\quad \\text{for}~0\\leq \\theta\\leq 1\\]\n\n\\(E[\\theta]=\\frac{a}{a+b}\\)\n\\(Var[\\theta] = \\frac{ab}{(a+b+1)(a+b)^2} = \\frac{E[\\theta]E[1-\\theta]}{a+b+}\\)"
  },
  {
    "objectID": "ch3.html#inference-for-exchangeable-binary-data",
    "href": "ch3.html#inference-for-exchangeable-binary-data",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.4 Inference for exchangeable binary data",
    "text": "3.4 Inference for exchangeable binary data\nIf \\(Y_{1},\\dots,Y_n|\\theta\\) are i.i.d. binary (\\(\\theta\\)):\n\\[p(\\theta|y_1,\\dots,y_n) = \\frac{\\theta^{\\sum y_i}(1-\\theta)^{n-\\sum y_{i}} \\times p(\\theta)}{p(y_1,\\dots,y_n)} \\tag{3.1}\\]"
  },
  {
    "objectID": "ch3.html#sufficient-statistics",
    "href": "ch3.html#sufficient-statistics",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.5 Sufficient statistics",
    "text": "3.5 Sufficient statistics\nIf compare the relative probabilities of any two \\(\\theta\\)-values, \\(\\theta_a\\) and \\(\\theta_b\\) (from Equation 3.1):\n\\[\\frac{p(\\theta_a|y_1,\\dots,y_n)}{p(\\theta_b|y_1,\\dots,y_n)} = (\\frac{\\theta_{a}}{\\theta_b})^{\\sum y_i}(\\frac{1-\\theta_{a}}{1-\\theta_b})^{n - \\sum y_i}\\frac{p(\\theta_a)}{p(\\theta_b)} \\tag{3.2}\\]\nEquation 3.2 shows that\n\\[p(\\theta\\in A|Y_1=y_1,\\dots,Y_n = y_n) = p(\\theta \\in A|\\sum^{n}_{i=1} Y_i=\\sum^{n}_{i=1}y_i)\\]\n\\(\\sum^{n}_{i=1} Y_i\\) is a sufficient statistic for \\(\\theta\\) and \\(p(y_1,\\dots,y_n|\\theta)\\). It is sufficient to know \\(\\sum Y_i\\) to make inference about \\(\\theta\\).\nIn this case where \\(Y_1, \\dots, Y_n|\\theta\\) are i.i.d. binary (\\(\\theta\\)) random variables, the sufficient statistics \\(Y=\\sum^{n}_{i=1} Y_i\\) has a binomial distribution with parameters \\((n,\\theta)\\)."
  },
  {
    "objectID": "ch3.html#conjugacy",
    "href": "ch3.html#conjugacy",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.6 Conjugacy",
    "text": "3.6 Conjugacy\n\nBeta prior and binomial sampling leads to beta posterior\n\nbeta prior is conjugate for the binomial sampling.\n\n\n\n\n\n\n\n\nDefinition: Conjugate\n\n\n\nA class \\(\\mathcal{P}\\) of prior distribution for \\(\\theta\\) is called conjugate for a sampling model \\(p(y|\\theta)\\) if\n\\[p(\\theta) \\in \\mathcal{P} \\Rightarrow p(\\theta|y) \\in \\mathcal{P}\\]\n\n\n\nConjugate priors make posterior calculations easy."
  },
  {
    "objectID": "ch3.html#combining-information",
    "href": "ch3.html#combining-information",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.7 Combining information",
    "text": "3.7 Combining information\nIf \\(\\theta|Y=y \\sim beta(a+y, b+n-y)\\), then\n\\[\\begin{aligned}\nE[\\theta|y] &=\\frac{a+y}{a+b+n}\\\\\n            &= \\frac{a+b}{a+b+n}\\underbrace{\\frac{a}{a+b}}_{\\text{prior expectation}} + \\frac{n}{a+b+n}\\underbrace{\\frac{y}{n}}_{\\text{data average}}\\\\\n\\end{aligned} \\tag{3.3}\\]\nFrom Equation 3.3, the posterior expectation is a weighted average of the prior expectation and the sample average. This leads to interpretation of \\(a\\) and \\(b\\) as “prior data”:\n\n\\(a\\): prior number of 1’s\n\\(b\\): prior number of 0’s\n\\(a+b\\): prior sample size"
  },
  {
    "objectID": "ch3.html#predictive-distribution",
    "href": "ch3.html#predictive-distribution",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.8 Predictive distribution",
    "text": "3.8 Predictive distribution\nThe predictive distribution of \\(\\tilde{Y}\\) is the conditional distribution of \\(\\tilde{Y}\\) given \\(\\{Y_1=y_1,\\dots,Y_n=y_n\\}\\)\n\\[Pr(\\tilde{Y}=1|y_1,\\dots,y_n) = E[\\theta|y_1,\\dots,y_n] = \\frac{a+\\sum^{n}_{i=1}y_i}{a+b+n}\\]\n\nPredictive distribution does not depend on any unknown quantities.\nPredictive distribution depends on our observed data."
  },
  {
    "objectID": "ch3.html#confidence-regions",
    "href": "ch3.html#confidence-regions",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.9 Confidence regions",
    "text": "3.9 Confidence regions\n\n\n\n\n\n\nBayesian coverage\n\n\n\nAn interval \\([l(y), u(y)]\\), based on the observed data \\(Y=y\\), has \\(95\\%\\) Bayesian coverage for \\(\\theta\\) if \\[Pr(l(y)<\\theta<u(y)|Y=y) = .95 \\tag{3.4}\\]\n\nEquation 3.4 describes the information about the true value of \\(\\theta\\) after observing \\(Y=y\\).\npost-experimental coverage\n\n\n\n\n\n\n\n\n\nFrequentist coverage\n\n\n\nA random interval \\([l(Y), u(Y)]\\) has \\(95\\%\\) frequentist coverage for \\(\\theta\\) if, before the data are gathered,\n\\[Pr(l(Y) < \\theta < u(Y)|\\theta) = .95 \\tag{3.5}\\]\n\nEquation 3.5 describes the probability that the interval will cover the true value before the data are observed\npre-experimental coverage"
  },
  {
    "objectID": "ch3.html#binomial-distribution",
    "href": "ch3.html#binomial-distribution",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.10 Binomial distribution",
    "text": "3.10 Binomial distribution\n\\[p(Y=y|\\theta) = dbinom(y,n,\\theta) = {n\\choose y}\\theta^{y}(1-\\theta)^{n-y},\\quad y\\in\\{0,1,\\dots, n\\}\\]"
  },
  {
    "objectID": "ch3.html#the-poisson-model",
    "href": "ch3.html#the-poisson-model",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.11 The Poisson model",
    "text": "3.11 The Poisson model\n\n\n\n\n\n\nPoisson distribution\n\n\n\n\\[Pr(Y=y|\\theta) = \\theta^{y}\\frac{e^{-\\theta}}{y!}\\quad \\text{for} y\\in \\{0,1,2,\\dots\\}\\]\n\n\\(E[Y|\\theta] = \\theta\\)\n\\(Var[Y|\\theta] = \\theta\\)\n\n\n\n\n3.11.1 Posterior inference\nLet \\(Y_1,\\dots,Y_n\\) as i.i.d. Poisson with mean \\(\\theta\\), then the joint pdf is\n\\[\\begin{aligned}\nPr(Y_1 =y_1,\\dots, Y_n = y_n |\\theta) &= \\prod^{n}_{i=1} p(y_i|\\theta)\\\\\n                                     &= \\prod^{n}_{i=1} \\frac{1}{y_{i}!} \\theta^{y_i}e^{-\\theta}\\\\\n                                     &= c(y_1, \\dots, y_n)\\theta^{\\sum y_i}e^{-n\\theta}\n\\end{aligned}\\]\n{#eq-pois-son}"
  },
  {
    "objectID": "ch3.html#some-one-parameter-models",
    "href": "ch3.html#some-one-parameter-models",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.12 Some one-parameter models",
    "text": "3.12 Some one-parameter models"
  },
  {
    "objectID": "ch3.html#bayesian-prediction",
    "href": "ch3.html#bayesian-prediction",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.13 Bayesian prediction",
    "text": "3.13 Bayesian prediction\n\n3.13.1 The marginal\n\\[\\begin{aligned}\n  p(y) &= \\int p(y,\\theta)d\\theta\\\\\n       &= \\int_{\\Theta}p(y|\\theta)p(\\theta)d\\theta\n\\end{aligned}\\]\n\n\n3.13.2 Posterior predictive distribution\nLet \\(\\bar{Y}\\) be a data point that is yet to be observed.\n\\[\\begin{aligned}\n  p(\\bar{y}|y) &= \\int_{\\Theta} p(\\bar{y}, \\theta|y)d\\theta\\\\\n               &= \\int_{\\Theta} p(\\bar{y}|\\theta,y)p(\\theta|y)d\\theta\n\\end{aligned}\\]\n\n\n3.13.3 Sufficient statistics\nComparing two values of \\(\\theta\\) a poseteriori,\n\\[\\frac{p(\\theta_a|y_1,\\dots,y_n)}{p(\\theta_b|y_1,\\dots,y_n)} = \\frac{e^{-n\\theta_a}}{-n\\theta_b}\\frac{\\theta_{a}^{\\sum y_i}}{\\theta_{b}^{\\sum y_i}}\\frac{p(\\theta_a)}{p(\\theta_b)}\\]\n\n\n3.13.4 Conjugate prior\n\\[p(\\theta|y_1,\\dots,y_n) \\propto p(\\theta) \\times \\underbrace{p(y_1,\\dots,y_n|\\theta)}_{\\theta^{\\sum y_i} e^{-n\\theta}}\\]\n\n\\(\\theta^{c_1}e^{-c_2 \\theta}\\): Gamma distribution\n\n\n\n\n\n\n\nGamma distribution\n\n\n\n\\[p(\\theta) = \\frac{b^a}{\\Gamma(a)}\\theta^{a-1}e^{-b\\theta} \\quad \\text{for } \\theta, a, b > 0\\]\n\n\\(E[\\theta] = \\frac{a}{b}\\)\n\\(Var[\\theta] = \\frac{a}{b^2}\\)\n\n\n\n\n\n\n\n\n\nGamma pdf integration\n\n\n\n\\[\\int^{\\infty}_{0} \\theta^{a-1}e^{-b\\theta}d\\theta = \\frac{\\Gamma(a)}{b^a}\\]"
  },
  {
    "objectID": "ch3.html#jeffreys-prior",
    "href": "ch3.html#jeffreys-prior",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.14 Jeffreys prior",
    "text": "3.14 Jeffreys prior\n\nNotes 12. The Jeffreys Prior"
  },
  {
    "objectID": "ch3.html#gamma-distribution-1",
    "href": "ch3.html#gamma-distribution-1",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.15 Gamma Distribution",
    "text": "3.15 Gamma Distribution\n\nConjuagate prior of Poisson data\n\n\\[p(\\theta) = \\frac{b^a}{\\Gamma(a)}\\theta^{a-1} e^{-b\\theta}I_{0,\\infty}(\\theta)\\]\n\nposterior of poisson data\n\n\\[E(\\theta|y) = \\frac{a+n\\bar{y}}{b+n} = \\frac{b}{b+n}\\frac{a}{b} + \\frac{n}{b+n}\\frac{n\\bar{y}}{n} = (1-\\omega_n)E(\\theta) + \\omega_n \\bar{y}\\]\n\na=1; b=1\ncurve(dgamma(x,a,b),0, 10)\n\n\n\n\n\na=4; b=4\ncurve(dgamma(x,a,b),0, 10)\n\n\n\n\n\na=16; b=4\ncurve(dgamma(x,a,b),0, 10)"
  },
  {
    "objectID": "ch3.html#exponential-families-and-conjugate-priors",
    "href": "ch3.html#exponential-families-and-conjugate-priors",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.16 Exponential Families and conjugate priors",
    "text": "3.16 Exponential Families and conjugate priors\n\n\\(p(y|\\phi) = h(y)c(\\phi)e^{\\phi t(y)}\\)\n\n\\(\\phi\\) is unknown parameter\n\\(t(y)\\) is the sufficient statistic\n\nGeneral exponential family models for particular prior\n\n\\(p(\\phi|n_0,t_0) = \\kappa(n_0,t_0)c(\\phi)^{n_0}e^{n_0 t_0 \\phi}\\)\nhave to posterior distribution\n\n\n\\[\\begin{align}  \n  p(\\phi|y_1,\\dots, y_n) &\\propto p(\\phi)p(y_1,\\dots,y_n|\\phi)\\\\\n                         &\\propto c(\\phi)^{n_0 + n} \\exp \\left(\\phi \\times \\left[ n_0 t_0 + \\sum_{i=1}^{n}t(y_i) \\right]\\right)\\\\\n                         &\\propto p(\\phi|n_0 +n, n_0t_0 +n \\bar{t}(y))\n\\end{align}\\]\nwhere \\(\\bar{t}(y)=\\frac{\\sum t(y_i)}{n}\\)"
  },
  {
    "objectID": "ch3.html#mixture-distribution",
    "href": "ch3.html#mixture-distribution",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.17 Mixture distribution",
    "text": "3.17 Mixture distribution\n\nhttp://www.mas.ncl.ac.uk/~nmf16/teaching/mas3301/week11.pdf"
  },
  {
    "objectID": "ch3.html#installation",
    "href": "ch3.html#installation",
    "title": "3  Chapter 3: One-parameter models",
    "section": "3.18 Installation",
    "text": "3.18 Installation\nR installation: https://www.drdataking.com/post/how-to-add-existing-r-to-jupyter-notebook/"
  },
  {
    "objectID": "ch4.html#mote-carlo-estimate",
    "href": "ch4.html#mote-carlo-estimate",
    "title": "4  Chapter 4: Monte Carlo Approximation",
    "section": "4.1 Mote Carlo Estimate",
    "text": "4.1 Mote Carlo Estimate\n\nEstimation of \\(Var[\\theta|y_1,\\dots,y_n]\\): \\(\\hat{\\sigma}^2 = \\sum \\frac{(\\theta^{(s)}\\bar{\\theta})^2}{S-1}\\)\nStandard error: \\(\\sqrt{\\frac{\\hat{\\sigma}^2}{S}}\\)\n\\(95\\%\\) Monte Carlo confidence interval: \\(\\hat{\\theta} \\pm 2 \\sqrt{\\frac{\\hat{\\sigma}^2}{S}}\\)\n\n\\[std = \\sqrt{\\frac{\\sigma^2}{n}}\\]"
  },
  {
    "objectID": "ch4.html#posterior-inference-for-arbitrary-functions",
    "href": "ch4.html#posterior-inference-for-arbitrary-functions",
    "title": "4  Chapter 4: Monte Carlo Approximation",
    "section": "4.2 Posterior inference for arbitrary functions",
    "text": "4.2 Posterior inference for arbitrary functions\n\\[\\log \\text{odds}(\\theta) = \\log \\frac{\\theta}{1-\\theta} = \\gamma\\]\n\n#####   STAT 638\n#### Matthias Katzfuss\n\n\n#########   R code for Chapter 4   ##########\n\n\n######   illustration of Monte Carlo for inverse rate in exponential\n\nK=100000\n\n## parameters of the posterior of the rate parameter\na=50\nb=37815\n\n## posterior draws of the rate parameter\ntheta=rgamma(K,a,rate=b)\n\n## transform to mean parameter\npsi=1/theta\n\n\n## plot posterior of psi\nhist(psi,freq=FALSE,main='posterior') # histogram of psi draws\nlines(density(psi)) # kernel-density estimate\ncurve(invgamma::dinvgamma(x,a,b),add=TRUE,col=2) # exact\n\n\n## posterior mean\nb/(a-1) # exact\nmean(psi) # monte carlo approximation\n\n\n## the larger K, the closer MC estimate \n##   *tends* to be to exact summary\nplot(cumsum(psi)/(1:K),xlim=c(1,1000),type='l',\n     xlab='K',ylab='MC mean based on K samples')\nabline(h=b/(a-1),col='red')\n\n\n## posterior standard deviation\nsqrt(b^2/(a-1)^2/(a-2)) # exact\nsd(psi) # monte carlo approximation\n\n\n## 95% credible interval (quantile-based)\nci.exact=invgamma::qinvgamma(c(.025,.975),a,b)\nci.mc=quantile(psi,c(.025,.975))\n\n# add intervals to posterior plot\nplot(density(psi)) # kernel-density estimate\ncurve(invgamma::dinvgamma(x,a,b),add=TRUE,col=2)\nabline(v=ci.exact,col=2)\nabline(v=ci.mc)\n\n\n## P( psi < 600 | data )\ninvgamma::pinvgamma(600,a,b)\nmean(psi<600)\n\n\n## MC error decreases slowly for large K\nplot(cumsum(psi)/(1:K),xlim=c(1,10000),type='l',\n     xlab='K',ylab='MC mean based on K samples')\nabline(h=b/(a-1),col='red')\n\n\n\n######   MC for predictive distribution in Poisson-gamma model\n\n## assumed parameters\na=0.5; b=0 # jeffreys prior\ny=c(5,4) # poisson observations\n\n## posterior of mean parameter theta\nn=length(y)\na2=a+sum(y)\nb2=b+n\ncurve(dgamma(x,a2,b2),0,2.5*mean(y))\n\n\n## exact posterior predictive distributions\ny.tildes=0:round(3*mean(y))\nplot(y.tildes,dnbinom(y.tildes,a2,mu=a2/b2),ylim=c(0,.2),type='h')\n\n## MC approximation of post. pred. distr.\nthetas=rgamma(K,a2,b2) # sample from posterior\npp.mc=numeric(length=length(y.tildes))\nfor(j in 1:length(pp.mc)) pp.mc[j]=mean(dpois(y.tildes[j],thetas))\nlines(y.tildes+1e-1,pp.mc,type='h',col=2)\n\n## predictive distribution, plugging in MLE\nlines(y.tildes+2e-1,dpois(y.tildes,mean(y)),type='h',col=3)\n\n\n\n\n#### posterior predictive distribution for exponential-gamma model\n\nK=1e6\n\n## parameters of the posterior of the rate parameter\na=50\nb=37815\n\n## posterior draws of the rate parameter\nthetas=rgamma(K,a,rate=b)\n\n## MC samples from post. pred. distr.\ny.tildes=rexp(K,thetas)\n\n## MC (kernel density) estimate of posterior pred\nplot(density(y.tildes,from=0),xlim=c(0,5*b/a))\n\n## exact posterior pred\ncurve(a*b^a/(x+b)^(a+1),add=TRUE,col=2)\n\n## MC estimate of PP mean\nmean.pp=mean(y.tildes)\nmean.pp # exact mean: 771.7\nabline(v=mean.pp)\n\n771.734693877551\n\n\n771.831659052567\n\n\n\n\n\n111.390308313294\n\n\n111.218024621904\n\n\n\n\n\n0.0401536510132767\n\n\n0.03917\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n770.948738931396"
  },
  {
    "objectID": "hw/hw1.html#homework-description",
    "href": "hw/hw1.html#homework-description",
    "title": "5  Homework 1",
    "section": "5.1 Homework Description",
    "text": "5.1 Homework Description\n\nRead Chapters 1 and 2 in the Hoff book. Then, do the following exercises in Hoff (p. 225-226): 2.1, 2.2, 2.3, 2.5 You must turn in your solutions as a pdf file here on Canvas. Use the Submit Assignment button on the top right. If your solutions are on paper, please scan them to pdf using a scanner or a scanner app on your phone. Please do not take a regular photo, as this can result in very large file sizes. Make sure that everything is legible. Please note that late homework will not be accepted and will result in a score of zero. To avoid late submissions due to technical issues, we recommend turning in your homework the night before the due date.\n\n\nDeadline: Sep. 8 by 12:01pm\nPDF version"
  },
  {
    "objectID": "hw/hw1.html#problem-2.1",
    "href": "hw/hw1.html#problem-2.1",
    "title": "5  Homework 1",
    "section": "5.2 Problem 2.1",
    "text": "5.2 Problem 2.1\nMarginal and conditional probability: The social mobility data from Section 2.5 gives a joint probability distribution on \\((Y_1, Y_2)=\\) (fathers’s occupation, son’s occupation)\n\n\nTable 5.1: The social mobility data (Hoff 2009, 580:24)\n\n\n\n\n\nson’s occupation\n\n\n\n\n\n\nfather’s occupation\nfarm\noperatives\ncraftsmen\nsales\nprofessional\n\n\nfarm\n0.018\n0.035\n0.031\n0.008\n0.018\n\n\noperatives\n0.002\n0.112\n0.064\n0.032\n0.069\n\n\ncraftsman\n0.001\n0.066\n0.094\n0.032\n0.084\n\n\nsales\n0.001\n0.018\n0.019\n0.010\n0.051\n\n\nprofessional\n0.001\n0.029\n0.032\n0.043\n0.130\n\n\n\n\n\n5.2.1 (a) The marginal probability distribution of a father’s occupation\nAccording to Table 5.1, let \\(\\mathbb{Y_1}\\) and \\(\\mathbb{Y_2}\\) be sets of father’s and son’s occupations:\n\\[\\mathbb{Y_1} = \\mathbb{Y_2} = \\{\\text{farm},\\text{operatives},\\text{craftsmen},\\text{sales},\\text{professional}\\}\\]\n\\[\n\\begin{aligned}\n    Pr(Y_1 = \\text{farm}) &= \\sum_{y_2\\in\\mathbb{Y_2}}Pr(Y_1= \\text{farm} \\cap Y_2=y_2)\\\\\n    &= Pr(Y_1= \\text{farm} \\cap Y_2=\\text{farm}) + Pr(Y_1= \\text{farm} \\cap Y_2=\\text{operatives}) \\\\\n    & +Pr(Y_1= \\text{farm} \\cap Y_2=\\text{craftsmen}) + Pr(Y_1= \\text{farm} \\cap Y_2=\\text{sales}) \\\\\n    & +Pr(Y_1= \\text{farm} \\cap Y_2=\\text{professional})\\\\\n    &= 0.018 + 0.035 + 0.031 + 0.008 + 0.018\\\\\n    &= 0.11\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    Pr(Y_1 = \\text{operatives}) &= \\sum_{y_2\\in\\mathbb{Y_2}}Pr(Y_1= \\text{operatives} \\cap Y_2=y_2)\\\\\n    &= Pr(Y_1= \\text{operatives} \\cap Y_2=\\text{farm}) + Pr(Y_1= \\text{operatives} \\cap Y_2=\\text{operatives}) \\\\\n    & +Pr(Y_1= \\text{operatives} \\cap Y_2=\\text{craftsmen}) + Pr(Y_1= \\text{operatives} \\cap Y_2=\\text{sales}) \\\\\n    & +Pr(Y_1= \\text{operatives} \\cap Y_2=\\text{professional})\\\\\n    &= 0.002 + 0.112 + 0.064 + 0.032 + 0.069\\\\\n    &= 0.279\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    Pr(Y_1 = \\text{craftsman}) &= \\sum_{y_2\\in\\mathbb{Y_2}}Pr(Y_1= \\text{craftsman} \\cap Y_2=y_2)\\\\\n    &= Pr(Y_1= \\text{craftsman} \\cap Y_2=\\text{farm}) + Pr(Y_1= \\text{craftsman} \\cap Y_2=\\text{operatives}) \\\\\n    & +Pr(Y_1= \\text{craftsman} \\cap Y_2=\\text{craftsmen}) + Pr(Y_1= \\text{craftsman} \\cap Y_2=\\text{sales}) \\\\\n    & +Pr(Y_1= \\text{craftsman} \\cap Y_2=\\text{professional})\\\\\n    &= 0.001+0.066+0.094+0.032+0.084\\\\\n    &= 0.277\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    Pr(Y_1 = \\text{sales}) &= \\sum_{y_2\\in\\mathbb{Y_2}}Pr(Y_1= \\text{sales} \\cap Y_2=y_2)\\\\\n    &= Pr(Y_1= \\text{sales} \\cap Y_2=\\text{farm}) + Pr(Y_1= \\text{sales} \\cap Y_2=\\text{operatives}) \\\\\n    & +Pr(Y_1= \\text{sales} \\cap Y_2=\\text{craftsmen}) + Pr(Y_1= \\text{sales} \\cap Y_2=\\text{sales}) \\\\\n    & +Pr(Y_1= \\text{sales} \\cap Y_2=\\text{professional})\\\\\n    &= 0.001 + 0.018 + 0.019 + 0.010 + 0.051\\\\\n    &= 0.099\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    Pr(Y_1 = \\text{professional}) &= \\sum_{y_2\\in\\mathbb{Y_2}}Pr(Y_1= \\text{professional} \\cap Y_2=y_2)\\\\\n    &= Pr(Y_1= \\text{professional} \\cap Y_2=\\text{farm}) + Pr(Y_1= \\text{professional} \\cap Y_2=\\text{operatives}) \\\\\n    & +Pr(Y_1= \\text{professional} \\cap Y_2=\\text{craftsmen}) + Pr(Y_1= \\text{professional} \\cap Y_2=\\text{sales}) \\\\\n    & +Pr(Y_1= \\text{professional} \\cap Y_2=\\text{professional})\\\\\n    &= 0.001 + 0.029 + 0.032 + 0.043 + 0.130\\\\\n    &= 0.235\n\\end{aligned}\n\\]\n\n\nTable 5.2: Marginal probability of father’s occupation\n\n\nmarginal probability\nvalue\n\n\n\n\n\\(p(Y_1=\\text{farm})\\)\n0.11\n\n\n\\(p(Y_1=\\text{operatives})\\)\n0.279\n\n\n\\(p(Y_1=\\text{craftsmen})\\)\n0.277\n\n\n\\(p(Y_1=\\text{sales})\\)\n0.099\n\n\n\\(p(Y_1=\\text{professional})\\)\n0.235\n\n\nSUM\n1.0\n\n\n\n\nTable 5.2 shows that the sum of marginal probability is \\(1\\).\n\n5.2.1.0.1 (b) The marginal probability distribution of a son’s occupation\n\\[\n\\begin{aligned}\n    Pr(Y_2 = \\text{farm}) &= \\sum_{y_1\\in\\mathbb{Y_1}}Pr(Y_2= \\text{farm} \\cap Y_1=y_1)\\\\\n    &= Pr(Y_2= \\text{farm} \\cap Y_1=\\text{farm}) + Pr(Y_2= \\text{farm} \\cap Y_1=\\text{operatives}) \\\\\n    & +Pr(Y_2= \\text{farm} \\cap Y_1=\\text{craftsmen}) + Pr(Y_2= \\text{farm} \\cap Y_1=\\text{sales}) \\\\\n    & +Pr(Y_2= \\text{farm} \\cap Y_1=\\text{professional})\\\\\n    &= 0.018 + 0.002 + 0.001 + 0.001 + 0.001\\\\\n    &= 0.023\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    Pr(Y_2 = \\text{operatives}) &= \\sum_{y_1\\in\\mathbb{Y_1}}Pr(Y_2= \\text{operatives} \\cap Y_1=y_1)\\\\\n    &= Pr(Y_2= \\text{operatives} \\cap Y_1=\\text{farm}) + Pr(Y_2= \\text{operatives} \\cap Y_1=\\text{operatives}) \\\\\n    & +Pr(Y_2= \\text{operatives} \\cap Y_1=\\text{craftsmen}) + Pr(Y_2= \\text{operatives} \\cap Y_1=\\text{sales}) \\\\\n    & +Pr(Y_2= \\text{operatives} \\cap Y_1=\\text{professional})\\\\\n    &= 0.035 + 0.112 + 0.066 + 0.018 + 0.029\\\\\n    &= 0.26\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    Pr(Y_2 = \\text{craftsmen}) &= \\sum_{y_1\\in\\mathbb{Y_1}}Pr(Y_2= \\text{craftsmen} \\cap Y_1=y_1)\\\\\n    &= Pr(Y_2= \\text{craftsmen} \\cap Y_1=\\text{farm}) + Pr(Y_2= \\text{craftsmen} \\cap Y_1=\\text{operatives}) \\\\\n    & +Pr(Y_2= \\text{craftsmen} \\cap Y_1=\\text{craftsmen}) + Pr(Y_2= \\text{craftsmen} \\cap Y_1=\\text{sales}) \\\\\n    & +Pr(Y_2= \\text{craftsmen} \\cap Y_1=\\text{professional})\\\\\n    &= 0.031 + 0.064 + 0.094 + 0.019 + 0.032\\\\\n    &= 0.24\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    Pr(Y_2 = \\text{sales}) &= \\sum_{y_1\\in\\mathbb{Y_1}}Pr(Y_2= \\text{sales} \\cap Y_1=y_1)\\\\\n    &= Pr(Y_2= \\text{sales} \\cap Y_1=\\text{farm}) + Pr(Y_2= \\text{sales} \\cap Y_1=\\text{operatives}) \\\\\n    & +Pr(Y_2= \\text{sales} \\cap Y_1=\\text{craftsmen}) + Pr(Y_2= \\text{sales} \\cap Y_1=\\text{sales}) \\\\\n    & +Pr(Y_2= \\text{sales} \\cap Y_1=\\text{professional})\\\\\n    &= 0.008+0.032+0.032+0.010+0.043\\\\\n    &= 0.125\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    Pr(Y_2 = \\text{professional}) &= \\sum_{y_1\\in\\mathbb{Y_1}}Pr(Y_2= \\text{professional} \\cap Y_1=y_1)\\\\\n    &= Pr(Y_2= \\text{professional} \\cap Y_1=\\text{farm}) + Pr(Y_2= \\text{professional} \\cap Y_1=\\text{operatives}) \\\\\n    & +Pr(Y_2= \\text{professional} \\cap Y_1=\\text{craftsmen}) + Pr(Y_2= \\text{professional} \\cap Y_1=\\text{sales}) \\\\\n    & +Pr(Y_2= \\text{professional} \\cap Y_1=\\text{professional})\\\\\n    &= 0.018+ 0.069+0.084+0.051+0.130\\\\\n    &= 0.352\n\\end{aligned}\n\\]\n\n\nTable 5.3: Marginal probability of son’s occupation\n\n\nmarginal probability\nvalue\n\n\n\n\n\\(p(Y_2=\\text{farm})\\)\n0.023\n\n\n\\(p(Y_2=\\text{operatives})\\)\n0.26\n\n\n\\(p(Y_2=\\text{craftsmen})\\)\n0.24\n\n\n\\(p(Y_2=\\text{sales})\\)\n0.125\n\n\n\\(p(Y_2=\\text{professional})\\)\n0.352\n\n\nSUM\n1.0\n\n\n\n\nTable 5.3 shows that the sum of marginal probability is \\(1\\).\n\n\n\n5.2.2 (c) The conditional distribution of a son’s occupation, given that the father is a farmer;\nThe conditional distribution of a son’s occupation can be expressed as \\(p(y_2 | y_1 = \\text{farmer})\\).\n\\[p(y_2= * | y_1 = \\text{farmer}) = \\frac{p(y_1=\\text{farm} \\cap y_2 = *)}{p(y_1 = \\text{farm})}\\]\nwhere \\(* \\in \\mathbb{Y_2}\\). As described in Table 5.2, \\(p(y_1=\\text{farm})= 0.11\\). Use Table 5.1 to calculate the distribution:\n\\[\n\\begin{aligned}\n    p(y_2 = \\text{farm} | y_1 = \\text{farm}) &= \\frac{0.018}{0.11} \\approx 0.16\\\\\n    p(y_2 = \\text{operatives} | y_1 = \\text{farm}) &= \\frac{0.035}{0.11} \\approx 0.32\\\\\n    p(y_2 = \\text{craftsman} | y_1 = \\text{farm}) &= \\frac{0.031}{0.11} \\approx 0.28\\\\\n    p(y_2 = \\text{sales} | y_1 = \\text{farm}) &= \\frac{0.008}{0.11} \\approx 0.072\\\\\n    p(y_2 = \\text{professional} | y_1 = \\text{farm}) &= \\frac{0.018}{0.11} \\approx 0.16 \\\\\n\\end{aligned}\n\\]\n\n\n5.2.3 (d) The conditional distribution of a father’s occupation, given that the son is a farmer.\n\\[p(y_1= * | y_2 = \\text{farm}) = \\frac{p(y_1=\\text{*} \\cap y_2 = \\text{farm})}{p(y_2 = \\text{farm})}\\]\nAccording to Table 5.3, \\(p(y_2 = \\text{farm}) = 0.023\\). Use Table 5.1 to calculate the distribution:\n\\[\n\\begin{aligned}\n    p(y_1 = \\text{farm} | y_2 = \\text{farm}) &= \\frac{0.018}{0.023} \\approx 0.78\\\\\n    p(y_1 = \\text{operatives} | y_2 = \\text{farm}) &=\\frac{0.002}{0.023} \\approx 0.09\\\\\n    p(y_1 = \\text{craftsman} | y_2 = \\text{farm}) &= \\frac{0.001}{0.023} \\approx 0.04\\\\\n    p(y_1 = \\text{sales} | y_2 = \\text{farm}) &= \\frac{0.001}{0.023} \\approx 0.04\\\\\n    p(y_1 = \\text{professional} | y_2 = \\text{farm}) &= \\frac{0.001}{0.023} \\approx 0.04\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "hw/hw1.html#problem-2.2",
    "href": "hw/hw1.html#problem-2.2",
    "title": "5  Homework 1",
    "section": "5.3 Problem 2.2",
    "text": "5.3 Problem 2.2\n\nExpectations and variances: Let \\(Y_1\\) and \\(Y_2\\) be two independent random variables, such that \\(E[Y_i]=\\mu_i\\) and \\(Var[Y_i] = \\sigma_{i}^2\\). Using the definition of expectation and variance, computing the following quantities, where \\(a_1\\) and \\(a_2\\) are given constants.\n\nBecause \\(Y_1\\) and \\(Y_2\\) are independent,\n\n5.3.1 (a) \\(E[a_1 Y_1 +a_2 Y_2]\\), \\(Var[a_1 Y_1 + a_2 Y_2]\\)\nBecause \\(Y_1\\) and \\(Y_2\\) are independent: \\[\nE[Y_1 Y_2] = E[Y_1] E[Y_2]\n\\]\nThus\n\\[\n\\begin{aligned}\n  E[a_1 Y_1 + a_2 Y_2] &= E[a_1 Y_1] + E[a_2 Y_2]\\\\\n                       &= a_1 E[Y_1] + a_2 E[Y_2]\\\\\n                       &= \\underline{a_1 \\mu_1 + a_2 \\mu_{2}}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    Var[a_1 Y_1 + a_2 Y_2] &= E[ [(a_1 Y_1 + a_2 Y_2) - E[a_1 Y_1 + a_2 Y_2]]^{2}]\\\\\n    &= E[(a_1 Y_1 + a_2 Y_2)^2] - E[a_1 Y_1 + a_2 Y_2]^2\\\\\n    &= E[a_{1}^{2}Y_{1}^{2} + 2a_1 a_2 Y_1 Y_2 + a_{2}^{2}Y_{2}^{2}] - (a_1 \\mu_1 + a_2 \\mu_2)^2\\\\\n    &= a^{2}_{1}E[Y_1^2] + 2a_1 a_2 \\underbrace{\\mu_1 \\mu_2}_{E[Y_1 Y_2]=E[Y_1]E[Y_2]\\quad \\because \\text{Indep.}} + a_{2}^{2}E[Y_{2}^{2}] - (a_1 \\mu_1 + a_2 \\mu_2)^2\\\\\n    &= a^{2}_{1}E[Y_1^2] + 2a_1 a_2\\mu_1 \\mu_2 + a_{2}^{2}E[Y_{2}^{2}] - (a_{1}^{2} \\mu_{1}^{2}+ 2a_{1}a_{2}\\mu_{1}\\mu_{2} + a_{2}^{2} \\mu_{2}^{2})\\\\\n    &= a_{1}^{2}(\\underbrace{E[Y_{1}^2] - \\mu_{1}^{2}}_{\\sigma_{1}^{2}}) + a_{2}^{2}(\\underbrace{E[Y^{2}_{2}] - \\mu_{2}^{2}}_{\\sigma_{2}^{2}})\\\\\n    &= \\underline{a_{1}^{2}\\sigma_{1}^{2} + a_{2}^{2}\\sigma_{2}^{2}}\\\\\n\\end{aligned}\n\\]\n\n\n5.3.2 (b) \\(E[a_1 Y_1 - a_2 Y_2]\\), \\(Var[a_1 Y_1 - a_2 Y_2]\\)\n\\[\n\\begin{aligned}\n  E[a_1 Y_1 - a_2 Y_2] &= E[a_1 Y_1] - E[a_2 Y_2]\\\\\n                       &= a_1 E[Y_1] - a_2 E[Y_2]\\\\\n                       &= \\underline{a_1 \\mu_1 - a_2 \\mu_{2}}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    Var[a_1 Y_1 - a_2 Y_2] &= E[ [(a_1 Y_1 - a_2 Y_2) - E[a_1 Y_1 - a_2 Y_2]]^{2}]\\\\\n    &= E[(a_1 Y_1 - a_2 Y_2)^2] - E[a_1 Y_1 - a_2 Y_2]^2\\\\\n    &= E[a_{1}^{2}Y_{1}^{2} - 2a_1 a_2 Y_1 Y_2 + a_{2}^{2}Y_{2}^{2}] - (a_1 \\mu_1 - a_2 \\mu_2)^2\\\\\n    &= a_{1}^{2}E[Y^{2}_{1}] + a_{2}^{2}E[Y^{2}_{2}] - 2a_1 2_2 \\underbrace{E[Y_1Y_2]}_{=E[Y_1]E[Y_2]\\quad\\because \\text{Indep.}} - (a_1 \\mu_1 - a_2 \\mu_2)^2\\\\\n    &= a_{1}^{2}E[Y^{2}_{1}] + a_{2}^{2}E[Y^{2}_{2}] - 2a_1 2_2 \\mu_1\\mu_2 - (a_{1}^2 \\mu_{1}^2 - 2 a_{1}a_{2}\\mu_1 \\mu_2 + a_{2}^{2} \\mu_{2}^{2})\\\\\n    &= a_{1}^{2}(\\underbrace{E[Y_{1}^2] - \\mu_{1}^{2}}_{\\sigma_{1}^{2}}) + a_{2}^{2}(\\underbrace{E[Y^{2}_{2}] - \\mu_{2}^{2}}_{\\sigma_{2}^{2}})\\\\\n    &= \\underbrace{a_{1}^{2}\\sigma_{1}^{2} + a_{2}^{2}\\sigma^{2}_{2}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "hw/hw1.html#problem-2.3",
    "href": "hw/hw1.html#problem-2.3",
    "title": "5  Homework 1",
    "section": "5.4 Problem 2.3",
    "text": "5.4 Problem 2.3\n\nFull conditionals: Let \\(X\\), \\(Y\\), \\(Z\\) be random variables with joint density (discrete or continuous) \\(p(x,y,z) \\propto f(x,z)g(y,z)h(z)\\). Show that\n\n\n5.4.1 (a) \\(p(x|y,z) \\propto f(x,z)\\) i.e. \\(p(x|y,z)\\) is a function of \\(x\\) and \\(z\\);\nLet \\(c, d\\in \\mathbb{R}\\) constants \\[\n\\begin{aligned}\n    p(x|y,z) &= \\frac{p(x,y,z)}{p(y,z)}\\\\\n             &= \\frac{c\\cdot f(x,z)g(y,z)h(z)}{p(y,z)}\n             &= \\frac{c\\cdot f(x,z)g(y,z)h(z)}{\\int_{x\\in\\mathbb{X}} p(x,y,z)dx}\\\\\n             &= \\frac{c\\cdot f(x,z)g(y,z)h(z)}{d\\cdot \\int_{x\\in \\mathbb{X}} f(x,z)g(y,z)h(z) dx}\\\\\n             &= \\frac{c\\cdot f(x,z)g(y,z)h(z)}{d\\cdot g(y,z)h(z)\\int_{x\\in \\mathbb{X}} f(x,z) dx}\\\\\n             &= \\frac{c\\cdot f(x,z)}{d\\cdot\\int_{x\\in \\mathbb{X}} f(x,z) dx}\\\\\n             &\\propto \\underline{\\frac{f(x,z)}{\\int_{x\\in \\mathbb{X}} f(x,z) dx}}\n\\end{aligned}\n\\]\nThus, \\(p(x|y,z)\\) is a function of \\(f(x,z)\\).\n\n\n5.4.2 (b) \\(p(y|x,z) \\propto g(y,z)\\) i.e. \\(p(y|x,z)\\) is a function of \\(y\\) and \\(z\\);\nLet \\(c, d\\in \\mathbb{R}\\) constants \\[\n\\begin{aligned}\n    p(y|x,z) &= \\frac{p(x,y,z)}{p(x,z)}\\\\\n             &= \\frac{p(x,y,z)}{\\int_{y\\in \\mathbb{Y}}p(x,y,z)dy}\\\\\n             &= \\frac{c\\cdot f(x,z)g(y,z)h(z)}{d\\cdot \\int_{y\\in\\mathbb{Y}} f(x,z)g(y,z)h(z)dy}\\\\\n             &= \\frac{c\\cdot f(x,z)g(y,z)h(z)}{d\\cdot f(x,z)h(z)\\int_{y\\in\\mathbb{Y}} g(y,z)dy}\\\\\n             &\\propto \\frac{f(y,z)}{\\int_{y\\in\\mathbb{Y}}g(y,z)dy}\\\\\n\\end{aligned}\n\\]\n\n\n5.4.3 (c) \\(X\\) and \\(Y\\) are conditionally independent given \\(Z\\).\nLet \\(a_1, a_2 \\in \\mathbb{R}\\) constant,\n\\[\n\\begin{aligned}\np(x|z) &= \\frac{p(x,z)}{p(z)}\\\\\n       &= \\frac{\\int_{y\\in\\mathbb{Y}}p(x,y,z)dy}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} p(x,y,z) dydx}\\\\\n       &= \\frac{\\int_{y\\in\\mathbb{Y}}f(x,z)g(y,z)h(z) dy}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} p(x,y,z) dydx}\\\\\n       &= \\frac{f(x,z)h(z)\\int_{y\\in\\mathbb{Y}}g(y,z)dy}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} p(x,y,z) dydx}\\\\\n       &= \\frac{f(x,z)\\int_{y\\in\\mathbb{Y}}g(y,z)dy}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} f(x,z)g(y,z) dydx}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    p(y|z) &= \\frac{p(y,z)}{p(z)}\\\\\n           &=  \\frac{\\int_{x\\in\\mathbb{X}}p(x,y,z)dx}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} p(x,y,z) dydx}\\\\\n           &= \\frac{\\int_{x\\in \\mathbb{X}} f(x,z)g(y,z)h(z) dx}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} p(x,y,z) dydx}\\\\\n           &= \\frac{g(y,z)h(z)\\int_{x\\in \\mathbb{X}} f(x,z)dx}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} p(x,y,z) dydx}\\\\\n           &= \\frac{g(y,z)\\int_{x\\in \\mathbb{X}} f(x,z)dx}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} f(x,z)g(y,z) dydx}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\np(x|z)p(y|z) &= \\frac{f(x,z)\\int_{y\\in\\mathbb{Y}}g(y,z)dy}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} f(x,z)g(y,z) dydx} \\cdot \\frac{g(y,z)\\int_{x\\in \\mathbb{X}} f(x,z)dx}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} f(x,z)g(y,z) dydx}\\\\\n            &= \\frac{f(x,z)g(y,z)}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} f(x,z)g(y,z) dydx}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    p(x,y|z) &= \\frac{p(x,y,z)}{p(z)}\\\\\n             &= \\frac{f(x,z)g(y,z)h(z)}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} p(x,y,z) dy dx}\\\\\n             &= \\frac{f(x,z)g(y,z)h(z)}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} f(x,z)g(y,z)h(z) dy dx}\\\\\n             &= \\frac{f(x,z)g(y,z)h(z)}{\\int_{x\\in\\mathbb{X}}f(x,z)h(z)\\left[\\int_{y\\in\\mathbb{Y}} g(y,z) dy\\right] dx}\\\\\n             &= \\frac{f(x,z)g(y,z)h(z)}{\\int_{x\\in\\mathbb{X}}f(x,z)h(z)dx \\cdot \\int_{y\\in\\mathbb{Y}} g(y,z) dy}\\\\\n             &=  \\frac{f(x,z)}{\\int_{x\\in\\mathbb{X}}f(x,z)dx}\\frac{g(y,z)}{\\int_{y\\in\\mathbb{Y}} g(y,z) dy}\\\\\n             &= \\frac{f(x,z)g(y,z)}{\\int_{x\\in\\mathbb{X}}\\int_{y\\in\\mathbb{Y}} f(x,z)g(y,z) dydx}\\\\\n             &= \\underline{p(x|z)p(y|z)}\\\\\n\\end{aligned}\n\\]\nThus, \\(p(x,y|z) = p(x|z)p(y|z)\\) that means \\(p(x,y|z)\\) is conditionally independent given \\(z\\)."
  },
  {
    "objectID": "hw/hw1.html#problem-2.5",
    "href": "hw/hw1.html#problem-2.5",
    "title": "5  Homework 1",
    "section": "5.5 Problem 2.5",
    "text": "5.5 Problem 2.5\n\nUrns: Suppose urn \\(H\\) is filled with \\(40\\%\\) green balls and \\(60\\%\\) red balls, and urn \\(T\\) is filled with \\(60\\%\\) green balls and \\(40\\%\\) red balls. Someone will flip a coin and then select a ball from urn \\(H\\) or urn \\(T\\) depending on whether the coin lands heads or tails, respectively. Let \\(X\\) be 1 or 0 if the coin lands heads or tails, and let \\(Y\\) be \\(1\\) or \\(0\\) if the ball is green or red.\n\n\n\nTable 5.4: Probability of choosing a certain ball in a given urn.\n\n\n\nGreen\nRed\n\n\n\n\n\\(H\\) (chosen if head[1])\n0.4\n0.6\n\n\n\\(T\\) (chosen if tail[0])\n0.6\n0.4\n\n\n\n\n\nEvent coding\n\n\n\n1\n0\n\n\n\n\n\\(X\\)\nHead\nTail\n\n\n\\(Y\\)\nGreen\nRed\n\n\n\n\n5.5.1 (a) Write out the joint distribution of \\(X\\) and \\(Y\\) in a table.\nSuppose the coin is fair,\n\\[\n\\begin{aligned}\n    p(X=0 \\cap Y=0) &= p(X=0) p(Y=0|X=0)= 0.5\\cdot 0.4 = 0.2\\\\\n    p(X=0 \\cap Y=1) &= p(X=0) p(Y=1|X=0)= 0.5\\cdot 0.6 = 0.3\\\\\n    p(X=1 \\cap Y=0) &= p(X=1) p(Y=0|X=1)= 0.5\\cdot 0.6 = 0.3\\\\\n    p(X=1 \\cap Y=1) &= p(X=1) p(Y=1|X=1)= 0.5\\cdot 0.4 = 0.2\\\\\n\\end{aligned}\n\\]\n\n\n5.5.2 (b) Find \\(E[Y]\\). What is the probability that the ball is green?\n\\[\n\\begin{aligned}\n    E[Y] &= \\sum_{y\\in \\{0,1\\}} p(Y=y)y\\\\\n         &= p(Y=1)\\cdot 1\\\\\n         &= \\sum_{x\\in\\{0,1\\}} p(Y=1 | X=x)p(X=x)\\\\\n         &= p(Y=1|X=0)p(X=0) + p(Y=1|X=1)p(X=1)\\\\\n         &= 0.6\\cdot 0.5 + 0.4 \\cdot 0.5 \\\\\n         &= 0.3 + 0.2 \\\\\n         &= 0.5\n\\end{aligned}\n\\]\n\n\n5.5.3 (c) Find \\(Var[Y|X=0]\\), \\(Var[Y|X=1]\\) and \\(Var[Y]\\). Thinking of variance as measuring uncertainty, explain intuitively why one of these variances is larger than others.\n\\[\n\\begin{aligned}\n    E[Y|X=0] &= \\sum_{y \\in\\{0,1\\}} P_{Y|X=0}(Y=y|X=0)y\\\\\n             &= P_{Y|X=0}(Y=1|X=0)\\cdot 1 \\\\\n             &= 0.6\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    E[(Y|X=0)^2] &= \\sum_{y \\in\\{0,1\\}} P_{Y|X=0}(Y=y|X=0)y^2\\\\\n             &= P_{Y|X=0}(Y=1|X=0)\\cdot 1 \\\\\n             &= 0.6\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    E[Y|X=1] &= \\sum_{y \\in\\{0,1\\}} P_{Y|X=1}(Y=y|X=1)y\\\\\n             &= P_{Y|X=1}(Y=1|X=1)\\cdot 1 \\\\\n             &= 0.4\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    E[(Y|X=1)^2] &= \\sum_{y \\in\\{0,1\\}} P_{Y|X=0}(Y=y|X=1)y^2\\\\\n             &= P_{Y|X=1}(Y=1|X=1)\\cdot 1 \\\\\n             &= 0.4\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    E[Y^2] &= \\sum_{y\\in\\{0,1\\}} P(Y=y)y^2\\\\\n           &= P(Y=1)\\cdot 1^2\\\\\n           &= P(Y=1 \\cap X=1) + P(Y=1 \\cap X=0)\\\\\n           &= 0.2 + 0.3 = 0.5\\\\\n\\end{aligned}\n\\]\nThus,\n\\[Var[Y|X=0] = E[(Y|X=0)^2] - E[Y|X=0]^2 = 0.6 - 0.6^2 = \\underline{0.24}\\] \\[Var[Y|X=1] = E[(Y|X=1)^2] - E[Y|X=1]^2 = 0.4 - 0.4^2 = \\underline{0.24}\\]\n\\[Var[Y] = E[Y^2] - E[Y]^2 = 0.5 - 0.5^2 = \\underline{0.25}\\]\nExplaination\n\\(Var[Y]\\) is larger than \\(Var[Y|X=0]\\) and \\(Var[Y|X=1]\\) because \\(Y\\) can be more determined by the information of \\(X\\). With known \\(X\\), the distribution of \\(Y\\) is set, and less uncertain with single confirmed distribution.\n\n\n5.5.4 (d) Suppose you see that the ball is green. What is the probability that the coin turned up tails?\n\\[\\begin{aligned}\n    p(X=0 | Y=1) &= \\frac{p(X=0 \\cap Y=1)}{p(Y=1)}\\\\\n                 &= \\frac{p(X=0 \\cap Y=1)}{p(Y=1 | X=0)p(X=0) + p(Y=1|X=1)p(X=1)}\\\\\n                 &= \\frac{0.5 \\cdot 0.6}{0.6\\cdot 0.5 + 0.4\\cdot 0.5}\\\\\n                 &= \\frac{0.3}{0.3+0.2}\\\\\n                 &= \\frac{0.3}{0.5}\\\\\n                 &= \\underline{0.6}\\\\\n\\end{aligned}\\]\n\n\n\n\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical Methods. Vol. 580. Springer."
  },
  {
    "objectID": "hw/hw2.html#homework-description",
    "href": "hw/hw2.html#homework-description",
    "title": "6  Homework 2",
    "section": "6.1 Homework Description",
    "text": "6.1 Homework Description\n\nRead Chapter 3 in the Hoff book.\nThen, do the following exercises in Hoff: 3.1, 3.3, 3.4, 3.7, 3.12.\nFor problems that require a computer, please do and derive as much as possible “on paper,” and include these derivations in your submission file. Then, for the parts that do require the use of a computer (e.g., creating plots), you are free to use any software (R, Python, …) of your choosing; no need to include your code in your write-up. Please make sure you create a single file for submission here on Canvas.\nFor computations involving gamma functions (e.g., 3.7), it is often helpful to work with log-gamma functions instead, to avoid numbers that are too large to be represented by a computer. In R, the functions lbeta() and lgamma() compute the (natural) log of the beta and gamma functions, respectively. See more here: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Special.html\n\n\nPDF version\nDeadline: Sep 20 by 12:01pm"
  },
  {
    "objectID": "hw/hw2.html#computational-enviromnent-setup",
    "href": "hw/hw2.html#computational-enviromnent-setup",
    "title": "6  Homework 2",
    "section": "6.2 Computational Enviromnent Setup",
    "text": "6.2 Computational Enviromnent Setup\n\n6.2.1 Third-party libraries\n\n%matplotlib inline\nimport sys # system information\nimport matplotlib # plotting\nimport scipy # scientific computing\nimport pandas as pd # data managing\nfrom scipy.special import comb\nfrom scipy import stats as st\nfrom scipy.special import gamma\nfrom scipy.special import comb\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Matplotlib setting\nplt.rcParams['text.usetex'] = True\nmatplotlib.rcParams['figure.dpi']= 300\n\n\n\n6.2.2 Version\n\nprint(sys.version)\nprint(matplotlib.__version__)\nprint(scipy.__version__)\nprint(np.__version__)\nprint(pd.__version__)\n\n3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]\n3.6.2\n1.9.3\n1.23.4\n1.5.1"
  },
  {
    "objectID": "hw/hw2.html#problem-3.1",
    "href": "hw/hw2.html#problem-3.1",
    "title": "6  Homework 2",
    "section": "6.3 Problem 3.1",
    "text": "6.3 Problem 3.1\n\nSample survey: Suppose we are going to sample 100 individuals from a county (of size much larger than 100) and ask each sampled person whether they support policy \\(Z\\) or not. Let \\(Y_i = 1\\) if person \\(i\\) in the sample supports the policy, and \\(Y_i = 0\\) otherwise.\n\n\n6.3.1 (a)\n\nAssume \\(Y_1,\\dots, Y_{100}\\) are, conditional on \\(\\theta\\), i.i.d. binary random variables with expectation \\(\\theta\\). Write down the joint distribution of \\(Pr(Y_1 =y1,\\dots, Y_{100} = y_{100}|\\theta)\\) in a compact form. Also write down the form of \\(Pr(\\sum Y_i = y|\\theta)\\).\n\n\\[Pr(Y_1 = y_1,\\dots, Y_{100}=y_{100}|\\theta) = \\underline{\\theta^{\\sum_{u=1}^{100}}(1-\\theta)^{100-\\sum_{u=1}^{100}}}\\]\n\\[Pr(\\sum_{i=1}^{100} Y_i = y |\\theta)= \\underline{{100 \\choose y}\\theta^{y}(1-\\theta)^{100-y}} \\tag{6.1}\\]\n\n\n6.3.2 (b)\n\nFor the moment, suppose you believed that \\(\\theta\\in\\{0.0,0.1,\\dots,0.9,1.0\\}\\). Given that the results of the survey were \\(\\sum^{100}_{i=1} Y_i = 57\\), compute \\(Pr(\\sum Y_{i} = 57|\\theta)\\) for each of these 11 values of \\(\\theta\\) and plot these probabilities as a function of \\(\\theta\\)\n\nFrom Equation 6.1, the sum of supports (\\(y\\)) is on the power term. Thus, directly computation is problematic with limited range of floating number. Converting probability to log scale is a way to bypass this problem.Another way is to use scipy.stats.binom function1\nThe distribution of \\(Pr(\\sum_{i=1}^{100} Y_i = y |\\theta)\\) along with \\(\\theta\\in\\{0.0,0.1,\\dots,0.9,1.0\\}\\) is shown in Table 6.1. The plot of distribution is shown in Figure 6.1.\n\nthetas = np.linspace(0.0,1.0,11)\ntot = 100\nprobs = np.zeros(len(thetas))\ncount = 57\n\nfor (i, theta) in enumerate(thetas):\n  probs[i] = st.binom.pmf(count, tot, theta)\n\n# list of probabilities\npd.DataFrame({\"Theta\": thetas, \"posteriori\": probs})\n\n\n\n\n\nTable 6.1:  Probabilities along with priors \n  \n    \n      \n      Theta\n      posteriori\n    \n  \n  \n    \n      0\n      0.0\n      0.000000e+00\n    \n    \n      1\n      0.1\n      4.107157e-31\n    \n    \n      2\n      0.2\n      3.738459e-16\n    \n    \n      3\n      0.3\n      1.306895e-08\n    \n    \n      4\n      0.4\n      2.285792e-04\n    \n    \n      5\n      0.5\n      3.006864e-02\n    \n    \n      6\n      0.6\n      6.672895e-02\n    \n    \n      7\n      0.7\n      1.853172e-03\n    \n    \n      8\n      0.8\n      1.003535e-07\n    \n    \n      9\n      0.9\n      9.395858e-18\n    \n    \n      10\n      1.0\n      0.000000e+00\n    \n  \n\n\n\n\n\n\nplt.plot(thetas, probs, 'ko');\nplt.xlabel(r\"$\\theta$\");\nplt.ylabel(r\"$Pr(\\sum Y_i = 57 |\\theta)$\");\n\n\n\n\nFigure 6.1: Probabilities along with priors\n\n\n\n\n\n\n6.3.3 (c)\n\nNow suppose you originally had no prior information to believe one of these \\(\\theta\\)-values over another, and so \\(Pr(\\theta=0.0)=Pr(\\theta=0.1)=\\dots=Pr(\\theta=0.9)=Pr(\\theta=1.0)\\). Use Bayes’ rule to compute \\(p(\\theta|\\sum^{n}_{i=1} Y_i = 57)\\) for each \\(\\theta\\)-value. Make a plot of this posterior distribtution as a function of \\(\\theta\\).\n\n\\[\\begin{align}\n  p(\\theta_i |\\sum^{n}_{i=1} Y_i = 57) &= \\frac{p(\\sum^{n}_{i=1} Y_i = 57|\\theta)p(\\theta_i)}{p(\\sum^{n}_{i=1} Y_i = 57)}\\\\\n  &= \\frac{p(\\sum^{n}_{i=1} Y_i = 57|\\theta)p(\\theta_i)}{\\sum_{\\theta\\in\\Theta}p(\\sum^{n}_{i=1} Y_i = 57 | \\theta)p(\\theta)}\n\\end{align}\\]\nThe following is the calculation of the posterior distribution (shown in Table 6.2), and the result is shown in Figure 6.2.\n\np_theta = 1.0/len(thetas)\n\np_y = np.sum( probs*p_theta)\npost_theta = np.zeros(len(thetas))\n\nfor (i, theta) in enumerate(thetas):\n  post_theta[i] = probs[i]*p_theta/p_y\n\n# list of probabilities\npd.DataFrame({\"Theta\": thetas, \"posteriori\":post_theta})\n\n\n\n\n\nTable 6.2:  Posterior distribution depends on discrete uniform distribution of theta. \n  \n    \n      \n      Theta\n      posteriori\n    \n  \n  \n    \n      0\n      0.0\n      0.000000e+00\n    \n    \n      1\n      0.1\n      4.153701e-30\n    \n    \n      2\n      0.2\n      3.780824e-15\n    \n    \n      3\n      0.3\n      1.321705e-07\n    \n    \n      4\n      0.4\n      2.311695e-03\n    \n    \n      5\n      0.5\n      3.040939e-01\n    \n    \n      6\n      0.6\n      6.748515e-01\n    \n    \n      7\n      0.7\n      1.874172e-02\n    \n    \n      8\n      0.8\n      1.014907e-06\n    \n    \n      9\n      0.9\n      9.502335e-17\n    \n    \n      10\n      1.0\n      0.000000e+00\n    \n  \n\n\n\n\n\n\nplt.plot(thetas, post_theta, 'ko');\nplt.xlabel(r\"$\\theta$\");\nplt.ylabel(r\"$p(\\theta_i |\\sum^{n}_{i=1} Y_i = 57)$\");\n\n\n\n\nFigure 6.2: Posterior distribution as a function of theta.\n\n\n\n\n\n\n6.3.4 (d)\n\nNow suppose you allow \\(\\theta\\) to be any value in the interval \\([0,1]\\). Using the uniform prior density for \\(\\theta\\), so that \\(p(\\theta) = 1\\), plot the posterior density \\(p(\\theta) \\times Pr(\\sum^{n}_{i=1} Y_i = 57 |\\theta)\\) as a function of \\(\\theta\\).\n\nAs shown in Figure 6.3.\n\nthetas = np.linspace(0,1, 1000)\np_theta = 1.0/len(thetas)\nprobs = np.zeros(len(thetas))\npost_theta = np.zeros(len(thetas))\ncount = 57\nfor (i, theta) in enumerate(thetas):\n  probs[i] = st.binom.pmf(count, tot, theta)\n  post_theta[i] = probs[i]\n\n# Plotting  \nplt.plot(thetas, post_theta, 'k-');\nplt.xlabel(r\"$\\theta$\");\nplt.ylabel(r\"$p(\\theta_i |\\sum^{n}_{i=1} Y_i = 57)$\");\n\n\n\n\nFigure 6.3: Posterior distribution with continouous uniform prior.\n\n\n\n\n\n\n6.3.5 (e)\n\nAs discussed in this chapter, the posterior distribution of \\(\\theta\\) is \\(beta(1+57, 1+100-57)\\). Plot the posterior density as a function of \\(\\theta\\). Discuss the relationships among all of the plots you have made for this exercise.\n\nThe \\(\\theta\\) with beta distribution is plotted in Figure 6.4.\nFigure 6.2 is the normalized probability via Bayes’ rule (Section 6.3.3). On the other hand, Figure 6.1 is not normalized.\nFigure 6.3 and Figure 6.4 has similar distribution, which means the prior \\(\\theta\\) has little influcence on the posterior distribution. This is because the sample number is large (\\(n=57\\)), and decrease the importance of the prior.\n\ngrid = np.linspace(0,1, 3000)\nthetas_rv = st.beta(1+57, 1+100-57)\nthetas = [thetas_rv.pdf(x) for x in grid]\n\np_theta = 1.0/len(thetas)\nprobs = np.zeros(len(thetas))\npost_theta = np.zeros(len(thetas))\ncount = 57\nfor (i, theta) in enumerate(thetas):\n  probs[i] = st.binom.pmf(count, tot, theta)\n  post_theta[i] = probs[i]\n\n# Plotting  \nplt.plot(thetas, post_theta, 'k-');\nplt.xlabel(r\"$\\theta\\sim beta$\");\nplt.ylabel(r\"$p(\\theta_i |\\sum^{n}_{i=1} Y_i = 57)$\");\n\n\n\n\nFigure 6.4: Posterior distribution with continouous beta prior."
  },
  {
    "objectID": "hw/hw2.html#sec-p-3-3",
    "href": "hw/hw2.html#sec-p-3-3",
    "title": "6  Homework 2",
    "section": "6.4 Problem 3.3",
    "text": "6.4 Problem 3.3\n\nTumor counts: A cancer laboratory is estimating the rate of tumorigenesis in two strains of mice, A and B. They have tumor count data for 10 mice in strain A and 13 mice in strain B. Type A mice have been well studied, and information from other laboratories suggests that type A mice have tumor counts that are approximately Poisson-distributed with a mean of 12. Tumor count rates for type B mice are unknown, but type B mice are related to type A mice. The observed tumor counts for the two populations are \\[\\mathcal{y}_{A} = (12, 9, 12, 14, 13, 13, 15, 8, 15, 6);\\] \\[\\mathcal{y}_{B} = (11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7).\\]\n\n\n6.4.1 (a)\n\nFind the posterior distributions, means, variances and \\(95\\%\\) quantile-based confidence intervals for \\(\\theta_A\\) and \\(\\theta_B\\), assuming a Poisson sampling distribution for each group and the following prior distribution: \\(\\theta_A \\sim gamma(120,10), \\theta_B \\sim gamma(12,1), p(\\theta_A, \\theta_B) = p(\\theta_A) \\times p(\\theta_B)\\)\n\nAccording to Hoff (2009, 580:46–47),\n\\[E[\\theta_{*} | y_1,\\dots, y_{n_{*}}] = \\frac{a_* + \\sum_{i=1}^{n_*} y_{i}}{b_* + n_*}\\]\nwhere \\(*\\in \\{A, B\\}\\). Given\n\\(\\begin{cases}  \\theta_{*} &\\sim gamma(a_*,b_*)\\\\  Y_1,\\dots, Y_{n_*}|\\theta_{*} &\\sim Poisson(\\theta_{*}) \\end{cases}\\)\n\\[\\Rightarrow\\{\\theta_{i}|Y_1,\\dots,Y_{n_*}\\}\\sim gamma(a + \\sum^{n_*}_{i=1} Y_i, b_* + n_*) \\tag{6.2}\\]\nThe properties of Gamma distribution (Hoff 2009, 580:45–46),\n\\[p(\\theta) = \\frac{b^a}{\\Gamma(a)}\\theta^{a-1}e^{-b\\theta}, \\quad \\theta,a,b > 0\\]\n\\[E[\\theta] = \\frac{a}{b} \\tag{6.3}\\] \\[Var[\\theta] = \\frac{a}{b^2} \\tag{6.4}\\]\nType A Mice\n\n\nTable 6.3: Parameters of type A mice\n\n\nParameter\nValue\n\n\n\n\n\\(a_A\\)\n120\n\n\n\\(b_A\\)\n10\n\n\n\\(n_A\\)\n10\n\n\n\\(\\sum_{i=1}^{n_{A}y_{i}}\\)\n\\(12+9+12+14+13+13+15+8+15+6=117\\)\n\n\n\n\nThe posterior distribution of mice A:\n\\[\\{\\theta_A|Y_1,\\dots,Y_{n_A} \\sim gamma(120 + 117, 10+10)= gamma(237,20)\\} \\]\n\n\\(E[\\theta_{A}|\\sum_{i=1}^{n_{A}} Y_{i}] = \\frac{237}{20}= \\underline{11.85}\\)\n\\(Var[\\theta_{A}|\\sum_{i=1}^{n_{A}} Y_{i}] = \\frac{237}{20^2}\\approx \\underline{0.59}\\)\n\\(95\\%\\) quantile-based confidence intervals is shown in Table 6.4\n\n\ndef interval_gamma_95(a,b):\n  rvA = st.gamma(a, scale=1/b)\n  ints = rvA.interval(0.95)\n  return pd.DataFrame({\"Left bound\":[ints[0]], \"Right bound\":[ints[1]]})\n\naA = 237\nbA = 20\ninterval_gamma_95(aA,bA)\n\n\n\n\n\nTable 6.4:  95% quantile-based confidence intervals of mice A. \n  \n    \n      \n      Left bound\n      Right bound\n    \n  \n  \n    \n      0\n      10.389238\n      13.405448\n    \n  \n\n\n\n\n\nType B Mice\nsimilarly,\n\n\nTable 6.5: Parameters of type B mice\n\n\nParameter\nValue\n\n\n\n\n\\(a_B\\)\n12\n\n\n\\(b_B\\)\n1\n\n\n\\(n_B\\)\n13\n\n\n\\(\\sum_{i=1}^{n_{B}y_{i}}\\)\n\\(11+11+10+9+9+8+7+10+6+8+8+9+7=113\\)\n\n\n\n\nThe posterior distribution of mice B:\n\\[\\{\\theta_B|Y_1,\\dots,Y_{n_B} \\sim gamma(12+113, 1+13)= gamma(125, 14)\\} \\]\n\n\\(E[\\theta_{B}|\\sum_{i=1}^{n_{B}} Y_{i}] = \\frac{125}{14} \\approx \\underline{8.93}\\)\n\\(Var[\\theta_{B}|\\sum_{i=1}^{n_{B}} Y_{i}] = \\frac{125}{14^2}\\approx \\underline{0.64}\\)\n\\(95\\%\\) quantile-based confidence intervals is shown in Table 6.6\n\n\naB = 125\nbB = 14\ninterval_gamma_95(aB,bB)\n\n\n\n\n\nTable 6.6:  95% quantile-based confidence intervals of mice B. \n  \n    \n      \n      Left bound\n      Right bound\n    \n  \n  \n    \n      0\n      7.432064\n      10.560308\n    \n  \n\n\n\n\n\n\n\n6.4.2 (b)\n\nComputing and plot the posterior expectation of \\(\\theta_B\\) under the prior distribution \\(\\theta_B \\sim gamma(12\\times n_0, n_0)\\) for each value of \\(n_0\\in \\{1,2,\\dots, 50\\}\\). Descirbe what sort of prior beliefs about \\(\\theta_B\\) to be close to that of \\(\\theta_A\\).\n\nThe posterior distribution can be derived from Equation 6.2. As shown in Figure 6.5, the mean value of \\(\\theta_B\\) with \\(n_0\\) close to \\(50\\) is necessary to have the similar posterior mean as \\(\\theta_A\\).\n\ndef post_gamma(a,b, sumY, n):\n  return st.gamma(a+sumY, scale=1/(b + n))\n\nn0s = np.arange(1, 50, 1)\nsumYB = 11+11+10+9+9+8+7+10+6+8+8+9+7\nnB = 13\npost_theta_rvBs = [post_gamma(12*n0, n0, sumYB, nB) for n0 in n0s]\n\nmeanBs = [post_theta_rvBs[i].mean() for i in range(0, len(n0s))]\n\n\n# Plotting\nplt.plot(n0s, meanBs, \"ko\")\nplt.xlabel(\"$n_0$\")\nplt.ylabel(\"$E[Pr(\\\\theta_{B}|y_{B})]$\");\n\n\n\n\nFigure 6.5: Mean of Posterior distribution of mice B with given n0s.\n\n\n\n\n\n\n6.4.3 (c)\n\nShould knowledge about population \\(A\\) tell us anything about population \\(B\\)? Discuss whether or not it makes sense to have \\(p(\\theta_A,\\theta_B)=p(\\theta_A)\\times p(\\theta_B)\\).\n\nThe understanding of mice \\(A\\) is well known. Though mice \\(B\\) is related to mice \\(A\\), there is possibility that mice \\(B\\) is different from the distribution of \\(A\\). Thus, viewing mice \\(A\\) and mice \\(B\\) with independent prior distribution makes sense."
  },
  {
    "objectID": "hw/hw2.html#problem-3.4",
    "href": "hw/hw2.html#problem-3.4",
    "title": "6  Homework 2",
    "section": "6.5 Problem 3.4",
    "text": "6.5 Problem 3.4\n\nMixtures of beta priors: Estimate the probability \\(\\theta\\) of teen recidivism based on a study in which there were \\(n=43\\) individuals released from incarceration and \\(y=15\\) re-offenders within \\(36\\) months.\n\n\n6.5.1 (a)\n\nUsing a \\(beta(2,8)\\) prior for \\(\\theta\\), plot \\(p(\\theta)\\), \\(p(y|\\theta)\\) and \\(p(\\theta|y)\\) as functions of \\(\\theta\\). Find the posterior mean, mode, and standard deviation of \\(\\theta\\). Find a \\(95\\%\\) quantile-based condifence interval.\n\n\n\\(p(\\theta) \\sim beta(2,8)\\)\n\nPlotted in Figure 6.6\n\n\nAccording to Hoff (2009) pp. 37-38, the conjugate posterior (\\(\\{\\theta|Y=y\\}\\)) given beta as prior is a beta distribution, and \\(Y\\sim binomial(n,\\theta)\\)\n\n\\(p(y|\\theta) = {n\\choose y}\\theta^{y}(1-\\theta)^{(n-y)}\\)\n\nPlotted in Figure 6.7\n\n\\(p(\\theta | y) \\sim beta(a+y, b + n-y) = beta(2+15, 8 + 43 - 15) = beta(17, 36)\\)\n\nPlotted in Figure 6.8\n\\(E[p(\\theta | y)] = \\frac{a}{a+b} = \\frac{17}{17+36} \\approx 0.32\\)\n\\(Mode(p(\\theta | y)) = \\frac{a-1}{a+b-2} \\approx 0.31\\)\n\\(std(p(\\theta | y)) = \\sqrt{var[p(\\theta | y)]} = \\sqrt{\\frac{ab}{(a+b)^2 (a+b+1)}} = \\sqrt{\\frac{17\\times 36}{(17+36)^2 (17+36+1)}}\\approx 0.06\\)\nProperties are shown in Table 6.7.\n\n\n\nthetas = np.linspace(0,1,1000)\nrv_theta = st.beta(2,8)\n\n# Plotting\nplt.plot(thetas, rv_theta.pdf(thetas), 'k-')\nplt.xlabel(\"$\\\\theta$\");\nplt.ylabel(\"$Pr(\\\\theta)$\");\n\n\n\n\nFigure 6.6: Prior distribution.\n\n\n\n\n\nthetas = np.linspace(0,1,1000)\nn = 43\ny = 15\n\npr_like = [st.binom.pmf(y, n, theta) for theta in thetas]\n\n# Plotting\nplt.plot(thetas, pr_like, 'k-')\nplt.xlabel(\"$\\\\theta$\");\nplt.ylabel(\"$Pr(y|\\\\theta)$\");\n\n\n\n\nFigure 6.7: Likelihood\n\n\n\n\n\nthetas = np.linspace(0,1,1000)\nrv_theta = st.beta(2+15, 8 + 43 - 15)\n\n# Plotting\nplt.plot(thetas, rv_theta.pdf(thetas), 'k-')\nplt.xlabel(\"$\\\\theta$\");\nplt.ylabel(\"$Pr(\\\\theta | y)$\");\n\n\n\n\nFigure 6.8: Posterior distribution\n\n\n\n\n\nints = rv_theta.interval(0.95)\n\npd.DataFrame({\"Properties\": [\"Left bound (CI)\", \"Right bound (CI)\", \"mean\", \"mode\", \"standard deviation\"], \"Values\": [ints[0], ints[1], rv_theta.mean(), (17-1)/(17+36-2), rv_theta.std()]})\n\n\n\n\n\nTable 6.7:  Properties of posterior distribution. \n  \n    \n      \n      Properties\n      Values\n    \n  \n  \n    \n      0\n      Left bound (CI)\n      0.203298\n    \n    \n      1\n      Right bound (CI)\n      0.451024\n    \n    \n      2\n      mean\n      0.320755\n    \n    \n      3\n      mode\n      0.313725\n    \n    \n      4\n      standard deviation\n      0.063519\n    \n  \n\n\n\n\n\n\n\n6.5.2 (b)\n\nRepeat (a), but using a \\(beta(8,2)\\) prior for \\(\\theta\\).\n\n\n\\(p(\\theta) \\sim beta(8,2)\\)\n\nPlotted in Figure 6.9\n\n\\(p(y|\\theta) = {n\\choose y}\\theta^{y}(1-\\theta)^{(n-y)}\\)\n\nPlotted in Figure 6.10\n\n\\(p(\\theta | y) \\sim beta(a+y, b + n-y) = beta(8+15, 2 + 43 - 15) = beta(23, 30)\\)\n\nPlotted in Figure 6.11\n\\(E[p(\\theta | y)] = \\frac{a}{a+b} = \\frac{23}{23+30} \\approx 0.434\\)\n\\(Mode(p(\\theta | y)) = \\frac{a-1}{a+b-2} \\approx 0.431\\)\n\\(std(p(\\theta | y)) = \\sqrt{var[p(\\theta | y)]} = \\sqrt{\\frac{ab}{(a+b)^2 (a+b+1)}} = \\sqrt{\\frac{23\\times 30}{(23+30)^2 (23+30+1)}}\\approx 0.07\\)\nProperties are shown in Table 6.8.\n\n\n\nthetas = np.linspace(0,1,1000)\nrv_theta = st.beta(8,2)\n\n# Plotting\nplt.plot(thetas, rv_theta.pdf(thetas), 'k-')\nplt.xlabel(\"$\\\\theta$\");\nplt.ylabel(\"$Pr(\\\\theta)$\");\n\n\n\n\nFigure 6.9: Prior distribution.\n\n\n\n\n\nthetas = np.linspace(0,1,1000)\nn = 43\ny = 15\n\npr_like = [st.binom.pmf(y, n, theta) for theta in thetas]\n\n# Plotting\nplt.plot(thetas, pr_like, 'k-')\nplt.xlabel(\"$\\\\theta$\");\nplt.ylabel(\"$Pr(y|\\\\theta)$\");\n\n\n\n\nFigure 6.10: Likelihood\n\n\n\n\n\nthetas = np.linspace(0,1,1000)\nrv_theta = st.beta(8+15, 2 + 43 - 15)\n\n# Plotting\nplt.plot(thetas, rv_theta.pdf(thetas), 'k-')\nplt.xlabel(\"$\\\\theta$\");\nplt.ylabel(\"$Pr(\\\\theta | y)$\");\n\n\n\n\nFigure 6.11: Posterior distribution\n\n\n\n\n\nints = rv_theta.interval(0.95)\n\npd.DataFrame({\"Properties\": [\"Left bound (CI)\", \"Right bound (CI)\", \"mean\", \"mode\", \"standard deviation\"], \"Values\": [ints[0], ints[1], rv_theta.mean(), (23-1)/(23+30-2), rv_theta.std()]})\n\n\n\n\n\nTable 6.8:  Properties of posterior distribution. \n  \n    \n      \n      Properties\n      Values\n    \n  \n  \n    \n      0\n      Left bound (CI)\n      0.304696\n    \n    \n      1\n      Right bound (CI)\n      0.567953\n    \n    \n      2\n      mean\n      0.433962\n    \n    \n      3\n      mode\n      0.431373\n    \n    \n      4\n      standard deviation\n      0.067445\n    \n  \n\n\n\n\n\n\n\n6.5.3 (c)\n\nConsider the following prior distribution for \\(\\theta\\): \\[p(\\theta) = \\frac{1}{4}\\frac{\\Gamma(10)}{\\Gamma(2)\\Gamma(8)}[3\\theta(1-\\theta)^7+\\theta^7(1-\\theta)]\\] which is a \\(75-25\\%\\) mixture of a \\(beta(2,8)\\) and a \\(beta(8,2)\\) prior distribution. Plot this prior distribution and compare it to the priors in (a) and (b). Describe what sort of prior opinion this may represent.\n\nThe mixture of beta distribution is plotted in Figure 6.12. This opinion merges two opposite suggestions with different weights:\n\n\\(\\theta\\) is low (Figure 6.6).\n\\(\\theta\\) is high (Figure 6.9).\n\n\ndef mixBeta(th):\n  return 0.25*gamma(10)/(gamma(2)*gamma(8))*( 3*th*((1-th)**7) + (th**7)*(1-th) )\n\nthetas = np.linspace(0,1, 1000)\nprs = [mixBeta(theta) for theta in thetas]\n\n# Plotting\nplt.plot(thetas, prs, \"k-\")\nplt.xlabel(\"$\\\\theta$\")\nplt.ylabel(\"$p(\\\\theta)$\");\n\n\n\n\nFigure 6.12: Mixture beta distribution\n\n\n\n\n\n\n6.5.4 (d)\n\nFor the prior in (c):\n\nWrite out mathematically \\(p(\\theta)\\times p(y|\\theta)\\) and simplify as much as possible.\nThe posterior distribution is a mixture of two distributions you know. Identify these distributions.\nOn a computer, calculate and plot \\(p(\\theta) \\times p(y|\\theta)\\) for a variety of \\(\\theta\\) values. Also find (approximately) the posterior mode, and discuss its relation to the modes in (a) and (b).\n\n\nPart 1.\nNoted that \\({43 \\choose 15} = \\frac{43!}{15!28!}=\\frac{\\Gamma(44)}{\\Gamma(16)\\Gamma(29)}\\)\n\\[\\begin{align}\n  p(\\theta)\\times p(y|\\theta) &= \\frac{1}{4}\\frac{\\Gamma(10)}{\\Gamma(2)\\Gamma(8)}[3\\theta(1-\\theta)^7+\\theta^7(1-\\theta)] \\times {43\\choose 15}\\theta^{15}(1-\\theta)^{(43-15)}\\\\\n  &= \\frac{1}{4}\\frac{\\Gamma(10)}{\\Gamma(2)\\Gamma(8)}\\underbrace{{43 \\choose 15}}_{\\frac{\\Gamma(44)}{\\Gamma(16)\\Gamma(29)}}(\\theta^{22}(1-\\theta)^{29} + 3\\theta^{16}(1-\\theta)^{35})\\\\\n  &= \\frac{1}{4}\\frac{\\Gamma(10)}{\\Gamma(2)\\Gamma(8)}\\frac{\\Gamma(44)}{\\Gamma(16)\\Gamma(29)}(\\underline{\\theta^{22}(1-\\theta)^{29}} + \\underline{3\\theta^{16}(1-\\theta)^{35}})\\\\\n\\end{align}\\]\nThe simplification is by the aid of walfram-alpha2.\nPart 2.\nThe distribution is the mixture of \\(Beta(23,30)\\) and \\(Beta(17,36)\\) with certain weights.\nPart 3.\n\nThe mode of \\(p(\\theta) \\times p(y|\\theta)\\) is \\(0.314\\) (See Figure 6.13).\nThe mode in (a): \\(0.313725\\)\nThe mode in (b): \\(0.431373\\)\n\nThus, the posterior distribution has the mode between \\(Beta(2,8)\\) ((a)) and \\(Beta(8,2)\\)((b)), and more close to \\(Beta(2.8)\\)\n\ndef mixture_post(th):\n  scale = 0.25 * gamma(10)/(gamma(2)*gamma(8)) * gamma(44)/(gamma(16)*gamma(29))\n  beta = (th**22)*(1-th)**29 + 3*(th**16)*(1-th)**35\n  return scale*beta\n\nprs = [mixture_post(theta) for theta in thetas]\n\nmaxTh = thetas[np.argmax(prs)]\n\nplt.plot(thetas, prs, 'k-')\nplt.xlabel(\"$\\\\theta$\")\nplt.ylabel(\"$p(\\\\theta)\\\\times p(y|\\\\theta)$\");\nplt.axvline(x=maxTh, linestyle='--', color='k', label= \"Mode={}\".format(maxTh));\nplt.legend();\n\n\n\n\nFigure 6.13: Posterior distribution with mixture of two beta distributions.\n\n\n\n\n\n\n6.5.5 (e)\n\nFind a general formula for the weights of the mixture distribution in (d) 2., and provide an interpretation for their values.\n\nLet \\(c_1 = \\frac{1}{4}\\frac{\\Gamma(10)}{\\Gamma(2)\\Gamma(8)}\\frac{\\Gamma(44)}{\\Gamma(16)\\Gamma(29)}\\)\n\\[\\begin{align}\n  p(\\theta)\\times p(y|\\theta) &= \\frac{1}{4}\\frac{\\Gamma(10)}{\\Gamma(2)\\Gamma(8)}\\frac{\\Gamma(44)}{\\Gamma(16)\\Gamma(29)}(\\underline{\\theta^{22}(1-\\theta)^{29}} + \\underline{3\\theta^{16}(1-\\theta)^{35}})\\\\\n                              &= c_1 (\\theta^{22}(1-\\theta)^{29} + 3\\theta^{16}(1-\\theta)^{35})\\\\\n                              &= c_1 \\theta^{22}(1-\\theta)^{29} + 3c_1 \\theta^{16}(1-\\theta)^{35})\\\\\n                              &= c_1 \\frac{\\Gamma(23)\\Gamma(30)}{\\Gamma(53)} Beta(\\theta, 23,30) + 3 c_1 \\frac{\\Gamma(17)\\Gamma(36)}{\\Gamma(51)} Beta(\\theta, 17,36)\\\\\n                              &= 0.0003 \\times Beta(\\theta, 23,30) + 58.16 \\times Beta(\\theta, 17,36)\\\\\n                              &= \\omega_1 \\cdot Beta(\\theta, 23,30) + \\omega_2 \\cdot Beta(\\theta, 17,36)\n\\end{align}\\]\nThat means \\(Beta(17,36)\\) is preferred to \\(Beta(23,30)\\). The updated posterior information is more close to the (a). That is because the mixture of priors has more weights (\\(75\\%\\)) on the prior of \\(Beta(2,8)\\)."
  },
  {
    "objectID": "hw/hw2.html#problem-3.7",
    "href": "hw/hw2.html#problem-3.7",
    "title": "6  Homework 2",
    "section": "6.6 Problem 3.7",
    "text": "6.6 Problem 3.7\n\nPosterior prediction: Consider a pilot study in which \\(n_1 = 15\\) children enrolled in special education classes were randomly selected and tested for a certain type of learning disability. In the pilot study, \\(y_1 = 2\\) children tested positive for the disability.\n\n\n6.6.1 (a)\n\nUsing a uniform prior distribution, find the posterior distribution of \\(\\theta\\), the fraction of students in special education classes who have the disability. Find the posterior mean, mode and standard deviation of \\(\\theta\\), and plot the posterior density.\n\n\\[\\theta \\sim beta(1,1) (uniform)\\] \\[Y\\sim binomial(n_1,\\theta)\\]\n\\[\\begin{align}\n  \\theta|Y=y &\\sim beta(1+y_1, 1+n_1 - y_1)\\\\\n             &= beta(1 + 2, 1+15-2)\\\\\n             &= beta(3, 14)\\\\\n             &= beta(a_p, b_p)\\\\\n\\end{align}\\]\n\nThe distribution is plotted in Figure 6.14\n\\(E[\\theta|Y] = \\frac{a_p}{a_p+b_p} = \\frac{3}{3+14} \\approx 0.1764\\)\n\\(Mode[\\theta|Y] = \\frac{(a_p - 1)}{a_p - 1 + b_p - 1} = \\frac{(3 - 1)}{3 - 1 + 14 - 1} \\approx 0.1333\\)\n\\(Std[\\theta|Y] = \\sqrt{ \\frac{a_p b_p}{(a_p+b_p+1)(a_p+b_p)^2} } = \\sqrt{ \\frac{3\\cdot 14}{(3+14+1)(3+14)^2}} \\approx 0.0899\\)\n\n\nthetas = np.linspace(0,1,1000)\npos = st.beta(3, 14)\npr_pos = [pos.pdf(theta) for theta in thetas]\n\nplt.plot(thetas, pr_pos, \"k-\")\nplt.xlabel(\"$\\\\theta$\")\nplt.ylabel(\"$Pr(\\\\theta|Y)$\");\n\n\n\n\nFigure 6.14: Posterior distribution\n\n\n\n\n\nResearchers would like to recruit students with the disability to participate in a long-term study, but first they need to make sure they can recruit enough students. Let \\(n_2 = 278\\) be the number of children in special education classes in this particular school district, and let \\(Y_2\\) be the number of students with the disability.\n\n\n\n6.6.2 (b)\n\nFind \\(Pr(Y_2=y_2|Y_1 =2)\\), the posterior predictive distribution of \\(Y_2\\), as follows:\n\nDiscuss what assumptions are needed about the joint distribution of \\((Y_1, Y_2)\\) such that the fololowing is true: \\[Pr(Y_2=y_2 |Y_1=2) = \\int^{1}_{0} Pr(Y_2=y_2|\\theta)p(\\theta|Y_1=2)d\\theta \\tag{6.5}\\]\nNow plug in the forms of \\(Pr(Y_2=y_2|\\theta)\\) and \\(p(\\theta|Y_1 =2)\\) in the above integral.\nFigure out what the above integral must be by using the calculus result discussed in Section 3.1.\n\n\nPart 1\n\nThe assumption is that \\(Y_2\\) is conditionally independent on \\(Y_1\\) over \\(\\theta\\)\n\nThus,\n\\[\\begin{align}\n  \\int^{1}_{0} Pr(Y_2=y_2|\\theta)p(\\theta|Y_1=2)d\\theta &= \\int^{1}_{0} Pr(Y_2=y_2)|\\theta, Y_1=2)p(\\theta |Y_1=2)d\\theta\\\\\n  &= \\int^{1}_{0} Pr(Y_2=y_2, \\theta |Y_1 =2) d\\theta\\\\\n  &= Pr(Y_2 = y_2 | Y_1=2)\n\\end{align}\\]\nThe equality of Equation 6.5 holds.\nPart 2\n\\[\\begin{align}\n  Pr(Y_2=y_2 |Y_1=2) &= \\int^{1}_{0} Pr(Y_2=y_2|\\theta)p(\\theta|Y_1=2)d\\theta\\\\\n                     &= \\int^{1}_{0} binomial(y_2, n_2, \\theta) beta(\\theta, 3,14) d\\theta\\\\\n                     &= \\int^{1}_{0} {n_2 \\choose y_2}\\theta^{y_2}(1-\\theta)^{n_2-y_2} \\frac{\\Gamma(17)}{\\Gamma(3)\\Gamma(14)}\\theta^{2}(1-\\theta)^{13} d\\theta\\\\\n                     &= {n_2 \\choose y_2}\\frac{\\Gamma(17)}{\\Gamma(3)\\Gamma(14)} \\int^{1}_{0} \\theta^{y_2}(1-\\theta)^{n_2-y_2} \\theta^{2}(1-\\theta)^{13} d\\theta\\\\&= {n_2 \\choose y_2}\\frac{\\Gamma(17)}{\\Gamma(3)\\Gamma(14)} \\int^{1}_{0} \\theta^{(2+y_2)}(1-\\theta)^{n_2 - y_2 +13} d\\theta\\\\\n                     &= {278\\choose y_2}\\frac{\\Gamma(17)}{\\Gamma(3)\\Gamma(14)} \\int^{1}_{0} \\theta^{(2+y_2)}(1-\\theta)^{278 - y_2 +13} d\\theta\\\\\n                     &= {278\\choose y_2}\\frac{\\Gamma(17)}{\\Gamma(3)\\Gamma(14)} \\int^{1}_{0} \\theta^{(2+y_2)}(1-\\theta)^{291 - y_2} d\\theta\\\\\n\\end{align}\\]\nPart 3\nUse the calculus trick:\n\\[\\int^{1}_{0} \\theta^{a-1}(1-\\theta)^{b-1}d\\theta = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\]\n\\[\\begin{align}\n  \\int^{1}_{0} \\theta^{(2+y_2)}(1-\\theta)^{291 - y_2} d\\theta &= \\int^{1}_{0} \\theta^{(3+y_2 - 1)}(1-\\theta)^{292 - y_2 - 1} d\\theta\\\\\n  &= \\frac{\\Gamma(3+y_2)\\Gamma(292 - y_2)}{\\Gamma(3+y_2 + 292 - y_2)}\\\\\n  &= \\frac{\\Gamma(3+y_2)\\Gamma(292 - y_2)}{\\Gamma(295)}\\\\\n\\end{align}\\]\n\\(\\therefore\\) \\[\\begin{align}\nPr(Y_2=y_2 |Y_1=2) &= {278\\choose y_2}\\frac{\\Gamma(17)}{\\Gamma(3)\\Gamma(14)}\\frac{\\Gamma(3+y_2)\\Gamma(292 - y_2)}{\\Gamma(295)}\\\\\n                   &= \\frac{\\Gamma(278)}{\\Gamma(y_2)\\Gamma(278-y_2)} \\frac{\\Gamma(17)}{\\Gamma(3)\\Gamma(14)}\\frac{\\Gamma(3+y_2)\\Gamma(292 - y_2)}{\\Gamma(295)}\\\\\n                   &= \\frac{\\Gamma(3+y_2)}{\\Gamma(y_2)}\\frac{\\Gamma(278)}{\\Gamma(295)}\\frac{\\Gamma(292-y_2)}{\\Gamma(278-y_2)} \\frac{\\Gamma(17)}{\\Gamma(3)\\Gamma(14)}\\\\\n                   &= \\prod^{3+y_2 -1}_{i=y_2} i \\times \\frac{1}{\\prod^{295-1}_{i=278}i}\\prod^{292-y_2 - 1}_{i=278-y_2}i \\times 1680\\\\\n                   &= \\prod^{2+y_2}_{i=y_2} i \\times \\frac{1}{\\prod^{294}_{i=278}i}\\prod^{291-y_2}_{i=278-y_2}i \\times 1680\\\\\n\\end{align}\\]\n\n\n6.6.3 (c)\n\nPlot the function \\(Pr(Y_2 = y_2 | Y_1 =2)\\) as a function of \\(y_2\\). Obtain the mean and standard deviation of \\(Y_2\\), given \\(Y_1 = 2\\).\n\n\nThe plot of \\(Pr(Y_2 = y_2 | Y_1 =2)\\) is in Figure 6.15.\nmean and standard deviation are displayed in Table 6.9.\n\n\ndef prod(a, b):\n  s = 1.0\n  for i in np.arange(a,b+1, 1.0):\n    s = s*i\n  return s\n\ndef pred_prob(y2, n2=278):\n  return prod(y2, 2+y2)*(1/prod(278, 294))*prod(278-y2, 291-y2)*1680\n\ny2s = np.linspace(0, 278, 279)\nprs = [pred_prob(y2) for y2 in y2s]\nprs = prs/np.sum(prs)\nplt.plot(y2s, prs, 'ko')\nplt.xlabel(\"$y_{2}$\")\nplt.ylabel(\"$p(Y_2 | Y_1=2)$\");\n\n\n\n\nFigure 6.15: Predictive distribution given Y1=2\n\n\n\n\n\nmean = np.sum(prs * y2s)/np.sum(prs)\nstd =  np.sqrt(np.sum(y2s*y2s*prs) - (np.sum(y2s*prs))**2)\n\npd.DataFrame({\"mean\": [np.sum(prs * y2s)/np.sum(prs)], \"std\": [std]})\n\n\n\n\n\nTable 6.9:  Predictive distribution given Y1=2 \n  \n    \n      \n      mean\n      std\n    \n  \n  \n    \n      0\n      59.105263\n      26.01193\n    \n  \n\n\n\n\n\n\n\n6.6.4 (d)\n\nThe posterior mode and the MLE (maximum likelihood estimate) of \\(\\theta\\), based on data from the pilot study, are both \\(\\hat{\\theta} = \\frac{2}{15}\\). Plot the distribution \\(Pr(Y_2 = y_2|\\theta=\\hat{\\theta})\\), and find the mean and standard deviation of \\(Y_2\\) given \\(\\theta=\\hat{\\theta}\\). Compare these results to the plots and calculation in (c) and discuss any differences. Which distribution for \\(Y_2\\) would you used to make predictions, and why?\n\n\\[\\begin{align}\n  Pr(Y_2 = y_2 |\\theta=\\hat{\\theta}) &= binomial(y_2, n_2, \\hat{\\theta})\\\\\n\\end{align}\\]\n\nThe plot of \\(Pr(Y_2 = y_2|\\theta=\\hat{\\theta})\\) distribution along with \\(y_2\\) is Figure 6.16.\nMean and standard deviation are shown in Table 6.10.\nCompare to Figure 6.15, Figure 6.16 has less variation and less mean, which is more close to the original average of \\(Y_1\\) data (\\(=\\frac{2}{15}\\)).\nFigure 6.15 provides better prediction with MLE \\(\\theta\\) because its properties is more related to the original average, and the likelihood is maximized with MLE method.\n\n\nn2 = 278\nth = 2/15\nrv = st.binom(n2, th)\n\ny2s = np.linspace(0,n2, n2+1)\nprs = [rv.pmf(y2) for y2 in y2s]\n\nplt.plot(y2s, prs, \"ko\")\nplt.xlabel(\"$y_2$\")\nplt.ylabel(\"$Pr(Y_2 = y_2|\\\\theta=\\\\hat{\\\\theta})$\");\n\n\n\n\nFigure 6.16: Predictive distribution given MLE theta\n\n\n\n\n\nmean = np.sum(y2s*prs)\nstd = np.sqrt(np.sum(y2s*y2s*prs) - (np.sum(y2s*prs))**2)\npd.DataFrame({\"mean\": [np.sum(prs * y2s)/np.sum(prs)], \"std\": [std]})\n\n\n\n\n\nTable 6.10:  Predictive distribution given MLE theta \n  \n    \n      \n      mean\n      std\n    \n  \n  \n    \n      0\n      37.066667\n      5.667843"
  },
  {
    "objectID": "hw/hw2.html#problem-3.12",
    "href": "hw/hw2.html#problem-3.12",
    "title": "6  Homework 2",
    "section": "6.7 Problem 3.12",
    "text": "6.7 Problem 3.12\n\nJeffrey’s prior: Jeffreys (1961) suggested a default rule for gnerating a prior distribution of a parameter \\(\\theta\\) in a sampling model \\(p(y|\\theta)\\). Jeffreys’ prior is given by \\(p_{J}\\propto \\sqrt{I(\\theta)}\\), where \\(I(\\theta) = - E[\\frac{\\partial^{2} \\log p(Y|\\theta)}{\\partial\\theta^2} | \\theta]\\) is the Fisher information.\n\n\n6.7.1 (a)\n\nLet \\(Y\\sim binomial(n,\\theta)\\). Obtain Jeffreys’ prior distribution \\(p_J(\\theta)\\) for this model.\n\n\\(\\because\\) \\(Y\\sim binomial(n,\\theta)\\) \\(\\therefore\\) \\(E[Y]=n\\theta\\)\n\\[\\begin{align}\np(y|\\theta) &= {n\\choose y}\\theta^{y}(1-\\theta)^{n-y}\\\\\n\\log(p(y|\\theta)) &= \\log {n\\choose y} + y\\log \\theta + (n-y)\\log (1-\\theta)\\\\\n\\frac{\\partial \\log(p(y|\\theta))}{\\partial \\theta} &= \\frac{y}{\\theta} - \\frac{n-y}{1-\\theta}\\\\\n\\frac{\\partial^2 \\log(p(y|\\theta))}{\\partial^2 \\theta} &= \\frac{-y}{\\theta^2} - \\frac{n-y}{(1-\\theta)^2}\\\\\nE[\\frac{\\partial^2 \\log(p(y|\\theta))}{\\partial^2 \\theta} |\\theta] &= -\\frac{n\\theta}{\\theta^2} - \\frac{n-n\\theta}{(1-\\theta)^2}\\\\\nE[\\frac{\\partial^2 \\log(p(y|\\theta))}{\\partial^2 \\theta} |\\theta] &= \\frac{-n}{\\theta} - \\frac{n}{1-\\theta}\\\\\n- E[\\frac{\\partial^2 \\log(p(y|\\theta))}{\\partial^2 \\theta} |\\theta] &= \\frac{n}{\\theta} + \\frac{n}{1-\\theta}\\\\\nI(\\theta) &= \\frac{n}{\\theta} + \\frac{n}{1-\\theta}\\\\\n          &= \\frac{n}{\\theta(1-\\theta)}\\\\\n\\end{align}\\]\n\\(\\because\\) \\(p_J \\propto \\sqrt{I(\\theta)}\\)\n\\[\\begin{align}\n  p_J(\\theta) \\propto &\\sqrt{I(\\theta)}\\\\\n                      &= \\sqrt{\\frac{n}{\\theta(1-\\theta)}}\\\\\n\\end{align}\\]\nLet \\(c\\) be the scalar. By the fact that \\(\\frac{d}{dx}(\\sin^{-1}x)=\\frac{1}{\\sqrt{1-x^2}}\\),\n\\[P_J (\\theta) = c \\times \\sqrt{\\frac{n}{\\theta(1-\\theta)}}\\]\n\\[\\begin{align}\n1 &= c \\int^{1}_{0} \\sqrt{\\frac{n}{\\theta(1-\\theta)}} d\\theta\\\\\n1 &= nc \\int^{1}_{0} \\sqrt{\\frac{1}{\\theta(1-\\theta)}} d\\theta\\\\\n1 &= nc \\left[  -2 \\sin^{-1}(\\sqrt{1-x})   \\right]^{1}_{0} \\\\\n1 &= -2\\times nc (\\underbrace{\\sin^{-1}(0)}_{=0} - \\underbrace{\\sin^{-1}(1)}_{=\\frac{\\pi}{2}})\\\\\n1 &= \\pi n c \\\\\nc &= \\frac{1}{\\pi n}\\\\\n\\end{align}\\]\nThus,\n\\[p_J(\\theta) = \\frac{1}{\\pi n} \\sqrt{\\frac{n}{\\theta(1-\\theta)}}\\]\n\\[p_J(\\theta) = \\underline{\\frac{1}{\\pi \\sqrt{n}}\\frac{1}{\\sqrt{\\theta(1-\\theta)}}} \\tag{6.6}\\]\n\n\n6.7.2 (b)\n\nReparameterize the binomial sampling model with \\(\\psi = \\log \\theta / (1-\\theta)\\), so that \\(p(y|\\psi) = {n\\choose y} e^{\\psi y} (1+e^{\\psi})^{-n}\\). Obtain Jefferys’ prior distribution \\(p_J (\\psi)\\) for this model.\n\n\\[\\begin{align}\n  p(y|\\psi) &= {n\\choose y} e^{\\psi y} (1+e^{\\psi})^{-n}\\\\\n  \\log(p(y|\\psi)) &= {n\\choose y} + \\psi y \\underbrace{\\log(e)}_{=1} - n\\log(1+e^{\\psi})\\\\\n  \\log(p(y|\\psi)) &= {n\\choose y} + \\psi y  - n\\log(1+e^{\\psi})\\\\\n  \\frac{\\partial \\log p(y|\\psi)}{\\partial \\psi} &= y - n\\frac{e^{\\psi}}{1+e^{\\psi}}\\\\\n  \\frac{\\partial^2 \\log p(y|\\psi)}{\\partial^2 \\psi} &= -n \\frac{e^{\\psi}}{(1+e^{\\psi})^2}\\\\\n  E[ \\frac{\\partial^2 \\log p(y|\\psi)}{\\partial^2 \\psi} | \\psi] &= -n \\frac{e^{\\psi}}{(1+e^{\\psi})^2}\\\\\n  I(\\psi) = -E[ \\frac{\\partial^2 \\log p(y|\\psi)}{\\partial^2 \\psi} | \\psi] &= n \\frac{e^{\\psi}}{(1+e^{\\psi})^2}\\\\\n\\end{align}\\]\n\\(\\therefore\\) \\(p_{J}(\\psi) \\propto \\sqrt{I(\\psi)} = \\sqrt{\\frac{n e^{\\psi}}{(1+e^{\\psi})^2}}\\)\n\\[p_{J}(\\psi) \\propto \\frac{\\sqrt{n e^{\\psi}}}{1+e^{\\psi}}\\]\n\n\n6.7.3 (c)\n\nTake the prior distribution from (a) and apply the change of variables formula from Exercise 3.10 to obtain the induced prior density on \\(\\psi\\).\nThis density should be the same as the one derived in part (b) of this exercise. This consistency under reparameterization is the defining characteristic of Jeffrey’s’ prior.\n\n\\[\\psi = g(\\theta) = \\log[\\frac{\\theta}{1-\\theta}]\\]\n\\[\\theta = h(\\psi) = \\frac{e^{\\psi}}{1+e^{\\psi}}\\]\nFrom Equation 6.6, \\(p_{\\theta}(\\theta) = \\frac{1}{\\pi \\sqrt{n}}\\frac{1}{\\sqrt{\\theta(1-\\theta)}}\\),\n\\[\\begin{align}\n  p_{\\psi}(\\psi) &= \\frac{1}{\\pi \\sqrt{n}} p_{\\theta}(h(\\psi)) \\times |\\frac{dh}{d\\psi}|\\\\\n                 &=  \\frac{1}{\\pi \\sqrt{n}} \\frac{1+e^{\\psi}}{\\sqrt{e^{\\psi}(1+e^{\\psi}-e^{\\psi})}}\\times \\frac{e^{\\psi}}{(1+e^{\\psi})^2}\\\\\n                 &= \\frac{1}{\\pi \\sqrt{n}} \\frac{1+e^{\\psi}}{\\sqrt{e^{\\psi}}}\\times \\frac{e^{\\psi}}{(1+e^{\\psi})^2}\\\\\n                 &= \\frac{1}{\\pi\\sqrt{n}} \\frac{\\sqrt{e^{\\psi}}}{1+e^{\\psi}}\\\\\n                 &\\propto \\frac{\\sqrt{e^{\\psi}}}{1+e^{\\psi}}\\\\\n                 &\\propto p_{J}(\\psi)\n\\end{align}\\]\n\n\n\n\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical Methods. Vol. 580. Springer."
  },
  {
    "objectID": "hw/hw3.html#description",
    "href": "hw/hw3.html#description",
    "title": "7  Homework 3",
    "section": "7.1 Description",
    "text": "7.1 Description\n\nCourse: STAT638, 2022 Fall\n\n\nDo the following exercises in Hoff: 3.8, 3.9, 3.14.\nIn Exercise 3.9, you should be able to avoid “brute-force” integration by exploiting the fact that the Galenshore distribution is a proper distribution, meaning that the density of the Galenshore(a,b) distribution integrates to one for any \\(a,b>0\\).\nFor 3.14(b), note that \\(p_U(\\theta)\\) is proportional to the density of a known distribution.\nPlease note that while there are only 3 problems in this assignment, some of them are fairly challenging. So please don’t wait too long to get started on this assignment.\n\n\nDeadline: Sept. 27, 12:01pm"
  },
  {
    "objectID": "hw/hw3.html#computational-enviromnent-setup",
    "href": "hw/hw3.html#computational-enviromnent-setup",
    "title": "7  Homework 3",
    "section": "7.2 Computational Enviromnent Setup",
    "text": "7.2 Computational Enviromnent Setup\n\n7.2.1 Third-party libraries\n\n%matplotlib inline\nimport sys # system information\nimport matplotlib # plotting\nimport scipy # scientific computing\nimport random \nimport pandas as pd # data managing\nfrom scipy.special import comb\nfrom scipy import stats as st\nfrom scipy.special import gamma\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Matplotlib setting\nplt.rcParams['text.usetex'] = True\nmatplotlib.rcParams['figure.dpi']= 300\n\n\n\n7.2.2 Version\n\nprint(sys.version)\nprint(matplotlib.__version__)\nprint(scipy.__version__)\nprint(np.__version__)\nprint(pd.__version__)\n\n3.8.12 (default, Oct 22 2021, 18:39:35) \n[Clang 13.0.0 (clang-1300.0.29.3)]\n3.3.1\n1.5.2\n1.19.1\n1.1.1"
  },
  {
    "objectID": "hw/hw3.html#problem-3.8",
    "href": "hw/hw3.html#problem-3.8",
    "title": "7  Homework 3",
    "section": "7.3 Problem 3.8",
    "text": "7.3 Problem 3.8\n\nCoins: Diaconis and Ylvisaker (1985) suggest that coins spun on a flat surface display long-run frequencies of heads that vary from coin to coin. About \\(20\\%\\) of the coins behave symmetrically, whereas the remaining coins tend to give frequencies of \\(\\frac{1}{3}\\) or \\(\\frac{2}{3}\\).\n\nLet \\(\\theta\\) be the priobability of tossing head.1\n\n7.3.1 (a)\n\nBased on the observations of Diaconis and Ylvisaker (1985), use an appropriate mixture of beta distributions as a prior distribution for \\(\\theta\\), the long-run frequency of heads for a particular coin. Plot your prior.\n\nLet the prior probability \\(p_i(\\theta)\\) be a mixture of \\(Beta(a_i,b_i)\\) with \\(i=[1,2,3]\\), and coeifficient \\(k = [k_1, k_2, k_3]\\) with \\(\\sum_{i=1}^{3} k_j = 1\\).\nLet the prior probabiility be\n\\[\\begin{align}\n  p(\\theta) &= \\sum_{i=1}^{3} k_i p_i(\\theta)\\\\\n  &= k_1 p_1(\\theta) + k_2 p_2(\\theta) + k_3 p_3(\\theta)\\\\\n  &= 0.2 \\times Beta(\\theta, a_1, b_1) + 0.4 \\times Beta(\\theta, a_2, b_2) + 0.4 \\times Beta(\\theta, a_3, b_3)\\\\\n  &= 0.2 \\times Beta(\\theta, 3, 3) + 0.4 \\times Beta(\\theta,2, 4) + 0.4 \\times Beta(\\theta,4, 2)\n\\end{align}\\]\nThe distribution is shown in Figure 7.1.\n\n\n\n\n\nFigure 7.1: Designed mixture prior.\n\n\n\n\n\n\n7.3.2 (b)\n\nChoose a single coin and spin it at least \\(50\\) times. Record the number of heads obtained. Report the year and denomination of the coin.\n\nLet \\(n>50\\) be the number of flips, and \\(x\\) be the number of heads obtained.\n\n# A single psudo coin with unknown probability of flipping head\nclass PseudoCoin:\n  def __init__(self, random_state=202209):\n    np.random.seed(random_state)\n    self.random_state = random_state\n    self.ph = np.random.rand()\n    self.rv = st.bernoulli(self.ph)\n\n  def flips(self, n):\n    return self.rv.rvs(n, random_state=self.random_state)\n\n# parameters setting\nn = 100 # number of flips\ncoin = PseudoCoin()\n\n# Experiment\nrs = coin.flips(n)\n\n# Results\nprint(rs)\n\n[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0]\n\n\n\n\n\n\n\n\nTable 7.1:  Satistics of the flipping coin experiment \n  \n    \n      \n      Properties\n      Values\n    \n  \n  \n    \n      0\n      N\n      100\n    \n    \n      1\n      Number of heads (y=1)\n      12\n    \n    \n      2\n      Number of tails\n      88\n    \n  \n\n\n\n\n\n\n\n7.3.3 (c)\n\nCompute your posterior for \\(\\theta\\), based on the information obtained in (b)\n\nFor \\(i = \\{1,2,3\\}\\), the posterior probability of single distribution is\n\\[\\begin{align}\np_{i}(\\theta|y) &= \\frac{p_{i}(\\theta)p(y|\\theta)}{\\underbrace{\\int_{\\theta\\in [0,1]}p_i(\\theta)p(y|\\theta) d\\theta}_{=C_j}}\\\\\n&= \\frac{Beta(\\theta, a_i, b_i) {n\\choose y}\\theta^{y}(1-\\theta)^{n-y}}{\\underbrace{\\int_{0}^{1}Beta(\\theta, a_i, b_i){n\\choose y}\\theta^{y}(1-\\theta)^{n-y} d\\theta}_{=C_j}} \\\\\n&= \\frac{\\frac{\\Gamma(a_i +b_i)}{\\Gamma(a_i)\\Gamma(b_i)} {n\\choose y}\\theta^{y}(1-\\theta)^{n-y}}{\\int_{0}^{1}\\frac{\\Gamma(a_i +b_i)}{\\Gamma(a_i)\\Gamma(b_i)}{n\\choose y}\\theta^{y}(1-\\theta)^{n-y} d\\theta}\\\\\n&= \\frac{\\frac{\\Gamma(a_i +b_i)}{\\Gamma(a_i)\\Gamma(b_i)}\\theta^{(a_i-1)}(1-\\theta)^{b_i - 1}{n\\choose y}\\theta^{y}(1-\\theta)^{n-y}}{\\int_{0}^{1}\\frac{\\Gamma(a_i +b_i)}{\\Gamma(a_i)\\Gamma(b_i)}\\theta^{(a_i-1)}(1-\\theta)^{b_i - 1}{n\\choose y}\\theta^{y}(1-\\theta)^{n-y} d\\theta}\\\\\n&= Beta(\\theta, a_i + y, b_i + n - y)\n\\end{align}\\]\nThe posterior distribution of individual prior is\n\\[\\begin{align}\n  p_{1}(\\theta|y) &= Beta(\\theta, a_1 + y, b_1 + n - y)\\\\\n                  &= Beta(\\theta, 3+12,3+100-12) = Beta(\\theta, 15,91)\\\\\n  p_{2}(\\theta|y) &= Beta(\\theta, a_2 + y, b_2 + n - y)\\\\\n                  &= Beta(\\theta, 2 + 12, 4 + 100 - 12)= Beta(\\theta, 14, 92)\\\\\n  p_{3}(\\theta|y) &= Beta(\\theta, a_3 + y, b_3 + n - y)\\\\\n                  &= Beta(\\theta, 4 + 12, 2 + 100 - 12)=Beta(\\theta, 16, 90)\\\\\n\\end{align}\\]\n\\[\\begin{align}\n  C_j &= \\int_{0}^{1} \\frac{\\Gamma(a_i +b_i)}{\\Gamma(a_i)\\Gamma(b_i)}\\theta^{(a_i-1)}(1-\\theta)^{b_i - 1}{n\\choose y}\\theta^{y}(1-\\theta)^{n-y} d\\theta\\\\\n  &= \\int_{0}^{1} \\frac{\\Gamma(a_i +b_i)}{\\Gamma(a_i)\\Gamma(b_i)}\\frac{\\Gamma(n)}{\\Gamma(y)\\Gamma(n-y)}\\theta^{(a_i-1)}(1-\\theta)^{b_i - 1}\\theta^{y}(1-\\theta)^{n-y} d\\theta\\\\\n  &=  \\frac{\\Gamma(a_i +b_i)}{\\Gamma(a_i)\\Gamma(b_i)}\\frac{\\Gamma(n)}{\\Gamma(y)\\Gamma(n-y)} \\int_{0}^{1} \\theta^{(a_i-1)}(1-\\theta)^{b_i - 1}\\theta^{y}(1-\\theta)^{n-y} d\\theta\\\\\n  &= \\frac{\\Gamma(a_i +b_i)}{\\Gamma(a_i)\\Gamma(b_i)}\\frac{\\Gamma(n)}{\\Gamma(y)\\Gamma(n-y)} \\int^{1}_{0} \\theta^{a_i +y - 1}(1-\\theta)^{b_i + n - y -1} d\\theta\\\\\n  &= \\frac{\\Gamma(a_i +b_i)}{\\Gamma(a_i)\\Gamma(b_i)}\\frac{\\Gamma(n)}{\\Gamma(y)\\Gamma(n-y)} \\frac{\\Gamma(a_i + y)\\Gamma(b_i +n -y)}{\\Gamma(a_i + b_i + n)}\n\\end{align}\\]\n\\[\\begin{align}\n  C_1 &= \\frac{\\Gamma(3+3)}{\\Gamma(3)\\Gamma(3)}\\frac{\\Gamma(100)}{\\Gamma(12)\\Gamma(88)}\\frac{\\Gamma(3+12)\\Gamma(3+100-12)}{\\Gamma(3+3+100)}\\\\\n      &= \\frac{\\Gamma(6)}{\\Gamma(3)\\Gamma(3)}\\frac{\\Gamma(100)}{\\Gamma(12)\\Gamma(88)}\\frac{\\Gamma(15)\\Gamma(91)}{\\Gamma(106)}\\\\\n  C_2 &= \\frac{\\Gamma(2 + 4)}{\\Gamma(2)\\Gamma(4)}\\frac{\\Gamma(100)}{\\Gamma(12)\\Gamma(88)} \\frac{\\Gamma(2 + 12)\\Gamma(4 + 100 - 12)}{\\Gamma(2 + 4 + 100)}\\\\\n  &= \\frac{\\Gamma(6)}{\\Gamma(2)\\Gamma(4)}\\frac{\\Gamma(100)}{\\Gamma(12)\\Gamma(88)} \\frac{\\Gamma(14)\\Gamma(92)}{\\Gamma(106)}\\\\\n  C_3 &= \\frac{\\Gamma(4 + 2)}{\\Gamma(4)\\Gamma(2)}\\frac{\\Gamma(100)}{\\Gamma(12)\\Gamma(100-12)} \\frac{\\Gamma(4 + 12)\\Gamma(2 + 100 - 12)}{\\Gamma(4 + 2 + 100)}\\\\\n  &= \\frac{\\Gamma(6)}{\\Gamma(4)\\Gamma(2)}\\frac{\\Gamma(100)}{\\Gamma(12)\\Gamma(88)} \\frac{\\Gamma(16)\\Gamma(90)}{\\Gamma(106)}\n\\end{align}\\]\nLet\n\\[C^{*}_i = \\frac{\\Gamma(a_i +y)\\Gamma(b_i + n - y)}{\\Gamma(a_i)\\Gamma(b_i)} \\tag{7.1}\\]\n\\[\\begin{align}\nk_{j}^{(1)} &= \\frac{k_{j}^{(0)} C_j}{\\sum_{i=1}^{J} k_{i}^{(0)} C_i}\\\\\n&= \\frac{k_{j}^{0} C_{j}^{*}}{0.2\\times \\underbrace{\\frac{\\Gamma(15)\\Gamma(61)}{\\Gamma(3)\\Gamma(3)}}_{C^{*}_{1}} + 0.4 \\times \\underbrace{\\frac{\\Gamma(14)\\Gamma(92)}{\\Gamma(2)\\Gamma(4)}}_{C^{*}_{2}} + 0.4 \\times \\underbrace{\\frac{\\Gamma(16)\\Gamma(90)}{\\Gamma(4)\\Gamma(2)}}_{C^{*}_{3}}}\n\\end{align}\\]\nwhere \\(j\\in \\{1,2,3\\}\\).\n\ndef gammafrac(a,b,c,d,):\n    return ((gamma(a)*gamma(b))**-1) * gamma(c) * gamma(d)\n\nc1 = gammafrac(3,3,15,61)\nc2 = gammafrac(2,4,14,92)\nc3 = gammafrac(4,2,16,90)\nC = c1 + c2 + c3\n\npd.DataFrame({\"Variables\": [\"C1*\", \"C2*\", \"C3*\", \"$k^{1}_{1}$\", \"$k^{1}_{2}$\", \"$k^{1}_{3}$\"], \"Values\": [c1,c2,c3, c1/C, c2/C, c3/C]})\n\n\n\n\n\n  \n    \n      \n      Variables\n      Values\n    \n  \n  \n    \n      0\n      C1*\n      1.813524e+92\n    \n    \n      1\n      C2*\n      1.403157e+149\n    \n    \n      2\n      C3*\n      3.597838e+147\n    \n    \n      3\n      $k^{1}_{1}$\n      1.260148e-57\n    \n    \n      4\n      $k^{1}_{2}$\n      9.750000e-01\n    \n    \n      5\n      $k^{1}_{3}$\n      2.500000e-02\n    \n  \n\n\n\n\n\\[\\begin{align}\n  p(\\theta|y) &= \\sum_{i=1}^{3} k_{i}^{(1)}p_{i}(\\theta|y)\\\\\n              &= k_{1}^{(1)}p_{1}(\\theta|y) + k_{2}^{(1)}p_{2}(\\theta|y) + k_{3}^{(1)}p_{3}(\\theta|y)\\\\\n              &= 1.260148\\times 10^{-57} \\times  Beta(\\theta, 15,91) \\\\\n              &+ 9.750000\\times 10^{-01} \\times Beta(\\theta, 14, 92) \\\\\n              &+ 2.500000e\\times 10^{-02} \\times Beta(\\theta, 16, 90)\n\\end{align}\\]\n\n\n7.3.4 (d)\n\nRepeat (b) and (c) for a different coin, but possibly using a prior for \\(\\theta\\) that includes some information from the first coin. Your choice of a new prior may be informal, but needs to be justified. How the results from the first experiment influence your prior for the \\(\\theta\\) of the second coin may depend on whether or not the two coins have the same denomination, have a similar year, etc. Report the year and denomination of this coin.\n\n\n# pick another coin\ncoin2 = PseudoCoin(random_state=202210)\n\n# parameters setting\nn2 = 100 # number of flips\n\n# Experiment\nrs2 = coin2.flips(n2)\n\n# Results\nprint(rs2)\n\n[1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0\n 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0\n 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1]\n\n\n\n\n\n\n\n\nTable 7.2:  Satistics of the flipping coin experiment \n  \n    \n      \n      Properties\n      Values\n    \n  \n  \n    \n      0\n      N\n      100\n    \n    \n      1\n      Number of heads (y=1)\n      51\n    \n    \n      2\n      Number of tails\n      49\n    \n  \n\n\n\n\n\n\\[\\begin{align}\np^{1}(\\theta) &= 1.260148\\times 10^{-57} \\times  Beta(\\theta, 15,91) \\\\\n              &+ 9.750000\\times 10^{-01} \\times Beta(\\theta, 14, 92) \\\\\n              &+ 2.500000e\\times 10^{-02} \\times Beta(\\theta, 16, 90)\n\\end{align}\\]\nApply Equation 7.1,\n\\[\\begin{align}\n  C^{*}_{1} &= \\frac{\\Gamma(15 + 51)\\Gamma(91 + 100 - 51)}{\\Gamma(15)\\Gamma(91)}\\\\\n            &= \\frac{\\Gamma(66)\\Gamma(140)}{\\Gamma(15)\\Gamma(91)}\\\\\n  C^{*}_{2} &= \\frac{\\Gamma(14 + 51)\\Gamma(92 + 100 - 51)}{\\Gamma(14)\\Gamma(92)}\\\\\n            &= \\frac{\\Gamma(65)\\Gamma(141)}{\\Gamma(14)\\Gamma(92)}\\\\\n  C^{*}_{3} &= \\frac{\\Gamma(16 + 51)\\Gamma(90 + 100 - 51)}{\\Gamma(16)\\Gamma(90)}\\\\\n            &= \\frac{\\Gamma(67)\\Gamma(139)}{\\Gamma(16)\\Gamma(90)}\\\\\n\\end{align}\\]\n\n\n\n\n\n\n  \n    \n      \n      Variables\n      Values\n    \n  \n  \n    \n      0\n      C1*\n      6.123054e+180\n    \n    \n      1\n      C2*\n      2.028941e+180\n    \n    \n      2\n      C3*\n      1.744410e+181\n    \n    \n      3\n      $k^{1}_{1}$\n      2.392183e-01\n    \n    \n      4\n      $k^{1}_{2}$\n      7.926761e-02\n    \n    \n      5\n      $k^{1}_{3}$\n      6.815141e-01\n    \n  \n\n\n\n\n\\[\\begin{align}\n  p^{2}(\\theta|y_2) &= \\sum_{i=1}^{3} k_{i}^{(2)} p^{(2)}_{i}(\\theta|y_2)\\\\\n                    &= k_{1}^{(2)} p^{(2)}_{1}(\\theta|y_2) + k_{2}^{(2)} p^{(2)}_{2}(\\theta|y_2) + k_{3}^{(2)} p^{(2)}_{3}(\\theta|y_2)\\\\\n                    &=k_{1}^{(2)} Beta(15+51, 91+49)\\\\\n                    &+ k_{2}^{(2)} Beta(14+51,92+49)\\\\\n                    &+ k_{3}^{(2)} Beta(16+51, 90 + 49)\\\\\n                    &= 2.32\\times 10^{-1} \\times Beta(66,140)\\\\\n                    &+ 7.93\\times 10^{-2} \\times Beta(65,141)\\\\\n                    &+ 6.82\\times 10^{-1} \\times Beta(67,139)\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "hw/hw3.html#p-3-9",
    "href": "hw/hw3.html#p-3-9",
    "title": "7  Homework 3",
    "section": "7.4 Problem 3.9",
    "text": "7.4 Problem 3.9\n\nGalenshore distribution: An unknown quantity \\(Y\\) has a Galenshore(\\(\\alpha\\), \\(\\theta\\)) distribution if its density is given by\n\\[p(y) = \\frac{2}{\\Gamma(a)}\\theta^{2a}y^{2a-1}e^{-\\theta^2 y^2}\\] for \\(y>0\\), \\(\\theta>0\\) and \\(a>0\\). Assume for now that \\(a\\) is known. For this density, \\[E[Y]=\\frac{\\Gamma(a+\\frac{1}{2})}{\\theta\\Gamma(a)}, \\quad E[Y^2]=\\frac{a}{\\theta^2}\\]\n\n\n7.4.1 (a)\n\nIdentify a class of conjugate prior densities for \\(\\theta\\). Plot a few members of this class of densities.\n\nIdentifying Galenshore distribution belongs to exponential family\n\\[\\begin{align}\n  p(y|\\theta) &= \\frac{2}{\\Gamma(a)}y^{2a-1}\\theta^{2a}e^{-\\theta^2 y^2}\\\\\n              &= \\left(\\frac{2}{\\Gamma(a)}y^{2a-1}\\right) \\left(\\theta^{2}\\right)^a\\left(e^{-\\theta^2 y^2}\\right)\\\\\n\\end{align}\\]\n\n\\(\\phi(\\theta) = \\theta^2\\)\n\\(h(y) = \\frac{2}{\\Gamma(a)}y^{2a-1}\\)\n\\(c(\\phi) = \\phi^a\\)\n\\(t(y) = -y^2\\)\n\n\\[p(y|\\phi) = \\underbrace{(\\frac{2}{\\Gamma(a)}y^{2a-1})}_{=h(y)}\\underbrace{(\\phi^a)}_{=c(\\phi)} \\exp(\\phi\\cdot\\underbrace{(-1)\\cdot y^2}_{=t(y)})\\]\nDerive the posterior distribution\n\\[\\begin{align}\n  p(\\phi|n_0, t_0) &= \\kappa(n_0, t_0)c(\\phi)^{n_0}\\exp(n_0 t_0 \\phi)\\\\\n                   &= \\kappa(n_0, t_0)\\phi^{a n_0}\\exp(n_0 t_0 \\phi)\\\\\n  p(\\theta^2|n_0, t_0) &= \\kappa(n_0, t_0)\\theta^{2 a n_0}e^{(n_0 t_0 \\theta^2)}\\\\\n\\end{align}\\]\nApply change of variables\nLet \\(f(\\phi) = \\sqrt{\\theta} = \\theta\\) (\\(f\\) is a monotonous function), and \\(\\phi(\\theta) = \\theta^2\\)\n\\[\\begin{align}\np_{\\phi}(\\phi) &= p_{\\theta}(f(\\phi)) \\times |\\frac{df}{d\\phi}|\\\\\n\\kappa(n_0, t_0)c(\\phi)^{n_0}\\exp(n_0 t_0 \\phi) &= p_{\\theta}(\\theta) \\times (\\frac{1}{2\\sqrt{\\theta}})\\\\\n\\kappa(n_0, t_0)c(\\theta^2)^{n_0}\\exp(n_0 t_0 \\theta^2) &= p_{\\theta}(\\theta) \\times (\\frac{1}{2\\sqrt{\\theta}})\\\\\np_{\\theta}(\\theta) &= \\kappa(n_0,t_0)\\theta^{2a}e^{(n_0 t_0 \\theta^2)}\n\\end{align}\\]\n\\[\\begin{align}\n  p_{\\theta}(\\theta) &= p_{\\phi}(\\phi(\\theta)) \\times |\\frac{d\\phi(\\theta)}{d\\theta}|\\\\\n                     &\\propto \\kappa(n_0, t_0)c(\\theta^2)^{n_0}\\exp(n_0 t_0 \\theta^2) \\times 2\\theta\\\\\n                     &\\propto  \\theta^{2a n_0 + 1}e^{(n_0 t_0 \\theta^2)}\\\\\n\\end{align}\\]\nTo avoid alias, let Galenshore pdf function be\n\\[p_{X\\sim Galenshore}(x;a,z) = \\frac{2}{\\Gamma(a)}z^{2a}x^{2a-1}e^{-z^2x^2} \\tag{7.2}\\]\nCombining Equation 7.2 together,\n\\[\\begin{align}\n  p_{\\theta}(\\theta | n_0, t_0) &\\propto dGalenshore(\\theta; an_0 +1, \\sqrt{-n_0t_0})\\\\\n\\end{align}\\]\n\\(\\because t_0=-y^2\\) \\(\\therefore -n_0t_0=n_0y^2\\geq 0\\).\n\n\n7.4.2 (b)\n\nLet \\(Y_1, \\dots, Y_n \\sim~i.i.d.\\) Galenshore(\\(a\\),\\(\\theta\\)). Find the posterior distribution of \\(\\theta\\) given \\(Y_1, \\dots, Y_n\\), using a prior from your conjugate class.\n\nUse the formula described in Hoff (2009, vol. 580, sec. 3.3).\n\n\\(n^{(1)} = n_0 + n\\)\n\\(t^{(1)} = n_0t_0 + n\\bar{t}(y)\\)\n\n\\[\\begin{align}\np(\\phi|Y) &\\propto Galenshore(an'+1, \\sqrt{-n't'})\\\\\n          &\\propto p(\\phi|n_0+n, n_0t_0 + n\\bar{t}(y))\\\\\np(\\theta|Y) &\\propto dGalenshore(\\theta; a(n_0 + n)+1, \\sqrt{(n_0+n)(n_0t_0+n\\bar{t}(y))})\n\\end{align}\\]\nwhere \\(\\bar{t}(y) = \\frac{\\sum t(y_i)}{n}\\)\n\n\n7.4.3 (c)\n\nWrite down \\(\\frac{p(\\theta_a | Y_1, \\dots, Y_n)}{p(\\theta_b | Y_1, \\dots, Y_n)}\\) and simplify. Identify a sufficient statistics.\n\nBecause \\(t(y) = -y^2\\) is the sufficient statistic of the exponential family, the sufficient statistics for \\(Y_1,\\cdots, Y_n\\) is\n\\[\\bar{t}(y) = \\frac{\\sum t(y_i)}{n} = \\frac{\\sum y_{i}^{2}}{n}\\]\n\n\n7.4.4 (d)\n\nDetermine \\(E[\\theta|y_1,\\dots,y_n]\\).\n\nUse the posterior distribution derived in (b).\n\\[\\begin{align}\n  p(\\theta|Y) &\\propto dGalenshore(\\theta; \\underbrace{a(n_0 + n)+1}_{a^{(1)}}, \\underbrace{\\sqrt{(n_0+n)(n_0t_0+n\\bar{t}(y))}}_{\\theta^{(1)}})\\\\\n  E[\\theta|Y] &= \\frac{\\Gamma(a(n_0 + n)+\\frac{3}{2})}{\\sqrt{(n_0+n)(n_0t_0+n\\bar{t}(y))}\\Gamma(a(n_0 + n)+1)}\n\\end{align}\\]\n\n\n7.4.5 (e)\n\nDetermine the form of the posterior predictive density \\(p(\\tilde{y}|y_1,\\dots, y_n)\\).\n\n\\[\\begin{align}\n  p(\\tilde{y} | Y) &= \\int_{\\theta} p(\\tilde{y} | \\theta)p(\\theta | Y) d\\theta\\\\\n\\end{align}\\]\n\n\\(p(\\tilde{y}|\\theta) = \\frac{2}{\\Gamma(a)}\\theta^{2a} \\tilde{y}^{2a -1} e^{-\\theta^2 \\tilde{y}^2}\\)\n\\(p(\\theta | Y) \\propto dGalenshore(\\theta; \\underbrace{a(n_0 + n)+1}_{a_{(1)}}, \\underbrace{\\sqrt{(n_0+n)(n_0t_0+n\\bar{t}(y))}}_{b_{(1)}})\\)\n\n\\(p(\\theta|Y) = \\frac{2}{\\Gamma(a_{(1)})}b_{(1)}^{2a_{(1)}}\\theta^{2a_{(1)} - 1}e^{-b_{(1)}^{2}\\theta^2}\\)\n\n\n\\[\\begin{align}\n  p(\\tilde{y}|Y)\n  &= \\int_{\\theta} \\frac{2}{\\Gamma(a)}\\theta^{2a} \\tilde{y}^{2a -1} e^{-\\theta^2 \\tilde{y}^2} \\times \\frac{2}{\\Gamma(a_{(1)})}b_{(1)}^{2a_{(1)}}\\theta^{2a_{(1)} - 1}e^{-b_{(1)}^{2}\\theta^2} d\\theta\\\\\n  &= \\frac{4\\tilde{y}^{2a-1}b^{2a_{(1)}}_{(1)}}{\\Gamma(a)\\Gamma(a_{(1)})}\\int_{\\theta} \\theta^{2a + 2a_{(1)} - 1}e^{-\\theta^2 \\tilde{y^2} - b_{(1)}^{2}\\theta^2} d\\theta\\\\\n  &= \\frac{4\\tilde{y}^{2a-1}b^{2a_{(1)}}_{(1)}}{\\Gamma(a)\\Gamma(a_{(1)})}\\int_{\\theta} \\theta^{2(a + a_{(1)}) - 1}e^{-(\\tilde{y^2} - b_{(1)}^{2})\\theta^2} d\\theta\\\\\n  &= \\tilde{y}^{2a_{(1)}-1}\\frac{2\\Gamma(an+1)}{\\Gamma(a_{(1)})\\Gamma(a)}\\frac{b_{(1)}^{2an}}{(b_{(1)} +\\tilde{y}^2)^{2(an+1)}}\n\\end{align}\\]"
  },
  {
    "objectID": "hw/hw3.html#problem-3.14",
    "href": "hw/hw3.html#problem-3.14",
    "title": "7  Homework 3",
    "section": "7.5 Problem 3.14",
    "text": "7.5 Problem 3.14\n\nUnit information prior: Let \\(Y_1,\\dots, Y_n \\sim~i.i.d. p(y|\\theta)\\). Having observed the values \\(Y_1 = y_1, \\dots, Y_n = y_n\\), the log likelihood is given by \\(l(\\theta|y)=\\sum\\log p(y_i|\\theta)\\), and the value \\(\\hat{\\theta}\\) of \\(\\theta\\) that maximize \\(l(\\theta|y)\\) is called the maximum likelihood estimator. The negative of the curvature of the log-likelihood, \\(J(\\theta)=-\\frac{\\partial^2 l}{\\partial \\theta^2}\\), describes the precision of the MLE \\(\\hat{\\theta}\\) and is called the observed Fisher information. For situations in which it is difficult to quantify prior information in terms of a probability distribution, some have suggested that the “prior” distribution be based on the likelihood, for example, by centering the prior distribution around the MLE \\(\\hat{\\theta}\\). To deal with the fact that the MLE is not really prior information, the curvature of the prior is chosen so that it has only “one \\(n\\)th” as much information as the likelihood, so that \\(-\\frac{\\partial^2 \\log p(\\theta)}{\\partial\\theta^2} = \\frac{J(\\theta)}{n}\\). Such a prior is called a unit information prior (Kass and Wasserman, 1995; Kass and Raftery, 1995), as it has as much information as the average amount of information from a single observation. The unit information prior is not really a prior distribution, as it is computed from the observed data. However, it can be roughly viewed as the prior information of someone with weak but accurate prior information.\n\n\n7.5.1 (a)\n\nLet \\(Y_1,\\dots,Y_n\\sim i.i.d.\\) binary (\\(\\theta\\)). Obtain the MLE \\(\\hat{\\theta}\\) and \\(\\frac{J(\\hat{\\theta})}{n}\\).\n\nThe Bernoullis distribution can be expressed as2\n\\[p(y_i|\\theta) = \\theta^{y_i}(1-\\theta)^{1-y_i}\\quad \\text{ for } y_i \\in\\{0,1\\}\\]\nBecause \\(Y_i,\\dots,Y_n\\sim i.i.d.\\), \\(k_1=\\dots=k_n=k\\).\n\\[\\begin{align}\n  l(\\theta|y) &= \\sum_{i=1}^{n} \\log p(y_i|\\theta)\\\\\n              &= \\sum_{i=1}^{n} \\log (\\theta^{y_i} (1-\\theta)^{1-y_i})\\\\\n              &= \\sum_{i=1}^{n} \\left( y_i\\log \\theta + (1-y_i)\\log(1-\\theta) \\right)\\\\\n              &= \\log\\theta \\sum_{i=1}^{n} y_i + \\log(1-\\theta)(n - \\sum_{i=1}^{n} y_i)\n\\end{align}\\]\nThus, \\(\\bar{y} = \\sum_{i=1}^{n} y_i\\) is the sufficient statistics.\n\\[l(\\theta|y) = \\bar{y}\\log\\theta + (n-\\bar{y})\\log(1-\\theta)\\]\n\\[\\begin{align}\n  \\frac{\\partial l(\\theta|y)}{\\partial\\theta} &= \\frac{\\bar{y}}{\\theta} - \\frac{n-\\bar{y}}{1-\\theta}\\\\\n  \\frac{\\partial^2 l(\\theta|y)}{\\partial^2 \\theta} &= \\frac{-\\bar{y}}{\\theta^2} - \\frac{\\overbrace{n-\\bar{y}}^{\\geq 0}}{(1-\\theta)^2} \\leq 0\n\\end{align}\\]\nThus, the curvature is concave because the second partial derivative is negative. Next, find the maximum \\(\\hat{\\theta}\\).\n\\[\\begin{align}\n  0 &= \\frac{\\partial l(\\hat{\\theta} | y)}{\\partial\\theta}= \\frac{\\bar{y}}{\\hat{\\theta}} - \\frac{n-\\bar{y}}{1-\\hat{\\theta}}\\\\\n  \\hat{\\theta} &= \\frac{\\bar{y}}{n}_{\\#}\n\\end{align}\\]\n\\[\\begin{align}\n  \\frac{J(\\hat{\\theta})}{n} &= -\\frac{\\partial^2 l}{\\partial\\theta^2}\\frac{1}{n}\\\\\n                            &= \\left(\\frac{\\bar{y}}{\\hat{\\theta}^2}+\\frac{n-\\bar{y}}{(1-\\hat{\\theta})^2} \\right)\\frac{1}{n}\\\\\n                            &= \\frac{\\hat{\\theta}}{\\hat{\\theta}^2} + \\frac{1-\\hat{\\theta}}{(1-\\hat{\\theta}^2)}\\\\\n                            &= \\frac{1}{\\hat{\\theta}} + \\frac{1-\\hat{\\theta}}{(1-\\hat{\\theta}^2)}\\\\\n\\end{align}\\]\n\n\n7.5.2 (b)\n\nFind a probability density \\(p_{U}(\\theta)\\) such that \\(\\log p_{U}(\\theta) = \\frac{l(\\theta|y)}{n} + c\\), where \\(c\\) is a constant that does not depend on \\(\\theta\\). Compute the information \\(-\\frac{\\partial^2 \\log p_U(\\theta)}{\\partial\\theta^2}\\) of this density.\n\nPart I: Derive \\(p_{U}(\\theta)\\)\n\\[\\begin{align}\n  p_{U}(\\theta) &= e^{\\frac{l(\\theta|y)}{n}}e^c\\\\\n  \\int_{0}^{1}p_{U}(\\theta)d\\theta &= 1 = e^c  \\int_{0}^{1} e^{\\frac{l(\\theta|y)}{n}} d\\theta\\\\\n  1 &= e^c \\int_{0}^{1} \\exp(\\hat{\\theta}\\log\\theta + (1-\\hat{\\theta})\\log(1-\\theta))) d\\theta\\\\\n  1 &= e^c \\int_{0}^{1} \\theta^{\\hat{\\theta}}(1-\\theta)^{1-\\hat{\\theta}} d\\theta\\\\\n  1 &= e^c \\frac{-\\pi}{2}(\\hat{\\theta}-1)\\hat{\\theta}\\csc(\\pi\\hat{\\theta})\\\\\n  e^c &= \\frac{-2}{\\pi(\\hat{\\theta}-1)\\hat{\\theta}\\csc(\\pi\\hat{\\theta})}\\\\\n  c &= \\log\\left( \\frac{-2}{\\pi(\\hat{\\theta}-1)\\hat{\\theta}\\csc(\\pi\\hat{\\theta})} \\right)\n\\end{align}\\]\nTherefore, we get\n\\[\\log p_{U}(\\theta) = \\frac{l(\\theta|y)}{n} + \\log\\left( \\frac{-2}{\\pi(\\hat{\\theta}-1)\\hat{\\theta}\\csc(\\pi\\hat{\\theta})} \\right)\\]\nPart II: Fisher inforamtion\n\\[\\begin{align}\n  -\\frac{\\partial^2 \\log p_{U}(\\theta)}{\\partial \\theta^2} &= \\frac{-1}{n}\\frac{\\partial^2 l(\\theta|y)}{\\partial \\theta^2}\\\\\n  &= \\frac{\\hat{\\theta}}{\\theta} + \\frac{1-\\hat{\\theta}}{1-\\theta^2}_{\\#}\n\\end{align}\\]\n\n\n7.5.3 (c)\n\nObtain a probability density for \\(\\theta\\) that is proportional to \\(p_{U}(\\theta) \\times p(y_1,\\dots, y_n |\\theta)\\). Can this be considered a posterior distribution for \\(\\theta\\)?\n\n\\[\\begin{align}\n  p_U(\\theta) \\times p(y_1,\\dots,y_n|\\theta)\\\\\n  &= \\frac{-2}{\\pi(\\hat{\\theta}-1)\\hat{\\theta}\\csc(\\pi\\hat{\\theta})}e^{\\frac{l(\\theta|y)}{n}}\\times  \\prod_{i=1}^{n}p(y_i|\\theta)\\\\\n  &= \\frac{-2}{\\pi(\\hat{\\theta}-1)\\hat{\\theta}\\csc(\\pi\\hat{\\theta})}e^{\\frac{l(\\theta|y)}{n}}\\times  \\prod_{i=1}^{n} \\theta^{y_i}(1-\\theta)^{1-y_i}\\\\\n  &= \\frac{-2}{\\pi(\\hat{\\theta}-1)\\hat{\\theta}\\csc(\\pi\\hat{\\theta})}\\frac{\\theta^{\\hat{\\theta}}}{(1-\\theta)^{\\hat{\\theta}}} \\times \\theta^{n\\hat{\\theta}}(1-\\theta)^{n(1-\\hat{\\theta})}\\\\\n  &= \\frac{-2}{\\pi(\\hat{\\theta}-1)\\hat{\\theta}\\csc(\\pi\\hat{\\theta})}\\frac{\\theta^{\\hat{\\theta}(n+1)}}{(1-\\theta)^{\\hat{\\theta}-n(1-\\hat{\\theta})}}\n\\end{align}\\]\n\nYes, the resulting is the unit information prior.\n\n\n\n7.5.4 (d)\n\nRepeat (a), (b) and (c) but with \\(p(y|\\theta)\\) being the Poisson distribution.\n\nFrom Hoff (2009, vol. 580, sec. 3.2), The PDF of poisson distribution is\n\\[p(Y=y|\\theta) = dpois(y,\\theta) = \\theta^y\\frac{e^{-\\theta}}{\\Gamma(y)} \\quad \\text{ for } y\\in\\{0,1,2,...\\}\\]\nPart I: MLE \\(\\hat{\\theta}\\)\n\\[\\begin{align}\n  l(\\theta|y) &= \\sum^{n}_{i=1} \\log p(y_i|\\theta)\\\\\n              &= \\sum^{n}_{i=1} \\log\\left( \\theta^y\\frac{e^{-\\theta}}{\\Gamma(y)} \\right)\\\\\n              &= \\log \\frac{\\theta^{\\sum y}e^{-n\\theta}}{\\sum \\Gamma(y)}\\\\\n              &= \\log \\left( \\theta^{\\sum y}e^{-n\\theta} \\right) - \\log(\\sum\\Gamma(y))\\\\\n              &= \\log\\left(\\frac{\\theta^{\\sum y}e^{-n\\theta}}{\\sum\\Gamma(y)} \\right)\n\\end{align}\\]\nGet the MLE \\(\\hat{\\theta}\\),\n\\[\\begin{align}\n  \\frac{\\partial l}{\\partial \\theta} &= \\frac{\\sum y}{\\theta} - n\\\\\n  \\hat{\\theta} = \\frac{\\sum_{i=1}^{n} y}{n}\n\\end{align}\\]\nPart II: Find Unit information prior\n\\[\\begin{align}\n  \\frac{J(\\hat{\\theta})}{n} &= -\\frac{\\partial^2 l}{\\partial\\theta^2}\\frac{1}{n}\\\\\n                      &= \\frac{\\sum_{i=1}^{n} y_i}{\\theta^2}\\frac{1}{n} = \\frac{1}{\\hat{\\theta}}\n\\end{align}\\]\nPart III: Derive \\(P_{U}\\)\n\\[\\begin{align}\n  P_{U}(\\theta) &= e^c e^{\\frac{l(\\theta|y)}{n}}\\\\\n                &= e^c \\left(\\frac{\\theta^{\\sum y}e^{-n\\theta}}{\\sum\\Gamma(y)}\\right)^{\\frac{1}{n}}\n\\end{align}\\]\n\\[\\begin{align}\n  \\int_{0}^{\\infty} P_{U}(\\theta) d\\theta = 1 &= e^c \\int_{0}^{\\infty}  \\left(\\frac{\\theta^{\\sum y}e^{-n\\theta}}{\\sum\\Gamma(y)}\\right)^{\\frac{1}{n}} d\\theta\\\\\n  &= \\frac{e^c}{(\\sum\\Gamma(y))^{\\frac{1}{n}}} \\int_{0}^{\\infty} \\theta^{\\hat{\\theta}}e^{-\\theta} d\\theta\n\\end{align}\\]\nUse the fact that \\(\\int_{0}^{\\infty} x^a e^{-x}dx = -\\Gamma(a+1)\\).\n\\[\\begin{align}\n  1 &= \\frac{e^c}{(\\sum\\Gamma(y))^{\\frac{1}{n}}} \\Gamma(\\hat{\\theta}+1)\\\\\n  c &= \\log\\left(\\frac{(\\sum\\Gamma(y))^{\\frac{1}{n}}}{\\Gamma(\\hat{\\theta}+1)}\\right)\n\\end{align}\\]\nTherefore,\n\\[\\begin{align}\n  P_U(\\theta) &= e^c e^{\\frac{l(\\theta|y)}{n}}\\\\\n              &= e^c \\left[\\frac{\\theta^{\\sum y}e^{-n\\theta}}{\\sum \\Gamma(y)}\\right]^{\\frac{1}{n}}\\\\\n              &= \\frac{(\\sum\\Gamma(y))^{\\frac{1}{n}}}{\\Gamma(\\hat{\\theta}+1)} \\left[\\frac{\\theta^{\\sum y}e^{-n\\theta}}{\\sum \\Gamma(y)}\\right]^{\\frac{1}{n}}\n\\end{align}\\]\nPart IV: Fisher information of \\(P_{U}\\)\n\\[\\begin{align}\n  \\log p_{U}(\\theta) &= \\log \\left(\\frac{(\\sum\\Gamma(y))^{\\frac{1}{n}}}{\\Gamma(\\hat{\\theta}+1)}\\right) + \\frac{1}{n} \\log \\left[\\frac{\\theta^{\\sum y}e^{-n\\theta}}{\\sum \\Gamma(y)}\\right]\\\\\n  &= \\log \\left(\\frac{(\\sum\\Gamma(y))^{\\frac{1}{n}}}{\\Gamma(\\hat{\\theta}+1)}\\right) + \\frac{1}{n} \\log \\left[\\theta^{\\sum y}e^{-n\\theta} - \\frac{1}{n} \\log (\\sum\\Gamma(y))\\right]\n\\end{align}\\]\nUse the fact that \\(\\frac{\\partial^2}{\\partial x^2}\\left[ \\frac{\\log(x^ae^{-nx})}{n}\\right] = \\frac{-a}{nx^2}\\).3\n\\[\\begin{align}\n  -\\frac{\\partial^2 \\log P_U(\\theta)}{\\partial \\theta^2} = \\frac{\\sum y}{n\\theta^2}_{\\#}\n\\end{align}\\]\nPart V: Obtain the posterior distribution\n\\[\\begin{align}\n  p_U(\\theta) \\times p(y_1,\\dots, y_n|\\theta)%\n  &= \\frac{(\\sum\\Gamma(y))^{\\frac{1}{n}}}{\\Gamma(\\hat{\\theta}+1)}e^{\\frac{l(\\theta|y)}{n}} \\times \\prod_{i=1}^{n} p(y_i|\\theta)\\\\\n  &= \\frac{(\\sum\\Gamma(y))^{\\frac{1}{n}}}{\\Gamma(\\hat{\\theta}+1)}e^{\\frac{l(\\theta|y)}{n}} \\times \\frac{\\theta^{\\sum y}e^{-n\\theta}}{\\sum\\Gamma(y)}\\\\\n  &= \\frac{(\\sum\\Gamma(y))^{\\frac{1}{n}}}{\\Gamma(\\hat{\\theta}+1)} \\left(\\frac{\\theta^{n\\hat{\\theta}}e^{-n\\theta}}{\\sum\\Gamma(y)}\\right)^{\\frac{1}{n}+1}\n\\end{align}\\]\n\n\n\n\n\n\nDiaconis, Persi, and Donald Ylvisaker. 1985. “Quantifying Prior Opinion, Bayesian Statistics. Vol. 2.” North Holland Amsterdam:\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical Methods. Vol. 580. Springer."
  },
  {
    "objectID": "hw/hw4.html#description",
    "href": "hw/hw4.html#description",
    "title": "8  Homework 4",
    "section": "8.1 Description",
    "text": "8.1 Description\n\nCourse: STAT638, 2022 Fall\n\n\nRead Chapter 4 in Hoff.\nThen, do the following exercises in Hoff: 4.1, 4.2, 4.6, 4.8\nAll datasets in the Hoff book can be downloaded from https://pdhoff.github.io/book/ (Links to an external site.).\n\n\nDeadline: Oct 4 by 12:01pm"
  },
  {
    "objectID": "hw/hw4.html#computational-enviromnent-setup",
    "href": "hw/hw4.html#computational-enviromnent-setup",
    "title": "8  Homework 4",
    "section": "8.2 Computational Enviromnent Setup",
    "text": "8.2 Computational Enviromnent Setup\n\n8.2.1 Third-party libraries\n\n%matplotlib inline\nimport sys # system information\nimport matplotlib # plotting\nimport scipy # scientific computing\nimport random \nimport pandas as pd # data managing\nfrom scipy.special import comb\nfrom scipy import stats as st\nfrom scipy.special import gamma\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Matplotlib setting\nplt.rcParams['text.usetex'] = True\nmatplotlib.rcParams['figure.dpi']= 300\nnp.random.seed(20220928) # Consistent random effect\n\n\n\n8.2.2 Version\n\nprint(sys.version)\nprint(matplotlib.__version__)\nprint(scipy.__version__)\nprint(np.__version__)\nprint(pd.__version__)\n\n3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]\n3.6.2\n1.9.3\n1.23.4\n1.5.1"
  },
  {
    "objectID": "hw/hw4.html#problem-4.1",
    "href": "hw/hw4.html#problem-4.1",
    "title": "8  Homework 4",
    "section": "8.3 Problem 4.1",
    "text": "8.3 Problem 4.1\n\nPosterior comparisons: Reconsider the sample survey in Exercise 3.1. Suppose you are interested in comparing the rate of support in that county to the rate in another county. Suppose that a survey of sample size \\(50\\) was done in the second county, and the total number of people in the sample who supported the policy was \\(30\\). Identify the posterior distribution of \\(\\theta_2\\) assuming a uniform prior. Sample \\(5000\\) values of each of \\(\\theta_1\\) and \\(\\theta_2\\) from their posterior distributions and estimate \\(Pr(\\theta_1 < \\theta_2|\\text{ the data and prior })\\).\n\n\nPrior:\n\n\\(\\theta \\sim beta(1,1)\\)\n\nModel:\n\n\\(p(\\sum Y =n | \\theta) = {N \\choose n}\\theta^{n}(1-\\theta)^{N-n}\\)\n\nPosterior distribution (Hoff 2009, 580:37):\n\n\\(\\theta | \\sum_{i=1}^{N} Y_i=n \\sim Beta(beta(1+n, 1+N-n))\\)\n\n\\(\\theta_1 \\sim Beta(1+57, 1+100-57) = Beta(58, 44)\\)\n\\(\\theta_2 \\sim Beta(1+30, 1+50-30) = Beta(31, 21)\\)\n\n\n\n\nN = 5000\nt1s = st.beta.rvs(58, 44, size=N)\nt2s = st.beta.rvs(31, 21, size=N)\np_t2bigger = np.mean(t1s < t2s) \n\n# Display\npd.DataFrame({\"Item\": [\"Pr(theta_1 < theta_2 | the data and prior)\"],\\\n              \"Value\":[p_t2bigger]})\n\n\n\n\n\n  \n    \n      \n      Item\n      Value\n    \n  \n  \n    \n      0\n      Pr(theta_1 < theta_2 | the data and prior)\n      0.6292"
  },
  {
    "objectID": "hw/hw4.html#problem-4.2",
    "href": "hw/hw4.html#problem-4.2",
    "title": "8  Homework 4",
    "section": "8.4 Problem 4.2",
    "text": "8.4 Problem 4.2\n\nTumor count comparisons: Reconsider the tumor count data in Exercise 3.3:\n\n\n8.4.1 (a)\n\nFor the prior distribution given in part (a) of that exercise, obtain \\(Pr(\\theta_B < \\theta_A | y_A, y_B)\\) via Monte Carlo sampling.\n\n\nPrior distribution\n\n\\(\\theta_A|y_A \\sim gamma(120+117, 10+10) = gamma(237, 20)\\)\n\\(\\theta_B|y_B \\sim gamma(12+113, 1+13) = gamma(125, 14)\\)\n\n\n\nN = 5000\ntheta_A = st.gamma.rvs(237, scale=1/20, size=N)\ntheta_B = st.gamma.rvs(125, scale=1/14, size=N)\nres = np.mean(theta_B < theta_A)\n\n# Display\npd.DataFrame({\"Item\": [\"Pr(theta_B < theta_A | y_A, y_B)\"],\\\n              \"Value\":[res]})\n\n\n\n\n\n  \n    \n      \n      Item\n      Value\n    \n  \n  \n    \n      0\n      Pr(theta_B < theta_A | y_A, y_B)\n      0.995\n    \n  \n\n\n\n\n\n\n8.4.2 (b)\n\nFor a range of values of \\(n_0\\), obtain \\(Pr(\\theta_B < \\theta_A |y_A, y_B)\\) for \\(\\theta_A \\sim gamma(120,10)\\) and \\(\\theta_B \\sim gamma(12\\times n_0, n_0)\\). Describe how sensitive the conclusions about the event \\(\\{\\theta_B < \\theta_A\\}\\) are to the prior distribution on \\(\\theta_B\\).\n\nIn Figure 8.1, the \\(n_0\\) decreases the probability of \\(p(\\{\\theta_B < \\theta_A\\})\\) with linear effect. From small to large \\(n_0\\), the prior distribution of \\(\\theta_B\\) keep influences the result.\n\ndef get_A_bigger(n0, N=5000):\n  theta_A = st.gamma.rvs(120 + 117, scale=1/(10+10), size=N)\n  theta_B = st.gamma.rvs(12*n0+113, scale=1/(n0+13), size=N)\n  res = np.mean(theta_B < theta_A)\n  return res\n\nn0s = np.arange(1,50, 1)\nress = [get_A_bigger(n0) for n0 in n0s]\n\n# Plotting\nfig, ax = plt.subplots()\nax.plot(n0s, ress, \"o\", color=\"k\")\nax.set_xlabel(\"$n_0$\")\nax.set_ylabel(\"$p(\\\\theta_B < \\\\theta_A | y_A, y_B, n_0)$\");\n\n\n\n\nFigure 8.1: The effect of n0\n\n\n\n\n\n\n8.4.3 (c)\n\nRepeat part (a) and (b), replacing the event \\(\\{\\theta_B < \\theta_A\\}\\) with the event \\(\\{\\tilde{Y}_{B} < \\tilde{Y}_{A}\\}\\), where \\(\\tilde{Y}_A\\) and \\(\\tilde{Y}_B\\) are samples from the posterior predictive distribution.\n\nPart I (a)\n\n\\(\\tilde{Y}_A \\sim nbinom(a+\\sum Y^{(A)}, b + n) = nbinom(120+117,10+10)= nbinom(237, 20)\\)\n\\(\\tilde{Y}_B \\sim nbinom(a+\\sum Y^{(B)}, b + n) = nbinom(12+113, 1+13) = nbinom(125, 14)\\)\nFind \\(p(\\tilde{Y}_{B} < \\tilde{Y}_A | y_A, y_B)\\)\nUse scipy.stats.nbinom1\n\n\\(p = 1 - \\frac{1}{b+n+1}\\)\n\\(n = a+\\sum Y\\)\n\\(k = \\tilde{Y}\\)\n\n\n\nN = 5000\ntilde_y_A = st.nbinom.rvs(120+117, 1 - 1/(10+10+1), size=N)\ntilde_y_B = st.nbinom.rvs(12+113, 1 - 1/(1+13+1), size=N)\np2 = np.mean(tilde_y_B < tilde_y_A)\n\n\n# Display\n# Display\npd.DataFrame({\"Item\": [\"Pr(tildeY_B < tildeY_A | y_A, y_B)\"],\\\n              \"Value\":[p2]})\n\n\n\n\n\n  \n    \n      \n      Item\n      Value\n    \n  \n  \n    \n      0\n      Pr(tildeY_B < tildeY_A | y_A, y_B)\n      0.6934\n    \n  \n\n\n\n\nPart II (b)\nIn Figure 8.2, the \\(n_0\\) has nonlinear negative effect on the probability \\(p(\\{\\tilde{Y}_{B} < \\tilde{Y}_{A}\\})\\) with the prior formation of \\(n_0\\).\n\ndef get_A_bigger(n0, N=5000):\n  tilde_y_A = st.nbinom.rvs(120+117, 1 - 1/(10+10+1), size=N)\n  tilde_y_B = st.nbinom.rvs(12*n0+113, 1 - 1/(n0+13+1), size=N)\n  res = np.mean(tilde_y_B < tilde_y_A)\n  return res\n\nn0s = np.arange(1,50, 1)\nress = [get_A_bigger(n0) for n0 in n0s]\n\n# Plotting\nfig, ax = plt.subplots()\nax.plot(n0s, ress, \"o\", color=\"k\")\nax.set_xlabel(\"$n_0$\")\nax.set_ylabel(\"$p(\\\\tilde{Y}_B < \\\\tilde{Y}_A | y_A, y_B, n_0)$\");\n\n\n\n\nFigure 8.2: The effect of n0"
  },
  {
    "objectID": "hw/hw4.html#problem-4.6",
    "href": "hw/hw4.html#problem-4.6",
    "title": "8  Homework 4",
    "section": "8.5 Problem 4.6",
    "text": "8.5 Problem 4.6\n\nNon-informative prior distributions: Suppose for a binary sampling problem we plan on using a uniform, or \\(beta(1,1)\\), prior for the population proportion \\(\\theta\\). Perhaps our reasoning is that this represents “no prior information about \\(\\theta\\).” However, some people like to look at proportions on the log-odds scale, that is, they are interested in \\(\\gamma = \\log\\frac{\\theta}{1−\\theta}\\). Via Monte Carlo sampling or otherwise, find the prior distribution for \\(\\gamma\\) that is induced by the uniform prior for \\(\\theta\\). Is the prior informative about \\(\\gamma\\)?\n\nPart I: Analytical Approach\n\n\\(\\gamma = g(\\theta) = \\log\\frac{\\theta}{1-\\theta}\\)\n\\(\\theta = g^{-1}(\\gamma) = \\frac{e^{\\gamma}}{1+e^{\\gamma}}\\)\n\n\\[\\begin{align}\np_{\\gamma}(\\gamma)\n&= \\underbrace{p_{\\theta}(g^{-1}(\\gamma))}_{=1 \\because \\theta\\sim uniform(0,1)} \\times \\left|\\frac{dg^{-1}(\\gamma)}{d\\gamma}\\right|\\\\\n&= \\left|\\frac{e^{\\gamma}}{(1+e^{\\gamma})^2}\\right|\n\\end{align}\\]\nPart II: Monte Carlo Approach\n\ndef true_gamma_pdf(y):\n    return np.absolute(np.exp(y)/(1+np.exp(y))**2)\n\nths = st.beta.rvs(1,1,size=1000)\ngammas = np.linspace(-10,10,100)\n\nys_true = [true_gamma_pdf(g) for g in gammas]\nys = [np.log(th/(1-th)) for th in ths]\n\nfig, ax = plt.subplots();\nax.hist(ys, weights=np.ones_like(ys)/len(ys));\nax.plot(gammas, ys_true, label=\"Analytical solution\");\nax.set_xlabel(\"$\\\\gamma=\\\\log\\\\frac{\\\\theta}{1-\\\\theta}$\")\nax.set_ylabel(\"$p(\\\\gamma)$\")\nax.legend();\n\n\n\n\n\nThe prior of \\(\\gamma\\) is informative since it is centered around the \\(0\\)."
  },
  {
    "objectID": "hw/hw4.html#sec-p48",
    "href": "hw/hw4.html#sec-p48",
    "title": "8  Homework 4",
    "section": "8.6 Problem 4.8",
    "text": "8.6 Problem 4.8\n\nMore posterior predictive checks: Let \\(\\theta_A\\) and \\(\\theta_B\\) be the average number of children of men in their 30s with and without bachelor’s degrees, respectively.\n\n\n8.6.1 (a)\n\nUsing a Poisson sampling model, a \\(gamma(2,1)\\) prior for each \\(\\theta\\) and the data in the files menchild30bach.dat and menchild30nobach.dat, obtain \\(5000\\) samples of \\(\\bar{Y}_{A}\\) and \\(\\bar{Y}_{B}\\) from the posterior predictive distribution of the two samples. Plot the Monte Carlo approximations to these two posterior predictive distributions. (data available in Appendix)\n\n\n# Data\ndataA = np.loadtxt(\"data/menchild30bach.dat\")\ndataB = np.loadtxt(\"data/menchild30nobach.dat\")\n\n# Display \npd.DataFrame({\n  \"Properties\": [\"Sum\", \"N (number of samples)\"],\n  \"A\": [np.sum(dataA), len(dataA)],\n  \"B\": [np.sum(dataB), len(dataB)]\n})\n\n\n\n\n\n  \n    \n      \n      Properties\n      A\n      B\n    \n  \n  \n    \n      0\n      Sum\n      54.0\n      305.0\n    \n    \n      1\n      N (number of samples)\n      58.0\n      218.0\n    \n  \n\n\n\n\nThe predictive distribution is\n\n\\(\\bar{Y_A} | y_A \\sim nbinom(2+54,1+58)\\)\n\\(\\bar{Y_B} | y_B \\sim nbinom(2+305, 1+218)\\)\n\n\nN = 5000\nbar_ya = st.nbinom.rvs(2+ np.sum(dataA), 1 - 1/(1+len(dataA)+1), size=N)\nbar_yb = st.nbinom.rvs(2+ np.sum(dataB), 1 - 1/(1+len(dataB)+1), size=N)\n\n# Display\nfig, ax = plt.subplots();\n[ax.hist(ys, weights=np.ones_like(ys)/len(ys), label=n, alpha=0.7, bins=np.arange(0,8,1)) for n,ys in zip([\"A\",\"B\"],[bar_ya, bar_yb])]\nax.set_xlabel(\"$N$\")\nax.set_ylabel(\"Probability\")\nax.legend();\n\n\n\n\n\n\n8.6.2 (b)\n\nFind \\(95\\%\\) quantile-based posterior confidence intervals for \\(\\theta_B - \\theta_A\\) and \\(\\tilde{Y}_B-\\tilde{Y}_A\\). Describe in words the differences between the two populations using these quantities and the plots in (a), along with any other results that may of interest to you.\n\n\n\\(\\theta_A | y_A \\sim gamma(2+54, 1+58)\\)\n\\(\\theta_B | y_B \\sim gamma(2+58, 1+218)\\)\n\\(\\bar{Y_A} | y_A \\sim nbinom(2+54,1+58)\\)\n\\(\\bar{Y_B} | y_B \\sim nbinom(2+305, 1+218)\\)\nThe \\(95\\%\\) confidence interval does not contain the negative region. Thus, the belief of \\(\\theta_B > \\theta_A\\) is confident.\nHowever, there is more uncertainty about the posterior predictive distribution, leading to the uncertain quantity comparison between \\(Y_A\\) and \\(Y_B\\).\n\n\nN = 5000\nthetaAs = st.gamma.rvs(2+np.sum(dataA), scale=1/(1+len(dataA)), size=N)\nthetaBs = st.gamma.rvs(2+np.sum(dataB), scale=1/(1+len(dataB)), size=N)\nYAs = st.nbinom.rvs(2+np.sum(dataA), 1 - 1/(1+len(dataA)+1), size=N)\nYBs = st.nbinom.rvs(2+np.sum(dataB), 1 - 1/(1+len(dataB)+1), size=N)\n\ntheta_diff = thetaBs - thetaAs\nY_diff = YBs - YAs\n\ntheta_quan = st.mstats.mquantiles(theta_diff, prob=[0.025, 0.975])\nY_quan = st.mstats.mquantiles(Y_diff, prob=[0.025, 0.975])\n\n# Display\npd.DataFrame({\"RVs\":[\"Interval (thetaB-thetaA)\", \"Interval (YB-YA)\"],\\\n              \"Value (2.5%; 97.5%)\":[theta_quan, Y_quan]})\n\n\n\n\n\n  \n    \n      \n      RVs\n      Value (2.5%; 97.5%)\n    \n  \n  \n    \n      0\n      Interval (thetaB-thetaA)\n      [0.14120795305853456, 0.7436235075695707]\n    \n    \n      1\n      Interval (YB-YA)\n      [-3.0, 4.0]\n    \n  \n\n\n\n\n\nplt.hist(theta_diff, weights=np.ones_like(theta_diff)/len(theta_diff), alpha=0.7)\nplt.title(\"Distribution of ThetaB - ThetaA\")\nplt.show();\n\n\n\n\n\nplt.hist(Y_diff, weights=np.ones_like(Y_diff)/len(Y_diff), alpha=0.7)\nplt.title(\"Distribution of YB - YA\")\nplt.show();\n\n\n\n\n\n\n8.6.3 (c)\n\nObtain the empirical distribution of the data in group \\(B\\). Compare this to the Poisson distribution with mean \\(\\hat{\\theta}=1.4\\). Do you think the Poisson model is a good fit? Why or Why not?\n\n\n\\(\\bar{Y}_B | y_B \\sim nbinom(307, 219)\\)\nThe poisson model is not a good fit to the group \\(B\\). As shown in the historgram, there are more than one peak in the distribution, that is not the characteristic of Poisson distribution.\n\n\nxs = np.arange(0,7,1)\nps = [st.poisson.pmf(x, 1.4) for x in xs]\n\nfig, ax = plt.subplots()\nax.hist(dataB, weights=np.ones_like(dataB)/len(dataB), alpha=0.7);\nax.plot(xs, ps, \"o\", label=\"Poisson($\\\\hat{\\\\theta}$=1.4)\")\nax.legend();\n\n\n\n\n\n\n8.6.4 (d)\n\nFor each of the \\(5000\\) \\(\\theta_B\\)-valeus you sampled, sample \\(n_B=218\\) Poisson random variables and count the number of \\(0\\)s and the number of \\(1\\)s in each of the \\(5000\\) simulated datasets. You should now have tow sequences of length \\(5000\\) each, one sequence counting the number of people having zero children for each of the \\(5000\\) posterior predictive datasets, the other counting the number of people with one child. Plot the two sequences against one another (one on the \\(x\\)-axis, one on the \\(y\\)-axis). Add to the plot a point marking how many people in the observed dataset had zero children and one child. Using this plot, describe the adequency of the Poisson model.\n\n\nthetaBs = thetaBs\nnB = 218\nybs0 = np.zeros(len(thetaBs))\nybs1 = np.zeros(len(thetaBs))\n\nfor (i, th) in enumerate(thetaBs):\n  seq = st.poisson.rvs(th, size=nB)\n  ybs0[i] = len(seq[seq==0])\n  ybs1[i] = len(seq[seq==1])\n\nfig, ax = plt.subplots()\nax.plot(ybs0, ybs1, \"o\", alpha=0.5)\nax.plot(len(dataB[dataB==0]), len(dataB[dataB==1]), 'o',color=\"r\")\nax.set_xlabel(\"N of people with zero children\")\nax.set_ylabel(\"N of people with one children\")\n\nText(0, 0.5, 'N of people with one children')\n\n\n\n\n\n\nThe observed data is in red, but the simulated data is centralized far from that point. Thus, the poisson model is not appropriate for this dataset."
  },
  {
    "objectID": "hw/hw4.html#appendix",
    "href": "hw/hw4.html#appendix",
    "title": "8  Homework 4",
    "section": "8.7 Appendix",
    "text": "8.7 Appendix\n\n8.7.1 Data set in Problem 4.8\n\nprint(\"menchild30bach.dat:\\n\", dataA)\nprint(\"menchild30nobach.dat:\\n\", dataB)\n\nmenchild30bach.dat:\n [1. 0. 0. 1. 2. 2. 1. 5. 2. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 2.\n 1. 3. 2. 0. 0. 3. 0. 0. 0. 2. 1. 0. 2. 1. 0. 0. 1. 3. 0. 1. 1. 0. 2. 0.\n 0. 2. 2. 1. 3. 0. 0. 0. 1. 1.]\nmenchild30nobach.dat:\n [2. 2. 1. 1. 2. 2. 1. 2. 1. 0. 2. 1. 1. 2. 0. 2. 2. 0. 2. 1. 0. 0. 3. 6.\n 1. 6. 4. 0. 3. 2. 0. 1. 0. 0. 0. 3. 0. 0. 0. 0. 0. 1. 0. 4. 2. 1. 0. 0.\n 1. 0. 3. 2. 5. 0. 1. 1. 2. 1. 2. 1. 2. 0. 0. 0. 2. 1. 0. 2. 0. 2. 4. 1.\n 1. 1. 2. 0. 1. 1. 1. 1. 0. 2. 3. 2. 0. 2. 1. 3. 1. 3. 2. 2. 3. 2. 0. 0.\n 0. 1. 0. 0. 0. 1. 2. 0. 3. 3. 0. 1. 2. 2. 2. 0. 6. 0. 0. 0. 2. 0. 1. 1.\n 1. 3. 3. 2. 1. 1. 0. 1. 0. 0. 2. 0. 2. 0. 1. 0. 2. 0. 0. 2. 2. 4. 1. 2.\n 3. 2. 0. 0. 0. 1. 0. 0. 1. 5. 2. 1. 3. 2. 0. 2. 1. 1. 3. 0. 5. 0. 0. 2.\n 4. 3. 4. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 2. 0. 0. 1. 1. 0. 2. 1. 3. 3. 2.\n 2. 0. 0. 2. 3. 2. 4. 3. 3. 4. 0. 3. 0. 1. 0. 1. 2. 3. 4. 1. 2. 6. 2. 1.\n 2. 2.]\n\n\n\n\n\n\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical Methods. Vol. 580. Springer."
  },
  {
    "objectID": "hw/hw5.html#description",
    "href": "hw/hw5.html#description",
    "title": "9  Homework 5",
    "section": "9.1 Description",
    "text": "9.1 Description\n\nCourse: STAT638, 2022 Fall\n\n\nRead Chapter 5 in the Hoff book.\nThen, do the following exercises in Hoff: 5.1, 5.2, 5.5.\nFor 5.2, _A denotes the mean exam score for students assigned to method A, and _B denotes the mean exam score for students assigned to method B.\nBe careful: Some of the notation differs between the lecture slides and the Hoff book.\n\n\nDeadline: Oct 12 by 12:01pm"
  },
  {
    "objectID": "hw/hw5.html#computational-enviromnent-setup",
    "href": "hw/hw5.html#computational-enviromnent-setup",
    "title": "9  Homework 5",
    "section": "9.2 Computational Enviromnent Setup",
    "text": "9.2 Computational Enviromnent Setup\n\n9.2.1 Third-party libraries\n\n%matplotlib inline\nimport sys # system information\nimport matplotlib # plotting\nimport scipy # scientific computing\nimport random \nimport pandas as pd # data managing\nfrom scipy.special import comb\nfrom scipy import stats as st\nfrom scipy.special import gamma\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import permutations \n# Matplotlib setting\nplt.rcParams['text.usetex'] = True\nmatplotlib.rcParams['figure.dpi']= 300\nnp.random.seed(20220928) # Consistent random effect\n\n\n\n9.2.2 Version\n\nprint(sys.version)\nprint(matplotlib.__version__)\nprint(scipy.__version__)\nprint(np.__version__)\nprint(pd.__version__)\n\n3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]\n3.6.2\n1.9.3\n1.23.4\n1.5.1"
  },
  {
    "objectID": "hw/hw5.html#problem-5.1",
    "href": "hw/hw5.html#problem-5.1",
    "title": "9  Homework 5",
    "section": "9.3 Problem 5.1",
    "text": "9.3 Problem 5.1\n\nStudying: The files school1.dat, school2.dat and school3.dat contina data on the amount of time students from \\(3\\) high schools spent on studying or homework during an exam period. Analyze data from each of these schools separately, using the normal model with a conjugate prior distribution, in which \\(\\{\\mu_0=5, \\sigma^{2}_{0}=4, \\kappa_0 = 1, \\nu_0 = 2\\}\\) and compute or approximate following:\n\n\nschs = [np.loadtxt(\"data/school{}.dat\".format(i)) for i in range(1,4)]\n\n\n9.3.1 (a)\n\nPosterior means and \\(95\\%\\) confidence intervals for the mean \\(\\theta\\) and standard deviation \\(\\sigma\\) from each school;\n\n\nclass NormalModel:\n  def __init__(self, data, mu0, var0, k0, v0):\n    # Prior\n    self.mu0 = mu0 # prior mean\n    self.var0= var0 # prior variance\n    self.k0= k0 # prior observation\n    self.v0 = v0 \n    # Data\n    self.data = data\n    self.mean = np.mean(data) # sample mean\n    self.n = len(data) # sample counts \n    self.kn = self.n + self.k0 \n    self.s2 = np.sum( (self.data- self.mean)**2 ) / (self.n - 1)\n\n    # Posterior parameters\n    self.vn = self.get_vn(self.v0, self.n) # Posterior v\n    self.varn = self.get_varn(self.vn, self.v0, self.var0, self.k0, self.n, self.kn, self.mean, self.mu0) # Posterior variance\n    self.mun = self.get_mun(self.k0, self.mu0, self.n, self.mean, self.kn) # Posterior mean\n\n\n  def get_vn(self, v0, n):\n    return v0 + n\n\n  def get_varn(self, vn, v0, var0, k0, n, kn, mean, mu0):\n    varn = (1/vn)*(v0*var0 + (n-1)*self.s2 + (k0*n/kn)*(mean - mu0)**2)\n    return varn\n\n  def get_mun(self, k0, mu0, n, mean, kn):\n    return (k0*mu0 + n*mean)/kn\n\n  def rv_theta_post(self):\n    mu0 = self.mu0; k0 = self.k0; \n    return st.norm(loc=self.mun, scale= np.sqrt(self.varn/self.kn))\n\n  def rv_pred(self):\n    mun = self.mun\n    varn = self.varn\n    return st.norm(loc=self.mun, scale= np.sqrt(self.varn))\n\nsetting = {\"mu0\": 5, \"var0\":4, \"k0\":1, \"v0\":2}\nnms = [NormalModel(sch, **setting) for sch in schs]\nrv_thps = [nm.rv_theta_post() for nm in nms]\n\n# Display data\npd.DataFrame({\"School\":[i+1 for i in range(0, len(nms))],\"Mean\":[rv_thp.mean() for rv_thp in rv_thps],\"95% confidence interval (left, right)\":[rv_thp.interval(0.95) for rv_thp in rv_thps]})\n\n\n\n\n\n  \n    \n      \n      School\n      Mean\n      95% confidence interval (left, right)\n    \n  \n  \n    \n      0\n      1\n      9.292308\n      (7.832422909482735, 10.752192475132652)\n    \n    \n      1\n      2\n      6.948750\n      (5.242990325564609, 8.654509674435392)\n    \n    \n      2\n      3\n      7.812381\n      (6.26475553664974, 9.360006368112167)\n    \n  \n\n\n\n\n\n\n9.3.2 (b)\n\nThe posterior probability that \\(\\theta_i < \\theta_j < \\theta_k\\) for all size permutataions \\(\\{i,j,k\\}\\) of \\(\\{1,2,3\\}\\);\n\n\nn_mc = 10000\nsamps_thp = [rv_thp.rvs(size=n_mc) for rv_thp in rv_thps]\nperm = list(permutations([0,1,2]))\ntext = [\"\"] * len(perm)\nprobs = [int] * len(perm)\nfor (a, i) in enumerate(perm):\n  text[a] = \"theta_{} < theta_{} < theta_{}\".format(i[0]+1, i[1]+1, i[2]+1)\n  probs[a] = np.sum(np.logical_and(samps_thp[i[0]] < samps_thp[i[1]], samps_thp[i[1]] < samps_thp[i[2]])) / n_mc\n\npd.DataFrame({\"Permuatations\": text, \"Probabilities\": probs})\n\n\n\n\n\n  \n    \n      \n      Permuatations\n      Probabilities\n    \n  \n  \n    \n      0\n      theta_1 < theta_2 < theta_3\n      0.0058\n    \n    \n      1\n      theta_1 < theta_3 < theta_2\n      0.0030\n    \n    \n      2\n      theta_2 < theta_1 < theta_3\n      0.0788\n    \n    \n      3\n      theta_2 < theta_3 < theta_1\n      0.6929\n    \n    \n      4\n      theta_3 < theta_1 < theta_2\n      0.0123\n    \n    \n      5\n      theta_3 < theta_2 < theta_1\n      0.2072\n    \n  \n\n\n\n\n\n\n9.3.3 (c)\n\nThe posterior probability that \\(\\tilde{Y}_i<\\tilde{Y}_j < \\tilde{Y}_k\\) for all siz permuatations \\(\\{i,j,k\\}\\) of \\(\\{1,2,3\\}\\), where \\(\\tilde{Y}_i\\) is a sample from the posterior predictive distribution of school \\(i\\).\n\n\nrv_posts = [nm.rv_pred() for nm in nms]\n\nsamps_post = [rv_post.rvs(size=n_mc) for rv_post in rv_posts]\nperm = list(permutations([0,1,2]))\ntext = [\"\"] * len(perm)\nprobs = [int] * len(perm)\nfor (a, i) in enumerate(perm):\n  text[a] = \"Y_tilde_{} < Y_tilde_{} < Y_tilde_{}\".format(i[0]+1, i[1]+1, i[2]+1)\n  probs[a] = np.sum(np.logical_and(samps_post[i[0]] < samps_post[i[1]], samps_post[i[1]] < samps_post[i[2]])) / n_mc\n\npd.DataFrame({\"Permuatations\": text, \"Probabilities\": probs})\n\n\n\n\n\n  \n    \n      \n      Permuatations\n      Probabilities\n    \n  \n  \n    \n      0\n      Y_tilde_1 < Y_tilde_2 < Y_tilde_3\n      0.1020\n    \n    \n      1\n      Y_tilde_1 < Y_tilde_3 < Y_tilde_2\n      0.1029\n    \n    \n      2\n      Y_tilde_2 < Y_tilde_1 < Y_tilde_3\n      0.1857\n    \n    \n      3\n      Y_tilde_2 < Y_tilde_3 < Y_tilde_1\n      0.2686\n    \n    \n      4\n      Y_tilde_3 < Y_tilde_1 < Y_tilde_2\n      0.1361\n    \n    \n      5\n      Y_tilde_3 < Y_tilde_2 < Y_tilde_1\n      0.2047\n    \n  \n\n\n\n\n\n\n9.3.4 (d)\n\nCompute the posterior probability that \\(\\theta_1\\) is bigger than both \\(\\theta_2\\) and \\(\\theta_3\\), and the posterior probability that \\(\\tilde{Y}_1\\) is bigger than both \\(\\tilde{Y}_2\\) and \\(\\tilde{Y}_3\\).\n\n\nth_biggest = np.sum(np.logical_and(samps_thp[0]>samps_thp[1], samps_thp[0]>samps_thp[2]))/n_mc\npost_biggest = np.sum(np.logical_and(samps_post[0]>samps_post[1], samps_post[0]>samps_post[2]))/n_mc\n\npd.DataFrame({\"Properties\": [\"Theta_1 is the biggest\", \"Tilde_1 is the biggest\"], \"Probabilities\":[th_biggest, post_biggest]})\n\n\n\n\n\n  \n    \n      \n      Properties\n      Probabilities\n    \n  \n  \n    \n      0\n      Theta_1 is the biggest\n      0.9001\n    \n    \n      1\n      Tilde_1 is the biggest\n      0.4733"
  },
  {
    "objectID": "hw/hw5.html#problem-5.2",
    "href": "hw/hw5.html#problem-5.2",
    "title": "9  Homework 5",
    "section": "9.4 Problem 5.2",
    "text": "9.4 Problem 5.2\n\nSensitivity analysis: \\(32\\) students in a science classroom were randomly assigned to one of two study methods, \\(A\\) and \\(B\\), so that \\(n_A = n_B = 16\\) students were assigned to each method. After several weeks of study, students were examined on the course material with an exam designed to give an average score of \\(75\\) with a standard deviation of \\(10\\). The scores for the two groups are summarized by \\(\\{\\bar{y}_A=75.2, s_A = 7.3\\}\\) and \\(\\{\\bar{y}_B=77.5, s_b = 8.1\\}\\). Consider independent, conjugate normal prior distributions for each of \\(\\theta_A\\) and \\(\\theta_B\\), with \\(\\mu_0=75\\) and \\(\\sigma_{0}^2 = 100\\) for both groups. For each \\((\\kappa_0, \\nu_0) \\in \\{(1,1),(2,2), (4,4),(8,8), (16,16), (32,32)\\}\\) (or more values), obtain \\(Pr(\\theta_A < \\theta_B | y_A, y_B)\\) via Monte Carlo Sampling. Plot this probability as a function of \\(\\kappa_0 = \\nu_0\\). Describe how you might use this plot to convey the evidence that \\(\\theta_A < \\theta_B\\) to people of a variety of prior opinions.\n\n\nIncrease \\((\\kappa_0, \\nu_0)\\) can descrease the probability of \\(B\\) bigger that than \\(A\\). But, the probability of \\(B\\) larger than \\(A\\) can hardly lower than \\(0.5\\).\n\n\nclass NormalModel:\n  def __init__(self, sample_size,sample_mean, sample_var, mu0, var0, k0, v0):\n    # Prior\n    self.mu0 = mu0 # prior mean\n    self.var0= var0 # prior variance\n    self.k0= k0 # prior observation\n    self.v0 = v0 \n    # Data\n    self.mean = sample_mean # sample mean\n    self.n = sample_size # sample counts \n    self.kn = self.n + self.k0 \n    self.s2 = sample_var\n\n    # Posterior parameters\n    self.vn = self.get_vn(self.v0, self.n) # Posterior v\n    self.varn = self.get_varn(self.vn, self.v0, self.var0, self.k0, self.n, self.kn, self.mean, self.mu0) # Posterior variance\n    self.mun = self.get_mun(self.k0, self.mu0, self.n, self.mean, self.kn) # Posterior mean\n\n\n  def get_vn(self, v0, n):\n    return v0 + n\n\n  def get_varn(self, vn, v0, var0, k0, n, kn, mean, mu0):\n    varn = (1/vn)*(v0*var0 + (n-1)*self.s2 + (k0*n/kn)*(mean - mu0)**2)\n    return varn\n\n  def get_mun(self, k0, mu0, n, mean, kn):\n    return (k0*mu0 + n*mean)/kn\n\n  def rv_theta_post(self):\n    mu0 = self.mu0; k0 = self.k0; \n    return st.norm(loc=self.mun, scale= np.sqrt(self.varn/self.kn))\n\n  def rv_pred(self):\n    mun = self.mun\n    varn = self.varn\n    return st.norm(loc=self.mun, scale= np.sqrt(self.varn))\n\ndef expAB(k0,v0,size=1000):\n  setting = {\"sample_size\":16, \"mu0\": 75, \"var0\": 100, \"k0\":k0, \"v0\":v0}\n  rvtA = NormalModel(sample_mean=75.2,\\\n    sample_var=7.3, **setting).rv_theta_post()\n  rvtB = NormalModel(sample_mean=77.5,\\\n    sample_var=8.1, **setting).rv_theta_post()\n\n  spA, spB = rvtA.rvs(size=size), rvtB.rvs(size=size)\n  return np.sum(spA < spB)/size\n\npars = [i**2 for i in range(0, 7)]\nvals = [float]*len(pars)\n\nfor (i, p) in enumerate(pars):\n  vals[i] = expAB(p,p)\n\n# Plotting\nfigV, axV = plt.subplots()\naxV.plot(pars,vals, 'o', color=\"k\")\naxV.set_xlabel(\"$(\\\\kappa_0, \\\\nu_0)$\")\naxV.set_ylabel(\"$Pr(\\\\theta_A < \\\\theta_B | y_A, y_B)$\");"
  },
  {
    "objectID": "hw/hw5.html#problem-5.5",
    "href": "hw/hw5.html#problem-5.5",
    "title": "9  Homework 5",
    "section": "9.5 Problem 5.5",
    "text": "9.5 Problem 5.5\n\nUnit information prior: Obtain a unit information prior for the normal model as follows:\n\n\n9.5.1 (a)\n\nReparameterize the normal model as \\(p(y|\\theta, \\psi)\\), where \\(\\psi=\\frac{1}{\\sigma^2}\\). Write out the log likelihood \\(l(\\theta, \\psi|y) = \\sum \\log p(y_i|\\theta, \\psi)\\) in terms of \\(\\theta\\) and \\(\\psi\\).\n\n\\[p(y|\\theta,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}(\\frac{y-\\theta}{\\sigma})^2}\\]\n\\[p(y|\\theta, \\psi) = \\frac{\\sqrt{\\psi}}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y-\\theta)^2\\psi}\\]\n\\[\\log p(y_i|\\theta,\\psi) = \\log\\left(\\sqrt{\\frac{\\psi}{2\\pi}}\\right) - \\frac{1}{2}(y_i -\\theta)^2\\psi\\]\n\\[\\begin{align}\nl(\\theta,\\psi|y) = \\sum_{i=1}^{n} \\log(y_i|\\theta, \\psi)%\n&= \\log\\left(\\frac{\\psi}{2\\pi}\\right)^{n/2} - \\frac{1}{2}\\psi \\sum_{i=1}^{n}(y_i -\\theta)^2\\\\\n&= \\frac{n}{2} \\log\\left( \\frac{\\psi}{2\\pi} \\right) - \\frac{\\psi}{2}\\sum^{n}_{i=1}(y_i - \\theta)^2\\\\\n&= \\frac{n}{2} \\log\\left( \\frac{\\psi}{2\\pi} \\right) - \\frac{\\psi}{2}\\sum^{n}_{i=1}(y_{i}^{2} - 2\\theta y_{i} + \\theta^2)\\\\\n&= \\frac{n}{2} \\log\\left( \\frac{\\psi}{2\\pi} \\right) - \\frac{\\psi}{2}(\\sum y_{i}^{2} - 2\\theta\\sum y_{i} + n\\theta^2)\\\\\n&= \\frac{-\\psi}{2} \\sum_{i=1}^{n} y_{i}^{2} + \\psi\\theta\\sum_{i=1}^{n} y_i - \\frac{n\\psi\\theta^2}{2} + \\frac{n}{2} \\log\\left( \\frac{\\psi}{2\\pi} \\right)\n\\end{align}\\]\n\n\n9.5.2 (b)\n\nFind a probability density \\(P_U(\\theta, \\psi)\\) so that \\(\\log p_U(\\theta, \\psi) = \\frac{l(\\theta, \\psi |y)}{n} + c\\), where \\(c\\) is a constant that does not depend on \\(\\theta\\) or \\(\\psi\\).\nHint: Write \\(\\sum (y_i-\\theta)^2\\) as \\(\\sum(y_i - \\bar{y} + \\bar{y} - \\theta)^2 = \\sum(y_i-\\bar{y})^2 + n(\\theta - \\bar{y})^2\\), and recall that \\(\\log p_U(\\theta, \\psi) = \\log p_U(\\theta|\\psi) + \\log p_U(\\psi)\\).\n\n\\[\\begin{align}\nl(\\theta, \\psi|y)%\n&=\\frac{n}{2} \\log\\left( \\frac{\\psi}{2\\pi} \\right) - \\frac{\\psi}{2}\\sum^{n}_{i=1}(y_i - \\theta)^2\\\\\n&=\\frac{n}{2}\\log\\left(\\frac{\\psi}{2\\pi}\\right) - \\frac{\\psi}{2}\\left[\\sum(y_i - \\bar{y})^2 + n(\\theta-\\bar{y})^2\\right]\\\\\n&=\\frac{n}{2}\\log\\left(\\frac{\\psi}{2\\pi}\\right) - \\frac{\\psi}{2}\\left[(n-1) s^2 + n(\\theta-\\bar{y})^2\\right]\\\\\n&= \\frac{n}{2}\\log\\left(\\frac{\\psi}{2\\pi}\\right) - \\frac{\\psi}{2}\\left((n-1)s^2 + n\\theta^2 - 2n\\theta\\bar{y} + n\\bar{y}^2\\right)\\\\\n&= \\frac{n}{2}\\log\\left(\\frac{\\psi}{2\\pi}\\right) - \\frac{\\psi}{2}(n-1)s^2 - \\frac{\\psi n \\theta^2}{2} + n\\psi\\theta\\bar{y} - \\frac{\\psi n \\bar{y}^2}{2}\\\\\n&= \\frac{n}{2}\\log\\left(\\frac{\\psi}{2\\pi}\\right) - \\frac{\\psi}{2}(n-1)s^2 - \\frac{\\psi n \\bar{y}^2}{2} + n\\psi\\theta\\bar{y} - \\frac{\\psi n \\theta^2}{2}\n\\end{align}\\]\n\\[\\begin{align}\n\\frac{l(\\theta, \\psi|y)}{n} + c &= \\underbrace{\\frac{1}{2}\\log\\left(\\frac{\\psi}{2\\pi}\\right) - \\frac{\\psi}{2n}(n-1)s^2 - \\frac{\\psi  \\bar{y}^2}{2}}_{=h(\\psi|y)} + \\underbrace{\\psi\\theta\\bar{y} - \\frac{\\psi\\theta^2}{2}}_{=g(\\theta|\\psi, y)} + c\n\\end{align}\\]\n\\[\\begin{align}\np_U(\\theta,\\psi)%\n&= \\exp\\left(\\frac{l(\\theta,\\psi|y)}{n}+ c\\right)\\\\\n&\\propto  \\exp\\left(\\frac{l(\\theta,\\psi|y)}{n}\\right)\\\\\n&= \\exp(h(\\psi|y)) \\exp(g(\\theta|\\psi,y))\n\\end{align}\\]\n\\[\\begin{align}\n\\exp(h(\\psi|y))%\n&= (\\frac{\\psi}{2\\pi})^{1/2}\\exp\\left(- (\\frac{(n-1)s^2}{2n} + \\frac{\\bar{y}^2}{2})\\psi\\right)\\\\\n&\\propto \\psi^{\\frac{1}{2}}\\exp\\left(- \\frac{\\psi}{(\\frac{(n-1)s^2}{2n} + \\frac{\\bar{y}^2}{2})^{-1}}\\right)\\\\\n&\\sim Gamma(\\psi, \\frac{3}{2}, (\\frac{(n-1)s^2}{2n} + \\frac{\\bar{y}^2}{2})^{-1})\n\\end{align}\\]\n\\[\\begin{align}\n\\exp(g(\\theta|\\psi, y))%\n&= \\exp\\left( \\psi\\theta\\bar{y} - \\frac{\\psi\\theta^2}{2}\\right)\\\\\n&= \\exp\\left( -\\frac{\\psi}{2}\\theta^2 + \\psi\\bar{y}\\theta - \\frac{\\psi\\bar{y}^2}{2} + \\frac{\\psi\\bar{y}^2}{2} \\right)\\\\\n&= \\exp\\left( -\\frac{\\psi}{2}(\\theta^2 - 2\\bar{y}\\theta + \\bar{y}^2) + \\frac{\\psi\\bar{y}^2}{2}\\right)\\\\\n&= \\exp\\left( -\\frac{\\psi}{2} (\\theta-\\bar{y})^2 + \\frac{\\psi\\bar{y}^2}{2}\\right)\\\\\n&\\propto \\exp\\left( -\\frac{\\psi}{2} (\\theta-\\bar{y})^2 \\right)\\\\\n&= \\exp\\left( - \\frac{1}{2}\\left(\\frac{\\theta-\\bar{y}}{\\psi^{-1/2}} \\right)^2\\right)\\\\\n&\\propto Normal(\\theta, \\bar{y}, \\psi^{-1/2})\n\\end{align}\\]\nThus1,\n\\[\\begin{align}\n  P_{U}(\\theta,\\psi) &= P_{U}(\\theta|\\psi)P_{U}(\\psi)\n  \\propto  Gamma(\\psi, \\frac{3}{2}, (\\frac{(n-1)s^2}{2n} + \\frac{\\bar{y}^2}{2})^{-1})\\\\\n  &\\times Normal(\\theta, \\bar{y}, \\psi^{-1/2})\\\\\n\\end{align}\\]\n\n\n9.5.3 (c)\n\nFind a probability density \\(p_U(\\theta, \\psi|y)\\) that is proportional to \\(p_U(\\theta, \\psi) \\times p(y_1, \\dots, y_n |\\theta, \\psi)\\). It may be convenient to write this joint density as \\(p_U(\\theta|\\psi, y)\\times p_{U}(\\psi|y)\\). Can this joint density be considered a posterior density?\n\n\\[\\begin{align}\nP_{U}(\\theta, \\psi|y)%\n&= P_{U}(\\psi|y) \\times p_{U}(\\theta|\\psi, y)\\\\\n&= Gamma(\\psi, \\frac{3}{2}, (\\frac{(n-1)s^2}{2n} + \\frac{\\bar{y}^2}{2})^{-1})\\\\\n  &\\times Normal(\\theta, \\bar{y}, \\psi^{-1/2})\\\\\n  &\\propto \\psi^{1/2}\\exp\\left( - (\\frac{(n-1)s^2}{2n} + \\frac{\\bar{y}^2}{2})\\psi \\right)\\times \\exp\\left(-\\frac{1}{2}\\left(\\frac{\\theta-\\bar{y}}{\\psi^{-1/2}}\\right)^2\\right)\\\\\n  &\\propto \\psi^{1/2}\\exp\\left( - (\\frac{(n-1)s^2}{2n} + \\frac{\\bar{y}^2}{2})\\psi \\right)\\times \\exp\\left(-\\frac{\\psi}{2}\\left(\\theta-\\bar{y}\\right)^2\\right)\\\\\n  &\\propto \\psi^{1/2}\\exp\\left( - (\\frac{(n-1)s^2}{2n} + \\frac{\\bar{y}^2}{2})\\psi \\right)\\times \\exp\\left(-\\frac{\\psi}{2}\\left(\\theta^2-2\\theta\\bar{y}+\\bar{y}^2\\right)\\right)\\\\\n  &\\psi^{1/2}\\exp\\left(-\\left( \\frac{(n-1)s^2}{2n} + \\frac{\\bar{y}^2}{2} + \\frac{\\bar{y}^2}{2} \\right)\\psi \\right) \\times \\exp\\left(-\\frac{\\psi}{2}(\\theta^2-2\\theta\\bar{y})\\right)\n\\end{align}\\]\n\n\\(p_U(\\theta|\\psi, y) \\propto Normal(\\theta, \\bar{y}, \\psi^{-1/2})\\)\n\\(p_U(\\psi|\\theta, y) \\sim Gamma(\\frac{3}{2}, (\\frac{(n-1)s^2}{2n} + \\frac{\\bar{y}^2}{2})^{-1})\\)\nYes, it can be considered as a posterior density because the unit information prior is the product of two proper distribution."
  },
  {
    "objectID": "hw/hw6.html#description",
    "href": "hw/hw6.html#description",
    "title": "10  Homework 6",
    "section": "10.1 Description",
    "text": "10.1 Description\n\nCourse: STAT638, 2022 Fall\n\n\nRead Hoff (2009, ch. 6). Then, do Hoff (2009, Exercise 6.1). You may assume that \\(\\theta\\) and are a priori independent, and that \\(Y_A\\) and \\(Y_B\\) are conditionally independent given \\(\\theta\\) and \\(\\gamma\\)."
  },
  {
    "objectID": "hw/hw6.html#computational-enviromnent-setup",
    "href": "hw/hw6.html#computational-enviromnent-setup",
    "title": "10  Homework 6",
    "section": "10.2 Computational Enviromnent Setup",
    "text": "10.2 Computational Enviromnent Setup\n\n10.2.1 Third-party libraries\n\n%matplotlib inline\nimport sys # system information\nimport matplotlib # plotting\nimport scipy # scientific computing\nimport random \nimport pandas as pd # data managing\nfrom scipy.special import comb\nfrom scipy import stats as st\nfrom scipy.special import gamma\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import permutations \n# Matplotlib setting\nplt.rcParams['text.usetex'] = True\nmatplotlib.rcParams['figure.dpi']= 300\nnp.random.seed(20220928) # Consistent random effect\n\n\n\n10.2.2 Version\n\nprint(sys.version)\nprint(matplotlib.__version__)\nprint(scipy.__version__)\nprint(np.__version__)\nprint(pd.__version__)\n\n3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]\n3.6.2\n1.9.3\n1.23.4\n1.5.1"
  },
  {
    "objectID": "hw/hw6.html#problem-6.1",
    "href": "hw/hw6.html#problem-6.1",
    "title": "10  Homework 6",
    "section": "10.3 Problem 6.1",
    "text": "10.3 Problem 6.1\n\nPoisson population comparisons: Let’s reconsider the number of children data of Exercise 4.8. We’ll assume Poisson sampling models for the two groups as before, but now we’ll parameterize \\(\\theta_A\\) and \\(\\theta_B\\) as \\(\\theta_A = \\theta, \\theta_B = \\theta\\times \\gamma\\). In the parameterization, \\(\\gamma\\) represents the relative rate \\(\\frac{\\theta_B}{\\theta_A}\\). Let \\(\\theta\\sim gamma(a_\\theta, b_\\theta)\\) and let \\(\\gamma\\sim gamma(a_\\gamma, b_\\gamma)\\).\n\n\n10.3.1 (a)\n\nAre \\(\\theta_A\\) and \\(\\theta_B\\) independent or dependent under this prior distribution? In what situations is such a joint prior distribution justified?\n\n\\[\\begin{align}\n    Cov(\\theta_A, \\theta_B)%\n    &= E[\\theta_A\\theta_B] - E[\\theta_A]E[\\theta_B]\\\\\n    &= E[\\theta \\theta\\gamma] - E[\\theta]E[\\theta\\gamma]\\\\\n    &= E[\\theta^2 \\gamma] - E[\\theta]E[\\theta\\gamma]\\\\\n    &= E[\\theta^2]E[\\gamma] - E[\\theta]^2 E[\\gamma]\\\\\n    &= E[\\gamma](E[\\theta^2] - E[\\theta]^2)\\\\\n    &= E[\\gamma]Var[\\theta]\\\\\n    &= \\frac{a_{\\gamma}}{b_{\\gamma}}\\frac{a_{\\theta}}{b_{\\theta}^2}\\\\\n    &\\neq 0\n\\end{align}\\]\nBecause \\(Cov(\\theta_A, \\theta_B) > 0\\), \\(\\theta_A\\) and \\(\\theta_B\\) are dependent.\n\n\n10.3.2 (b)\n\nObtain the form of the full conditional distribution of \\(\\theta\\) given \\(\\mathbb{y}_A\\), \\(\\mathbb{y}_B\\) and \\(\\gamma\\).\n\n\\[\\begin{align}\n    p(\\theta|y_A, y_B, \\gamma)%\n    &= \\frac{p(y_A, y_B | \\theta, \\gamma) p(\\theta|\\gamma)}{p(y_A, y_B |\\gamma)}\\\\\n    &= \\frac{p(y_A|\\theta,\\gamma)p(y_B|\\theta,\\gamma)p(\\theta|\\gamma)}{p(y_A|\\gamma)p(y_B|\\gamma)}\\\\\n    &= \\frac{p(y_A|\\theta)p(y_B|\\theta,\\gamma)p(\\theta)}{p(y_A)p(y_B|\\gamma)}\\\\\n    &\\propto p(y_A|\\theta) p(y_B |\\theta,\\gamma)p(\\theta)\\\\\n    &=  \\prod_{i=1}^{n_A} poisson(y_{Ai};\\theta)\\times \\prod_{i=1}^{n_B} poisson(y_{Bi};\\theta\\gamma) \\times gamma(\\theta; a_{\\theta}, b_{\\theta})\\\\\n    &= \\theta^{\\sum_{i=1}^{n_A}y_{Ai}}e^{-n_A\\theta} \\times (\\theta\\gamma)^{\\sum_{i=1}^{n_B}}e^{-n_B \\theta\\gamma} \\times  \\theta^{a_{\\theta}-1}e^{-b_{\\theta}\\theta}\\\\\n    &= \\theta^{n_A\\bar{y}_A}e^{-n_A \\theta}\\times (\\theta\\gamma)^{n_B\\bar{y}_B}e^{-n_B\\theta\\gamma}\\times \\theta^{a_{\\theta}-1}e^{-b_{\\theta}\\theta}\\\\\n    &= \\theta^{n_A\\bar{y}_A}e^{-n_A \\theta}\\times (\\theta)^{n_B\\bar{y}_B}e^{-n_B\\theta\\gamma}\\times \\theta^{a_{\\theta}-1}e^{-b_{\\theta}\\theta}\\\\\n    &\\propto \\theta^{n_A\\bar{y}_A + n_B\\bar{y}_B + a_{\\theta}-1}e^{-(n_A+n_B\\gamma+b_{\\theta})\\theta}\n\\end{align}\\]\nTherefore, \\(p(\\theta|y_A, y_B, \\gamma) \\sim gamma(\\theta| n_A\\bar{y}_A + n_B\\bar{y}_B + a_\\theta, n_A+ n_B\\gamma + b_\\theta)\\)\n\n\n10.3.3 (c)\n\nObtain the form of the full conditional distribution of \\(\\gamma\\) given \\(\\mathbb{y}_A\\), \\(\\mathbb{y}_{B}\\) and \\(\\theta\\).\n\n\\[\\begin{align}\n    p(\\gamma | y_A, y_B, \\theta)%\n    &= \\frac{p(y_A, y_B | \\gamma, \\theta)p(\\gamma|\\theta)}{p(y_A, y_B|\\theta)}\\\\\n    &\\propto p(y_A, y_B | \\gamma, \\theta)p(\\gamma|\\theta)\\\\\n    &= p(y_A|\\gamma,\\theta)p(y_B|\\gamma,\\theta)p(\\gamma)\\\\\n    &= p(y_A|\\theta)p(y_B|\\gamma,\\theta)p(\\gamma)\\\\\n    &= \\prod_{i=1}^{n_A}p(y_{Ai}|\\theta) \\prod_{i=1}^{n_B}p(y_{Bi}|\\gamma,\\theta)p(\\gamma)\\\\\n    &= \\theta^{s\\bar{y}_A}e^{-n_A\\theta}(\\gamma\\theta)^{n_B\\bar{y}_B}e^{-n_B\\gamma\\theta}\\gamma^{a_{\\gamma}-1}e^{-b_{\\gamma}\\gamma}\\\\\n    &\\propto \\gamma^{n_B\\bar{y}_B + a_\\gamma -1 } e^{-n_A\\theta-n_B\\gamma\\theta-b_\\gamma\\gamma}\\\\\n    &\\propto \\gamma^{n_B\\bar{y}_B + a_\\gamma -1 } e^{-(n_B\\theta+b_\\gamma)\\gamma}\n\\end{align}\\]\nTherefore, \\(p(\\gamma|y_A, y_B, \\theta) \\sim Gamma(n_B\\bar{y}_B+a_\\gamma, n_B\\theta+b_\\gamma)\\)\n\n\n10.3.4 (d)\n\nSet \\(a_{\\theta} = 2\\) and \\(b_{\\theta} =1\\). Let \\(a_{\\gamma} = b_{\\gamma} \\in \\{8,16,32,64,128\\}\\). For each of these five values, run a Gibbs sampler of at least \\(5000\\) iterations and obtain \\(E[\\theta_B - \\theta_A| \\mathbb{y}_A, \\mathbb{y}_B]\\). Describe the effects of the prior distribution for \\(\\gamma\\) on the results.\n\n\nclass GibbSampler:\n    def __init__(self, a_t, b_t, a_g, b_g):\n        # Theta\n        self.a_t = 2\n        self.b_t = 1\n        # Gamma\n        self.a_g = a_g\n        self.b_g = b_g\n        self.theta = st.gamma(a_t, scale=1/b_t)\n        self.gamma = st.gamma(a_g, scale=1/b_g)\n\n    def expection(self, obs_func, dataA, dataB, n_sampling):\n        # Sample number\n        nA=len(dataA)\n        nB= len(dataB)\n        # Sample mean\n        mA = np.mean(dataA)\n        mB = np.mean(dataB)\n        \n    \n        thetaAs = np.zeros(n_sampling) \n        thetaBs = np.zeros(n_sampling)\n\n        # initial\n        theta_pre = st.gamma.rvs(self.a_t, scale=(self.b_t)**-1)\n        gamma_pre = st.gamma.rvs(self.a_g, scale=(self.b_g)**-1)\n\n        # Gibbs iteration\n        thetaAs[0] = theta_pre\n        thetaBs[0] = theta_pre*gamma_pre\n        \n        for i in range(1,n_sampling):\n            # posterior\n            theta = st.gamma.rvs(nA*mA + nB*mB + self.a_t, scale= (nA + nB*gamma_pre + self.b_t)**-1)\n            theta_pre = theta\n            gamma = st.gamma.rvs(nB*mB + self.a_g, scale= (nB*theta_pre + self.b_g)**-1)\n            gamma_pre = gamma\n    \n            # transformation\n            thetaAs[i] = theta\n            thetaBs[i] = theta*gamma\n        \n        # Apply obs\n        obs = [obs_func(thetaAs[i], thetaBs[i]) for i in range(0,n_sampling)]\n        \n        return np.mean(obs)\n\n\ndef obs_func(theta_A, theta_B):\n    return theta_B - theta_A\n\n\nvals = np.array([8,16,32,64,128], dtype=float)       \n\npriors = [{\\\n    \"a_t\": 2.0,\n    \"b_t\": 1.0,\n    \"a_g\": a,\n    \"b_g\": a\n} for a in vals]\n\nobs = {\n    \"obs_func\": obs_func, \n    \"dataA\": np.loadtxt(\"data/menchild30bach.dat\"), \n    \"dataB\": np.loadtxt(\"data/menchild30nobach.dat\"), \n    \"n_sampling\": 5000\n}\n\nexps = np.zeros(len(vals))\n\nfor i in range(0, len(vals)):\n    g = GibbSampler(**priors[i])\n    exps[i] = g.expection(**obs)\n\n\nplt.plot(vals, exps, \"-o\",color=\"black\");\nplt.xlabel(\"$(a_{g}, b_{g})$\");\nplt.ylabel(\"$E[\\\\theta_{B}-\\\\theta_{A} | y_A, y_{B}]$\");"
  },
  {
    "objectID": "hw/hw6.html#problem-external",
    "href": "hw/hw6.html#problem-external",
    "title": "10  Homework 6",
    "section": "10.4 Problem External",
    "text": "10.4 Problem External\n\nAlso complete the following problem: We would like to study the survival times after patients receive a new cancer treatment. We observe the following survival times (in years) for \\(6\\) patients: \\(3\\), \\(5\\), \\(x\\), \\(4\\), \\(x\\), \\(x\\). Here, \\(x\\) denotes a censored observation, meaning that the respective patient survived for more than \\(5\\) years after the treatment (which is when the study ended). We consider the following model: \\[\\begin{equation}\nY_i = \\begin{cases}\nZ_i, & Z_i \\leq c\\\\\n\\times, & Z_i > c\n\\end{cases},\ni = 1, \\dots, n\n\\end{equation}\\] \\[Z_1, \\dots, Z_n |\\theta \\sim^{iid} Exponential(\\theta)\\] \\[\\theta\\sim Gamma(a,b)\\] We have \\(a=1\\), \\(b=4\\), \\(c=5\\), and \\(n=6\\).\n\n\n10.4.1 (a)\n\nFind the full-conditional distribution (FCD) of \\(\\theta\\)\n\n\\[\\begin{align}\n    p(\\theta|y, z)%\n    &\\propto \\underbrace{p(y|\\theta, z)}_{=1}p(\\theta|z)\\\\\n    &= p(\\theta|z)\\\\\n    &= \\frac{p(z|\\theta)p(\\theta)}{p(z)}\\\\\n    &\\propto p(z|\\theta)p(\\theta)\\\\\n    &= \\prod_{i=1}^{n=6} p(z_i|\\theta)p(\\theta)\n\\end{align}\\]\n\n\\(p(z_i|\\theta) \\propto \\theta e^{-\\theta z_i}\\)\n\\(p(\\theta) \\propto \\theta^{a-1}e^{-b\\theta}\\)\n\n\\[\\begin{align}\n    p(\\theta|y,z)%\n    &=  \\left(\\prod_{i=1}^{n=6} \\theta e^{-\\theta z_i}\\right) \\theta^{a-1}e^{-b\\theta}\\\\\n    &= \\theta^n e^{-\\theta n \\bar{z}}\\theta^{a-1}e^{-b\\theta}\\\\\n    &= \\theta^{n+a-1}e^{-b\\theta-\\theta n \\bar{z}}\\\\\n    &= \\theta^{(n+a)-1}e^{-(b+n\\bar{z})\\theta}\n\\end{align}\\]\nTherefore, \\(\\theta \\sim gamma(n+a, b+n\\bar{z})\\) - where \\(\\bar{z} = \\frac{1}{n}\\sum_{i=1}^{n}z_i\\)\n\n\n10.4.2 (b)\n\nFind the FCD of each \\(Z_i\\).\n(Hint: For uncensored \\(i\\), this distribution will be a degenerate point mass; for censored \\(i\\), the resulting distribution will be a so-called truncated exponential distribution, which is proportional to a exponential density but constrained to lie in an interval. Each FCD does not depend on other Z’s)\n\n\\[\\begin{align}\n    p(z_i |\\theta, y_i=x)%\n    &= \\theta e^{-\\theta(z_i - c)}\n\\end{align}\\]\n\n\\(p(z_i |\\theta, y_i \\text{ is uncensored }) = 1\\)\n\n\\[\\begin{align}\n    p(z_i|\\theta, y_i =x)%\n    &= p(z_i=c+t | \\theta, z_i>c)\\\\\n    &= \\frac{p(z_i=c+t \\cap z_i >c |\\theta)}{p(z_i >c |\\theta)}\\\\\n    &= \\frac{p(z_i = c+ t |\\theta)}{p(z_i>c|\\theta)}\\\\\n    &= \\frac{\\theta e^{-\\theta(c+t)}}{\\int_{c}^{\\infty}\\theta e^{-\\theta x}dx}\\\\\n    &= \\theta e^{-\\theta t}\\\\\n    &= \\theta e^{-\\theta (z_i - c)}\n\\end{align}\\]\nThis is the memoryless property of Exponential distribution1.\nIn summary,\n\\[\\begin{equation}\n    p(z_i|\\theta, y_i) = \\begin{cases}\n        1, & y = z_i\\\\\n        \\theta e^{-\\theta (z_i - c)} & y=x\n    \\end{cases}\n\\end{equation}\\]\n\n\n10.4.3 (c)\n\nImplement a Gibbs sampler that approximate the joint posterior of \\(\\theta\\) and \\(Z_1,\\dots, Z_n\\). (For example, you can use truncdist::rtrunc(3, spec=exp, a=c, rate=theta) (Use stats.truncexpon) to sample from a truncated exponential in R.) Run the sampler for enough iterations such that each of the effective sample sizes for \\(\\theta\\) and for the three censored \\(Z_i\\) are all greater than \\(1000\\). Provide the corresponding trace plots and discuss the mixing of the Markov chain.\n\n\nBoth sampling converges to a range of stochastic state, that means the sampling is not stuck in certain region. The dependence between two subsequence steps is low.\n\n\ndata = np.array([3,5,4], dtype=float)\nn = 6\nn_cs = n - len(data)\na = 1\nb = 4\nc = 5\nnSamp = 1000\n\nthetas = np.zeros(nSamp)\nzs = np.zeros((nSamp, n_cs))\n\nthetas[0] = 1\nzs[0] = [7,10,8]\n\nfor i in range(1, nSamp):\n    df = np.concatenate([data, zs[i-1]])\n    thetas[i] = st.gamma.rvs(n+a, scale= (b+np.sum(df))**-1)\n    zs[i] = st.expon.rvs(loc = c, scale= thetas[i], size =3)\n\n# Plotting\nfig, ax = plt.subplots(2,1)\n\nax[0].plot(thetas, \"-\", color=\"k\")\nax[1].plot(zs[:,0], \"-\", color=\"k\")\n\nax[1].set_xlabel(\"Iteration\")\nax[0].set_ylabel(\"$\\\\theta$\")\nax[1].set_ylabel(\"Z\");\n\n\n\n\n\nTruncated exponential distribution is sampled by scipy.stats.truncexpon 2\n\n\n\n10.4.4 (d)\n\nObtain an approximate \\(96\\%\\) posterior credible interval for \\(\\theta\\) based on the samples from (c).\n\n\nplt.hist(thetas);\n\n\n\n\n\ninvs = st.mstats.mquantiles(thetas, prob=[0.02, 0.98])\n\npd.DataFrame({\"96% Credible Interval of theta\": [\"Left bound\", \"Right bound\"], \"Value\": invs})\n\n\n\n\n\n  \n    \n      \n      96% Credible Interval of theta\n      Value\n    \n  \n  \n    \n      0\n      Left bound\n      0.085378\n    \n    \n      1\n      Right bound\n      0.444496\n    \n  \n\n\n\n\n\n\n\n\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical Methods. Vol. 580. Springer."
  },
  {
    "objectID": "hw/hw7.html#description",
    "href": "hw/hw7.html#description",
    "title": "11  Homework 7",
    "section": "11.1 Description",
    "text": "11.1 Description\n\nCourse: STAT638, 2022 Fall\n\n\nRead Chapter 7 in Hoff. Then, do the following exercises: 7.1, 7.3, 7.4.\nProblem 7.1 considers the standard/joint Jeffreys prior (as opposed to the independent Jeffreys prior considered on the lecture slides). You may find the following hints useful:\n\nYou can write \\(y_i-\\theta\\) as \\((y_i - \\bar{y}) + (\\bar{y}-\\theta)\\) and expand the quadratic form in the exponent in the multivariate normal likelihood accordingly.\n\\(\\sum_{i} b^{T}_{i} Ac = c^T A(\\Sigma_{i} b_i)\\)\nBrute-force integration can sometimes be avoided if the integrand is proportional to a known density (e.g., multivariate normal), as any density integrates to 1 and the normalizing constant is known for known densities. For 7.3, note that the rWishart() function in R returns a three-dimensional array, so we have to index the array as [,,1] to get to the actual matrix located within the array."
  },
  {
    "objectID": "hw/hw7.html#computational-enviromnent-setup",
    "href": "hw/hw7.html#computational-enviromnent-setup",
    "title": "11  Homework 7",
    "section": "11.2 Computational Enviromnent Setup",
    "text": "11.2 Computational Enviromnent Setup\n\n11.2.1 Third-party libraries\n\nusing Pkg\nPkg.activate(\"hw7\")\nusing Distributions\nusing DataFrames\nusing Turing\nusing Plots\nusing DelimitedFiles\nusing LinearAlgebra\nusing Statistics\nusing Turing\n\n  Activating project at `~/Documents/GitHub/STAT638_Applied-Bayes-Methods/hw/hw7`\n\n\n\n\n11.2.2 Version\n\nPkg.status()\nVERSION\n\nStatus `~/Documents/GitHub/STAT638_Applied-Bayes-Methods/hw/hw7/Project.toml`\n⌃ [a93c6f00] DataFrames v1.4.2\n⌃ [31c24e10] Distributions v0.25.76\n⌃ [91a5bcdd] Plots v1.35.5\n⌃ [fce5fe82] Turing v0.21.12\n  [8bb1440f] DelimitedFiles\nInfo Packages marked with ⌃ have new versions available and may be upgradable.\n\n\nv\"1.8.2\""
  },
  {
    "objectID": "hw/hw7.html#problem-7.1",
    "href": "hw/hw7.html#problem-7.1",
    "title": "11  Homework 7",
    "section": "11.3 Problem 7.1",
    "text": "11.3 Problem 7.1\n\nJeffrey’s prior: For the multivariate normal model, Jeffreys’ rule for generating a prior distribution on \\((\\theta, \\Sigma)\\) gives \\(p_J(\\theta, \\Sigma) \\propto |\\Sigma|^{-(p+2)/2}\\).\n\n\n11.3.1 (a)\n\nExplain why the function \\(p_J\\) cannot actually be a probability density for \\((\\theta, \\Sigma)\\).\n\nThe density is independent of \\(\\theta\\). The integration can be infinity and beyond \\(1\\).\n\n\n11.3.2 (b)\n\nLet \\(p_J(\\theta, \\Sigma|y_1, \\dots, y_n)\\) be the probability density that is proportional to \\(p_J(\\theta, \\Sigma)\\times p(y_1,\\dots, y_n|\\theta, \\Sigma)\\). Obtain the form of \\(p_J(\\theta, \\Sigma|y_1, \\dots, y_n)\\), \\(p_J(\\theta|\\Sigma, y_1, \\dots, y_n)\\) and \\(p_J(\\Sigma|y_1, \\dots, y_n)\\).\n\n\\[\\begin{align}\n    p_{J}(\\theta, \\Sigma | y_1, \\dots, y_n)%\n    &\\propto p_J(\\theta, \\Sigma) \\times p(y_1, \\dots, y_n | \\theta, \\Sigma)\\\\\n    &\\propto |\\Sigma|^{-\\frac{p+2}{2}} \\times \\left[|\\Sigma|^{-\\frac{n}{2}} \\exp\\left(-tr(S_{\\theta}\\Sigma^{-1})\\right)\\right /2]\\\\\n    &\\propto |\\Sigma|^{-\\frac{p+n+2}{2}}\\exp\\left( -tr(S_{\\theta}\\Sigma^{-1})/2 \\right)\n\\end{align}\\]\n\\[\\begin{align}\n    p_J(\\theta | \\Sigma, y_1, \\dots, y_n)%\n    &\\propto  \\exp\\left[ - \\sum^{n}_{i=1} (y_i - \\theta)^T \\Sigma^{-1} (y_i - \\theta)/2 \\right]\\\\\n    &\\propto \\exp \\left[ -n(\\bar{y} - \\theta)^T \\Sigma^{-1} (\\bar{y}-\\theta)/2 \\right]\\\\\n    &\\propto Normal(\\theta; \\bar{y},\\frac{\\Sigma}{n})\n\\end{align}\\]\n\\[\\begin{align}\n    p_{J}(\\Sigma | y_1, \\dots, y_n, \\theta)%\n    &\\propto |\\Sigma|^{-\\frac{p+n+2}{2}}\\exp\\left( -tr(S_\\theta \\Sigma^{-1})/2 \\right)\\\\\n    &\\propto inverse-Wishart(\\Sigma; n+1, S_{\\theta}^{-1} )\n\\end{align}\\]"
  },
  {
    "objectID": "hw/hw7.html#problem-7.3",
    "href": "hw/hw7.html#problem-7.3",
    "title": "11  Homework 7",
    "section": "11.4 Problem 7.3",
    "text": "11.4 Problem 7.3\n\nAustralian crab data: The files bluecrab.dat and orangecrab.dat contain measurements of body depth (\\(Y_1\\)) and rear width (\\(Y_2\\)), in millimeters, made on \\(50\\) male crabs from each of two species, blud and orange. We will model these data using a bivariate normal distribution.\n\n\ndblue = readdlm(\"data/bluecrab.dat\")\ndoran = readdlm(\"data/orangecrab.dat\");\n\n\n11.4.1 (a)\n\nFor each of the two species, obtain posterior distributions of the population mean \\(\\theta\\) and covariance matrix \\(\\Sigma\\) as follows: Using the semiconjugate prior distributions for \\(\\theta\\) and \\(\\Sigma\\), set \\(\\mu_0\\) equal to the sample mean of the data, \\(\\Lambda_0\\) and \\(S_0\\) equal to the sample covariance matrix and \\(\\nu_0 =4\\). Obtain \\(10000\\) posterior samples of \\(\\theta\\) and \\(\\Sigma\\). Note that this prior distribution lossely centers the parameters around empirical estimates based on the observed data (and is very similar to the unit information prior described in the previous exericise). It cannot be consitered as our true prior distribution, as it was derived from the observed data. However, it can roughly considered as the prior distribution of someone with weak but unbiased information.\n\n\n\\(p(\\theta) = \\exp\\left[ -\\frac{1}{2}\\theta^T A_0 \\theta + \\theta^T b_0\\right] = multivariate-normal(\\mu_0, \\Lambda_0)\\)\n\n\\(A_0 = \\Lambda_{0}^{-1}\\)\n\\(b_0 = \\Lambda_{0}^{-1}\\mu_0\\)\n\n\\(p(\\Sigma) = inverse-Whishart(\\nu_0, S_{0}^{-1})\\)\n\n\nS = 10000\nfunction sampling(crab)\n    n, p = size(crab)\n    μ₀ = transpose(mean(crab, dims=1))\n    Λ₀ = S₀= cov(crab)\n    ν₀ = 4\n    θs = zeros(S, p)\n    Σs = zeros(S, p, p);\n\n    # Gibbs sampling\n    for s in 1:S\n        # update θ\n        Λₙ = inv(inv(Λ₀) + n*inv(S₀))\n        μₙ = Λₙ * (inv(Λ₀)*μ₀ + n*inv(S₀)*μ₀)\n        θ = rand(MvNormal( vec(μₙ), Λₙ))\n\n        # update Σ\n        res = crab .- reshape(θ, 1, p)\n        Sₜₕ = transpose(res) * res\n        Sₙ = S₀ + Sₜₕ\n        Σ = rand(InverseWishart(ν₀ + n, Sₙ))\n        # Store data\n        θs[s,:] = θ\n        Σs[s,:, :] = Σ\n    end\n    return θs, Σs\nend \n\n\nθbs, Σbs = sampling(dblue)\nθos, Σos = sampling(doran);\n\n\n\n11.4.2 (b)\n\nPlot values of \\(\\theta=(\\theta_1, \\theta_2)'\\) for each group and compare. Describe any size differences between the two groups.\n\nThe blue crab has larger variance and lower means of \\(\\theta_1\\) and \\(\\theta_2\\) than orange one.\n\nplot\npb = scatter(θbs[:,1], θbs[:,2], label=\"Blue\")\npo = scatter(θos[:,1], θos[:,2], label=\"Orange\")\n\nμθb = mean(θbs, dims=1)\nμθo = mean(θos, dims=1)\n\nscatter!(pb, [μθb[1]], [μθb[2]], label=\"mean $(round.(μθb; digits = 1))))\", markersize = 10)\nscatter!(po, [μθo[1]], [μθo[2]], label=\"mean $(round.(μθo; digits = 1))\", markersize = 10)\n\nplot(pb, po, layout = (1, 2), xlabel=\"θ₁\", ylabel=\"θ₂\")\n\n\n\n\n\nmean(θos[:,1] .> θbs[:,1])\n\n0.8927\n\n\n\nmean(θos[:,2] .> θbs[:,2])\n\n0.9983\n\n\n\n\n11.4.3 (c)\n\nFrom each covariance matrix obtained from the Gibbs sampler, obtain the corresponding correlation coefficient. From these values, plot posterior densities of the correlations \\(\\rho_{\\text{blue}}\\) and \\(\\rho_{\\text{orange}}\\) for the two groups. Evaluate differences between the two species by comparing these posterior distributions. In particular, obtain an approximation to \\(Pr(\\rho_{\\text{blue}} < \\rho_{\\text{orange}} | y_{\\text{blue}}, y_{\\text{orange}})\\). What do the results suggest about differences between the two populations?\n\n\ncorrelation(covmat) = covmat[1,2] / sqrt(covmat[1,1] * covmat[2,2])\n\ncorrbs = [correlation(Σbs[i,:,:]) for i in 1:S]\ncorros = [correlation(Σos[i,:,:]) for i in 1:S];\n\nh = histogram(corrbs, label=\"Blue\", xlabel=\"Correlation\")\nhistogram!(h, corros, label=\"Orange\", ylabel=\"Count\")\n\n\n\n\n\\(Pr(\\rho_{\\text{blue}} < \\rho_{\\text{orange}} | y_{\\text{blue}}, y_{\\text{orange}})\\) is\n\nmean(corrbs .< corros)\n\n0.9876"
  },
  {
    "objectID": "hw/hw7.html#problem-7.4",
    "href": "hw/hw7.html#problem-7.4",
    "title": "11  Homework 7",
    "section": "11.5 Problem 7.4",
    "text": "11.5 Problem 7.4\n\nMarriage data: The file agehw.dat contains data on the ages of \\(100\\) married couples sampled from the U.S. population.\n\n\ndagew = readdlm(\"data/agehw.dat\")[2:end, :]\nsize(dagew)\n\n(100, 2)\n\n\n\n11.5.1 (a)\n\nBefore you look at the data, use your own knowledge to formulate a semiconjugate prior distribution for \\(\\theta=(\\theta_h, \\theta_w)^T\\) and \\(\\Sigma\\), where \\(\\theta_h\\), \\(\\theta_w\\) are mean husband and wife ages, and \\(\\Sigma\\) is the covariance matrix.\n\n\n\\(\\mu_0 = (50, 50)^T\\)\nprior corrleation: \\(0.7\\), variance \\(13\\)\n\n\\(0.7 = \\frac{\\sigma_{1,2}}{169}\\)\n\\(\\sigma_{1,2} = 118.3\\)\n\n\\(\\Lambda = \\begin{bmatrix} 169 & 118.3\\\\ 118.3 & 169\\end{bmatrix}\\)\nSet \\(S^{-1}_{0} = \\Lambda_{0}\\)\n\\(\\nu_0 = p + 2 = 4\\)\n\n\nn, p = size(dagew);\n\nμ₀ = ones(p,1) .* transpose(mean(dagew, dims=1))\nΛ₀ = S₀ = [ 169 118.3 ; 118.3 169]\nν₀ = p + 2;\n\n\n\n11.5.2 (b)\n\nGenerate a prior predictive dataset of size \\(n=100\\), by sampling \\((\\theta, \\Sigma)\\) from your prior distribution and then simulating \\(Y_1, \\dots, Y_n \\sim i.i.d.\\) multivariate normal \\((\\theta, \\Sigma)\\). Generate several such datasets, make bivariate scatterplots for each dataset, and make sure they roughly represent your prior beliefs about what such a dataset would actually look like. If your prior predictive datasets do not confirm to your beliefs, go back to part (a) and formulate a new prior. Report the prior that you eventually decide upon, and provide scatterplots for at least three prior predictive datasets.\n\nChoose\n\\(p = 2\\), and Λ₀ = S₀ = [ 169 118.3 ; 118.3 169]\n\nN = 100\nS = 9 \n\nYpreds = zeros(S, p, N)\nfor i in 1:S\n    θ = rand(MvNormal( vec(μ₀), Λ₀))\n    Σ = rand(InverseWishart(ν₀ + n, S₀))\n    Ypreds[i,:, :] = rand(MvNormal(θ, Σ), N)\nend\n\npvers = [plot() for i in 1:S]\nfor i in 1:S\n    scatter!(pvers[i], Ypreds[i, 1, :], Ypreds[i, 2, :], label=\"Dataset $i\")\nend\n\nplot(pvers..., layout = (3, 3), xlabel=\"Y₁\", ylabel=\"Y₂\")\n\n\n\n\n\n\n11.5.3 (c)\n\nUsing your prior distribution and the \\(100\\) values in the dataset, obtain an MCMC approximation to \\(p(\\theta, \\Sigma|y_1, \\dots, y_{100})\\). Plot the joint posterior distribution of \\(\\theta_h\\) and \\(\\theta_w\\), and also the marginal posterior density of the correlation between Y_h and Y_w, the ages of a husband and wife. Obtain \\(95\\%\\) posterior confidence intervals for \\(\\theta_h\\), \\(\\theta_w\\) and the correlation coefficient.\n\n\nS = 10000\n\nfunction mcmc(data, μ₀, Λ₀, S₀, ν₀)\n    yₙ = mean(data, dims=1)\n    n, p = size(data)\n\n    θs = zeros(S, p)\n    Σs = zeros(S, p, p)\n    Σ = cov(data)\n    for i in 1:S\n        #update θ\n        Λₙ = inv(inv(Λ₀) + n * inv(Σ))\n        Λₙ[1,2] = Λₙ[2,1]\n\n        μₙ = Λₙ * (inv(Λ₀)*μ₀ + n*inv(Σ) * transpose(yₙ))\n\n   \n        θ = rand(MvNormal( vec(μₙ), Λₙ))\n\n        #update Σ\n        res = data .- reshape(θ, 1, p)\n        Sₜₕ = transpose(res) * res\n        Sₙ = S₀ + Sₜₕ\n        Σ = rand(InverseWishart(ν₀ + n, Sₙ))\n        # Store data\n        θs[i,:] = θ\n        Σs[i,:, :] = Σ\n    end\n\n    return θs, Σs\nend\n\nθs, Σs = mcmc(dagew, μ₀, Λ₀, S₀, ν₀);\ncorrs = [correlation(Σs[i,:,:]) for i in 1:S];\n\nHusband Quantiles\n\nquantile(θs[:,1], [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 41.71712706938635\n 44.39676744614884\n 47.09431425711537\n\n\nWife Quantiles\n\nquantile(θs[:,2], [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 38.41939096164427\n 40.87349736587568\n 43.418570387079846\n\n\nCorrelation Quantiles\n\nquantile(corrs, [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 0.8598526557581118\n 0.9028287252227811\n 0.9325781232590956\n\n\n\n\n11.5.4 (d)\n\nObtain \\(95\\%\\) posterior confidence intervals for \\(\\theta_h\\), \\(\\theta_{\\omega}\\) and the correlation coefficient using the following prior distributions:\n\nJeffrey’s prior, described in Exercise 7.1;\nThe unit information prior, described in Exercise 7.2;\nA “diffuse prior” with \\(\\mu_0=0, \\Lambda_0 = 10^5 \\times I, S_0 = 1000\\times I\\) and \\(v_0 =3\\).\n\n\n\n11.5.4.1 Part I**\n\nθs, Σs = mcmc(dagew, μ₀, cov(dagew), cov(dagew), size(dagew)[1]+1);\ncorrs = [correlation(Σs[i,:,:]) for i in 1:S];\n\nHusband Quantiles\n\nquantile(θs[:,1], [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 42.52441579546774\n 44.418141464043245\n 46.33805864696494\n\n\nWife Quantiles\n\nquantile(θs[:,2], [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 39.08757959835684\n 40.86391657673728\n 42.72108377734497\n\n\nCorrelation Quantiles\n\nquantile(corrs, [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 0.8756579285507731\n 0.9039189535126206\n 0.9263941065474931\n\n\n\n\n11.5.4.2 Part II\n\nθs, Σs = mcmc(dagew, transpose(mean(dagew, dims=1)), cov(dagew)/100., cov(dagew)/100., 2. + 1.);\ncorrs = [correlation(Σs[i,:,:]) for i in 1:S];\n\nHusband Quantiles\n\nquantile(θs[:,1], [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 42.50529893890636\n 44.42248855701973\n 46.29402259410223\n\n\nWife Quantiles\n\nquantile(θs[:,2], [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 39.082198908879754\n 40.87726763238499\n 42.65717356460964\n\n\nCorrelation Quantiles\n\nquantile(corrs, [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 0.8630261005470224\n 0.904563195139358\n 0.9341516213080364\n\n\n\n\n11.5.4.3 Part III\n\nθs, Σs = mcmc(dagew, [0;0], [10^5 0; 0 10^5], [10^3 0; 0 10^3], 3);\ncorrs = [correlation(Σs[i,:,:]) for i in 1:S];\n\nHusband Quantiles\n\nquantile(θs[:,1], [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 41.692569822172345\n 44.4194659798749\n 47.209907857971324\n\n\nWife Quantiles\n\nquantile(θs[:,2], [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 38.263899627117944\n 40.879785191881425\n 43.48074886203784\n\n\nCorrelation Quantiles\n\nquantile(corrs, [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 0.792709080928878\n 0.8543476559888816\n 0.89959452338722\n\n\n\n\n\n11.5.5 (e)\n\nCompare the confidence intervals from (d) to those obtained in (c). Discuss whether or not you think that your prior information is helpful in estimating \\(\\theta\\) and \\(\\Sigma\\), or if you think one of the alternatives in (d) is preferable. What about if the sample size were much smaller, say \\(n=25\\)?\n\nThe prior information does not matter because the sample size is large. No matter how prior is setup, the posterior distribution is similar. However, for smaller sample size, those approaches may differ.\n\nμ₀ = [50.; 50.]\nΛ₀ = S₀ = [ 169 118.3 ; 118.3 169]\nν₀ = p + 2+9;\n\nθs, Σs = mcmc(dagew, μ₀, Λ₀, S₀, ν₀)\n\n([45.03212480248956 42.158942566022134; 44.19289889461345 39.913719022014824; … ; 43.118960131235504 39.85494130735777; 45.851070752555195 41.90866145945041], [160.05480435947902 134.01263101431502; 167.62930621107768 139.72641493969817; … ; 192.31055286979574 154.84156304833144; 223.28476886238866 181.61583904818187;;; 134.01263101431502 142.01458641718997; 139.72641493969817 147.83068828095497; … ; 154.84156304833144 150.27190716256538; 181.61583904818187 173.36910592951546])\n\n\nHusband Quantiles\n\nquantile(θs[:,1], [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 41.92193411691592\n 44.46735932525378\n 47.04949177514974\n\n\nWife Quantiles\n\nquantile(θs[:,2], [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 38.56649700730234\n 40.95246209220502\n 43.35536857557841\n\n\nCorrelation Quantiles\n\nquantile(corrs, [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 0.792709080928878\n 0.8543476559888816\n 0.89959452338722"
  },
  {
    "objectID": "hw/hw8.html#description",
    "href": "hw/hw8.html#description",
    "title": "12  Homework 8",
    "section": "12.1 Description",
    "text": "12.1 Description\n\nCourse: STAT638, 2022 Fall\n\n\nRead Chapter 8 in the Hoff book. Then do the following exercises in Hoff: 8.1 and 8.3.\nPlease note some typos in 8.1: All \\(\\theta_i\\)’s should be \\(\\theta_j\\)’s.\nFor 8.1(c), you may find the law of total (co-)variance useful. In addition, remember that all of these laws also hold for conditional distributions (e.g., when conditioning on additional quantities such as \\(\\mu\\) and \\(\\tau^2\\) in all terms on the left- and right-hand side of the equation)."
  },
  {
    "objectID": "hw/hw8.html#computational-enviromnent-setupques",
    "href": "hw/hw8.html#computational-enviromnent-setupques",
    "title": "12  Homework 8",
    "section": "12.2 Computational Enviromnent Setup1",
    "text": "12.2 Computational Enviromnent Setup1\n\n12.2.1 Third-party libraries\n\nusing Pkg\nPkg.activate(\"hw8\")\nusing Distributions\nusing DataFrames\nusing Turing\nusing Plots\nusing DelimitedFiles\nusing LinearAlgebra\nusing Statistics\nusing Turing\nusing StatsBase\nusing StatsPlots\nimport Random\nRandom.seed!(2022)\n\n  Activating project at `~/Documents/GitHub/STAT638_Applied-Bayes-Methods/hw/hw8`\n\n\nRandom.TaskLocalRNG()\n\n\n\n\n12.2.2 Version\n\nPkg.status()\nVERSION\n\nStatus `~/Documents/GitHub/STAT638_Applied-Bayes-Methods/hw/hw8/Project.toml`\n⌃ [a93c6f00] DataFrames v1.4.2\n⌃ [31c24e10] Distributions v0.25.76\n⌃ [be115224] MCMCDiagnosticTools v0.1.4\n⌃ [91a5bcdd] Plots v1.35.5\n  [2913bbd2] StatsBase v0.33.21\n  [f3b207a7] StatsPlots v0.15.4\n⌃ [fce5fe82] Turing v0.21.12\n  [8bb1440f] DelimitedFiles\n  [10745b16] Statistics\nInfo Packages marked with ⌃ have new versions available and may be upgradable.\n\n\nv\"1.8.2\"\n\n\n\n\n\n12.2.3 Problem 8.1\n\nComponents of variance: Consider the hierarchical model where\n\\[\\theta_1, \\dots, \\theta_m | \\mu, \\tau^2 \\sim i.i.d. \\text{normal}(\\mu, \\tau^2)\\]\n\\[y_{1,j}, \\dots, y_{n_j, j} |\\theta_j, \\sigma^2 \\sim i.i.d. \\text{normal}(\\theta_j, \\sigma^2)\\] For this problem, we will eventually compute the following:\n\n\\(Var[y_{i,j}|\\theta_i, \\sigma^2]\\), \\(Var[\\bar{y}_{\\cdot,j}|\\theta_i, \\sigma^2]\\), \\(Cov[y_{i_1,j}, y_{i_2, j}|\\theta_j, \\sigma^2]\\)\n\\(Var[y_{i,j}|\\mu, \\tau^2]\\), \\(Var[\\bar{y}_{\\cdot,j}|\\mu, \\tau^2]\\), \\(Cov[y_{i_1,j}, y_{i_2, j}|\\mu, \\tau^2]\\) First, lets use our intuition to guess at the answers:\n\n\n\n\n12.2.4 (a)\n\nWhich do you think is bigger, \\(Var[y_{i,j}|\\theta_i, \\sigma^2]\\) or \\(Var[y_{i,j}|\\mu, \\tau^2]\\)? To guide your intuition, you can interpret the first as the variability of the \\(Y\\)’s when sampling from a fixed group, and the second as the variability in first sampling a group, then sampling a unit from within the group.\n\n\n\\(Var[y_{i,j} | \\mu, \\tau^2]\\) because \\(\\theta_j\\) is uncertain and the between-group varibability create additional uncertainty.\n\n\n\n12.2.5 (b)\n\nDo you think \\(Cov[y_{i_1,j}, y_{i_2, j}|\\theta_j, \\sigma^2]\\) is negative, positive, or zero? Answer the same for \\(Cov[y_{i_1,j}, y_{i_2, j}|\\mu, \\tau^2]\\). You may want to think about what \\(y_{i_2, j}\\) tells you about \\(y_{i_1, j}\\) if \\(\\theta_j\\) is known, and what it tells you when \\(\\theta_j\\) is unknown.\n\n\\(Cov[y_{i_1,j}, y_{i_2, j}|\\theta_j, \\sigma^2]\\)\nBecause \\(y_{i_1, j}\\) and \\(y_{i_2, j}\\) is i.i.d. sampled, I expect \\(Cov[y_{i_1,j}, y_{i_2, j}|\\theta_j, \\sigma^2]\\) to be zero.\n\\(Cov[y_{i_1,j}, y_{i_2, j}|\\mu, \\tau^2]\\)\n\\(y_{1,j}\\) does tell information about \\(y_{2,j}\\). The covariance \\(Cov[y_{i_1,j}, y_{i_2, j}|\\mu, \\tau^2]\\) is likely to be positive because values from same \\(\\theta_j\\) tend to be close together.\n\n\n12.2.6 (c)\n\nNow compute each of the six quantities above and compare to your answers in (a) and (b). 2\n\n\\[\\begin{align}\n    Var[y_{i,j}|\\theta_i, \\sigma^2]%\n    &= \\sigma^2\n\\end{align}\\]\n\\[\\begin{align}\n    Var[\\bar{y}_{\\cdot,j}|\\theta_i, \\sigma^2]%\n    &= Var[\\sum_{i'=1}^{n_j}y_{i',j}/n |\\theta_i, \\sigma^2]\\\\\n    &= \\frac{1}{n^2}Var[\\sum_{i'=1}^{n_j}y_{i',j} |\\theta_i, \\sigma^2]\\\\\n    &= \\frac{1}{n^2} \\sum_{i'=1}^{n_j} Var[y_{i',j} |\\theta_i, \\sigma^2]\\\\\n    &= \\frac{1}{n} Var[y_{i',j} |\\theta_i, \\sigma^2]\\\\\n    &= \\frac{\\sigma^2}{n}\n\\end{align}\\]\n\\[\\begin{align}\n    Cov[y_{i_1,j}, y_{i_2, j}|\\theta_j, \\sigma^2]%\n    &= E[y_{i_1, j} y_{i_2, j}] - E[y_{i_1, j}]E[y_{i_2, j}]\\\\\n    &= E[y_{i_1, j}]E[y_{i_2, j}] - E[y_{i_1, j}]E[y_{i_2, j}]\\\\\n    &= 0\n\\end{align}\\]\n\\[\\begin{align}\n    Var[y_{i,j}|\\mu, \\tau^2]%\n    &= E(Var[y_{i,j}|\\mu, \\tau^2, \\theta, \\sigma^2]|\\mu, \\tau^2) + Var(E[y_{i,j}|\\mu, \\tau^2, \\theta, \\sigma^2]|\\mu, \\tau^2)\\\\\n    &= E(\\sigma^2 | \\mu, \\tau^2) + Var(\\theta | \\mu, \\tau^2)\\\\\n    &= \\sigma^2 + \\tau^2\n\\end{align}\\]\n\\[\\begin{align}\n    Var[\\bar{y}_{\\cdot,j}|\\mu, \\tau^2]%\n    &= E(Var[\\bar{y}_{\\cdot,j}|\\mu, \\tau^2, \\theta, \\sigma^2]|\\mu, \\tau^2) + Var(E[\\bar{y}_{\\cdot,j}|\\mu, \\tau^2, \\theta, \\sigma^2]|\\mu, \\tau^2)\\\\\n    &= E(\\frac{\\sigma^2}{n}|\\mu,\\tau^2) + Var(\\theta | \\mu, \\tau^2)\\\\\n    &= \\frac{\\sigma^2}{n} + \\tau^2\n\\end{align}\\]\n\\[\\begin{align}\n    Cov[y_{i_1, j}, y_{i_2, j}|\\mu, \\tau^2]%\n    &= E(Cov[y_{i_1, j}, y_{i_2, j} | \\theta, \\sigma^2, \\mu, \\tau^2]| \\mu, \\tau^2) \\\\\n    &+ Cov(E[y_{i_1, j} | \\theta, \\sigma^2, \\mu, \\tau^2], E[y_{i_2, j} | \\theta, \\sigma^2, \\mu, \\tau^2] | \\mu, \\tau^2)\\\\\n    &= 0 + Cov(\\theta, \\theta | \\mu, \\tau^2)\\\\\n    &= E[\\theta^2|\\mu, \\tau^2] - E[\\theta|\\mu, \\tau^2]^2\\\\\n    &= Var(\\theta |\\mu, \\tau^2)\\\\\n    &= \\tau^2\n\\end{align}\\]\n\n\n12.2.7 (d)\n\nNow assume we have a prior \\(p(\\mu)\\) for \\(\\mu\\). Using Bayes’ rule, show that \\[p(\\mu|\\theta_1, \\dots, \\theta_m, \\sigma^2, \\tau^2, y_1, \\dots, y_m) = p(\\mu|\\theta_1, \\dots, \\theta_m, \\tau^2)\\] Interpret in words what this means.\n\n\\[\\begin{align}\np(\\mu|\\theta_1, \\dots, \\theta_m, \\sigma^2, \\tau^2, y_1, \\dots, y_m)%\n&= \\frac{p(\\sigma^2, y_1, \\dots, y_m | \\mu, \\theta_1, \\dots, \\theta_m, \\tau^2) p(\\mu |\\theta_1, \\dots, \\theta_m, \\tau^2)}{ p(\\sigma^2, y_1, \\dots, y_m | \\theta_1, \\dots, \\theta_m, \\tau^2) }\\\\\n&= p(\\mu|\\theta_1, \\dots, \\theta_m, \\tau^2)\n\\end{align}\\]\nwhere \\(p(\\sigma^2, y_1, \\dots, y_m | \\mu, \\theta_1, \\dots, \\theta_m, \\tau^2) = p(\\sigma^2, y_1, \\dots, y_m | \\theta_1, \\dots, \\theta_m, \\tau^2)\\) because knowing \\(\\mu\\) doesn’t provide more information when \\(\\theta_1, \\dots, \\theta_m\\) are known."
  },
  {
    "objectID": "hw/hw8.html#problem-8.3",
    "href": "hw/hw8.html#problem-8.3",
    "title": "12  Homework 8",
    "section": "12.3 Problem 8.3",
    "text": "12.3 Problem 8.3\n\nHerarchical modeling: The files school1.dat through school8.dat give weekly hours spent on homework for students sampled from eight different schools. Obtain posterior distributions for the true means for the eight different schools using a herarchical normal model with the following prior parameters: \\[\\mu_0 = 7, \\gamma^{2}_{0} = 5, \\tau^{2}_{0}=10, \\eta_0 = 2, \\sigma^{2}_{0} = 15, \\nu_0 = 2\\]\n\n\ndsch = Dict()\nnsch = 8\nfor i in 1:nsch\n   dsch[i] = readdlm(\"data/school$i.dat\")\nend\n\n\n12.3.1 (a)\n\nRun a Gibbs sampling algorithm to approximate the posterior distribution of \\(\\{\\theta, \\sigma^2, \\mu, \\tau^2\\}\\). Assess the convergence of the Markov chain, and find the effective sample size for \\(\\{\\sigma^2, \\mu, \\tau^2\\}\\). Run the chain long enough so that the effective sample sizes are all above \\(1000\\).\n\n\n# Prior\nμ0 = 7.\nγ0² = 5.\nτ0² = 10.\nη0 = 2.\nσ0² = 15.\nν0 = 2.\n\n# Data\nns = [ length(dsch[i]) for i in 1:nsch]\nn = sum(ns)\nm = length(dsch)\nȳs = [mean(dsch[i]) for i in 1:nsch]\ns²s = [ (ns[i] - 1)^-1 * sum( (dsch[i] .- ȳs[i]).^2) for i in 1:nsch]\n# posterior\n\nfunction τ²_pos(m, η0, θv, μ, τ0²)\n    ths² = sum([ (θ- μ)^2 for θ in θv])\n    α = (m + η0)* 0.5\n    β = (ths² + η0*τ0²)/2\n    return InverseGamma(α, β)\nend\n\nfunction σ²_pos(n, ν0, σ0², ns, s²s, ȳs, θs)\n    α = (n+ν0)/2\n    β =( sum((ns .- 1) .* s²s .+ ns .* (ȳs .- θs).^2) + ν0*σ0²)/2\n    return InverseGamma(α, β)\nend\n\nfunction μ_pos(m, τ², θs, γ0², μ0)\n    γm² = (m/τ² + 1/γ0²)^-1\n    θ̄ = mean(θs)\n\n    a = γm²*(m*θ̄/τ² + μ0/τ²)\n    return Normal(a, γm²)\nend\n\nfunction θ_pos(τ², ȳ, n, σ², μ)\n    τ̃² = (n/σ² + 1/τ²)^-1\n    a = τ̃²*(n*ȳ/σ² + μ/τ²)\n    return Normal(a, τ̃²)\nend\n\n\"\"\"\nEffective Sample Size\n\"\"\"\nfunction ess(v)\n    n = length(v)\n    c = sum(autocov(v, collect(1:n-1)))\n    return n/(1+2*c)\nend\n\n# Sampling\nsmp = 4000\nτ²s = zeros(smp)\nσ²s = zeros(smp)\nμs = zeros(smp)\nθs = zeros(smp, m)\n\n\nσ²s[1] = rand(InverseGamma(ν0/2, ν0*σ0²/2))\nτ²s[1] = rand(InverseGamma(η0 /2, η0 *τ0²/2))\nμs[1] = rand(Normal(μ0, γ0²))\n#θs[1,:] = [rand(θ_pos(τ²s[1], ȳs[i], ns[i], σ²s[1], μs[1])) for i in 1:m]\nθs[1,:] = rand(Normal(μs[1], τ²s[1]), m)\n\nfor s in 2:smp\n    σ²s[s] = rand(σ²_pos(n, ν0, σ0², ns, s²s, ȳs, θs[s-1,:]))\n    τ²s[s] = rand(τ²_pos(m, η0, θs[s-1,:], μs[s-1], τ0²))\n    θs[s,:] = [rand(θ_pos(τ²s[s-1], ȳs[i], ns[i], σ²s[s-1], μs[s-1])) for i in 1:m]\n    μs[s] = rand(μ_pos(m, τ²s[s-1], θs[s-1,:], γ0², μ0))\nend\n\n\nfor i in [τ²s, σ²s, μs, θs]\n    plot(i)\nend\n\np1 = plot(τ²s[2:end], label=\"τ²\")\np2 = plot(σ²s[2:end], label=\"σ²\")\np3 = plot(μs[2:end], label=\"μ\")\n\nplot(p1, p2, p3)\n\n\n\n\n\nEffetive Sample Size\n\\(\\tau^2\\)\n\ness(τ²s)\n1523.525\n\n\\(\\sigma^2\\)\n\ness(σ²s)\n1234.234\n\n\\(\\mu\\)\n\ness(μs)\n1045.242\n\n\n12.3.2 (b)\n\nCompute posterior means and \\(95\\%\\) confidence regions for \\(\\{\\sigma^2, \\mu, \\tau^2\\}\\). Also, compare the posterior densities to the prior densities, and discuss what was learned from the data.\n\n\n\\(\\sigma^2\\)\n\n\nquantile(σ²s, [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 11.492557006363636\n 14.15210768607951\n 17.713837318481733\n\n\n\n\\(\\mu\\)\n\n\nquantile(μs, [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n 5.3131625605052655\n 7.688237183428129\n 8.736460757511468\n\n\n\n\\(\\tau\\)\n\n\nquantile(τ²s, [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n  1.8676028002352072\n  4.466482143729323\n 15.698240842165873\n\n\n\npu = density(μs, label=\"μₙ\")\npt = density(σ²s,label=\"σ²ₙ\")\nps = density(τ²s, label=\"τ²ₙ\")\n\nplot!(pu, Normal(μ0 , γ0²), label= \"μ0\")\nplot!(pt, InverseGamma(η0 /2, η0 *τ0²/2), label= \"σ²0\")\nplot!(ps, InverseGamma(ν0/2, ν0*σ0²/2), label= \"τ²0\")\n\nxlims!(pt, 0,30)\nxlims!(ps, 0,30)\n\nplot(pu, pt, ps)\n\n\n\n\nEstimations of \\(\\mu\\) and \\(\\tau\\) are similar in prior and posterior. However, \\(\\sigma^2\\) is different.\n\n\n12.3.3 (c)\n\nPlot the posterior density of \\(R=\\frac{\\tau^2}{\\sigma^2 + \\tau^2}\\) and compare it to a plot of the prior density of \\(R\\). Describe the evidence for between-school variation.\n\n\nσ²_prs = rand(InverseGamma(ν0/2, ν0*σ0²/2), 1000)\nτ²_prs = rand(InverseGamma(η0 /2, η0 *τ0²/2), 1000)\n\n\n\nR_prs = τ²_prs ./ (σ²_prs .+ τ²_prs)\nR_pos = τ²s ./ (σ²s .+ τ²s)\n\npr = density(R_prs, label=\"R Prior\", xlabel=\"R\", ylabel=\"density\")\ndensity!(pr, R_pos, label=\"R posterior\")\n\n\n\n\n\\(R\\) represents the quantity of vairance in between-group. The prior is not certain about the specific quantity, but after applying posterior inference. The posterior probability of \\(R\\) is peaked and more cetain about the value is around \\(0.3\\).\n\n\n12.3.4 (d)\n\nObtain the posterior probability that \\(\\theta_7\\) is smaller than \\(\\theta_6\\), as well as the posterior probability that \\(\\theta_7\\) is smaller than of all the \\(\\theta\\)’s.\n\n\np(\\(\\theta_7\\) is smaller than \\(\\theta_6\\))\n\n\nmean(θs[:,7] .< θs[:,6])\n\n0.53075\n\n\n\np(\\(\\theta_7\\) is smaller than of all the \\(\\theta\\)’s)\n\n\nres = zeros(size(θs)[1])\nfor i in 1 : size(θs)[1]\n    if argmin(θs[i,:]) == 7\n        res[i] = 1\n    end\nend\nmean(res)\n\n0.339\n\n\n\n\n12.3.5 (e)\n\nPlot the sample averages \\(\\bar{y}_1, \\dots, \\bar{y}_8\\) against the posterior expectations of \\(\\theta_1, \\dots, \\theta_8\\), and describe the relationship. Also compute the sample mean of all observations and compare it to the posterior mean of \\(\\mu\\).\n\n\npsmp = scatter(ȳs, mean(θs, dims = 1)[1,:], xlabel=\"Sample Average\", ylabel= \"Posterior Expectation\")\n\nhline!(psmp, [mean(μs)], label=\"posterior mean (μn)\")\nhline!(psmp, [sum(ȳs .* ns)/n], label=\"Pooled sample mean (μ)\" )"
  },
  {
    "objectID": "hw/hw9.html#computational-environment",
    "href": "hw/hw9.html#computational-environment",
    "title": "13  Homework 9",
    "section": "13.1 Computational Environment",
    "text": "13.1 Computational Environment\n\nusing Pkg\nPkg.activate(\"hw9\")\nusing Distributions\nusing DataFrames\nusing Plots\nusing DelimitedFiles\nusing LinearAlgebra\nusing Statistics\nusing ProtoStructs\nusing CSV\nimport Random\nRandom.seed!(2022)\n\n  Activating project at `~/Documents/GitHub/STAT638_Applied-Bayes-Methods/hw/hw9`\n\n\nRandom.TaskLocalRNG()"
  },
  {
    "objectID": "hw/hw9.html#description",
    "href": "hw/hw9.html#description",
    "title": "13  Homework 9",
    "section": "13.2 Description",
    "text": "13.2 Description\n\nCourse: STAT638, 2022 Fall\nDeadline: 2022/11/17, 12:00 pm\n\n\nRead Chapter 9 in the Hoff book. Then do Problems 9.1 and 9.2 in Hoff.\nFor both regression models, please include an intercept term (\\(\\beta_0\\)).\nIn 9.1(b), please replace “max” by “min”. (This is not listed in the official book errata, but appears to be a typo.)\nFor 9.2, the azdiabetes.dat data are described in Exercise 6 of Chapter 7 (see errata).\n\n\nNote: This PDF file is generated by Quarto and LualaTeX. There is unsolved problem to display the special character in the source code. Thus, I leave the html version here for reference that displays the complete source code:\n\n\nhttps://stchiu.quarto.pub/stat_638_hw_9/"
  },
  {
    "objectID": "hw/hw9.html#problem-9.1",
    "href": "hw/hw9.html#problem-9.1",
    "title": "13  Homework 9",
    "section": "13.3 Problem 9.1",
    "text": "13.3 Problem 9.1\n\nExtrapolation: The file swim.dat contains data on the amount of time in seconds, it takes each of four high school swimmers to swim \\(50\\) yards. Each swimmer has \\(6\\) times, taken on a biweekly basis.\n\n\n13.3.1 (a)\n\nPerform the following data analysis for each swimmer separately:\n\nFit a linear regression model of swimming time as the response and week as the explanatory variable. To formulate your prior, use the information that competitive times for this age group generally range from \\(22\\) to \\(24\\) seconds.\nFor each swimmer \\(j\\), obtain a posterior predictive distribution for \\(Y^{*}_j\\), their time if they were to swim \\(2\\) weeks from the last recorded time.\n\n\n\nSuppose a linear model \\[Y = X\\beta + \\epsilon\\]\n\n\\[Y_i = x_{i,1} \\beta_1 + x_{i,2} \\beta_2 + \\epsilon_i\\]\n\n\\(Y = \\begin{bmatrix} Y_1\\\\ \\vdots\\\\ Y_6\\end{bmatrix}\\). A swimmer’s record of \\(6\\). Series in time\n\\(X = \\begin{bmatrix} x_{1,1} & x_{1,2}\\\\ \\vdots & \\vdots \\\\ x_{6,1} & x_{6,2} \\end{bmatrix}\\)\n\n\\(x_{j,1}\\): \\(j\\)th record with swim score in the range of 22 to 24 second\n\\(x_{j,2}\\): Weeks of training\n\n\\(\\beta = \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\end{bmatrix}\\).\n\n\\(\\mu_0 = \\begin{bmatrix} 23\\\\ 0 \\end{bmatrix}\\)\n\nThe prior expectation of intercept of \\(y\\) is \\(23\\).\n\n\\(\\beta_0 \\sim N_p(\\mu_0, \\Sigma_0)\\).\n\nFCD: \\(\\beta|y, \\sigma^2 \\sim N_p(\\beta_n, \\Sigma_n)\\)\n\\(\\Sigma^{-1}_{n} = \\Sigma^{-1}_{0} + \\frac{X^T X}{\\sigma^2}\\)\n\\(\\beta_n = \\Sigma_n (\\Sigma^{-1}_{0} \\beta_0 + \\frac{X^T y}{\\sigma^2})\\)\n\n\n\nPrior setting\n\n\\(\\Sigma_{0} = \\begin{bmatrix} 0.1 & 0\\\\ 0 & 0.1 \\end{bmatrix}\\)\n\nThere is uncertainty about \\(\\beta\\) estimation.\nCovariance of time and intersept is believe as \\(0\\)\n\n\\(\\sigma^2 \\sim IG(\\nu_0/2 , \\nu_0 \\sigma^{2}_0 /2)\\)\n\nFCD: \\(\\sigma^2 |y,\\beta \\sim IG((\\nu_0 + n)/2, (\\nu_0\\sigma^{2}_{0}) + SSR(\\beta)/2)\\)\n\n\\(SSR(\\beta) = (y - X\\beta)^T (y-X\\beta)\\)\n\n\nys = readdlm(\"data/swim.dat\")\n\n4×6 Matrix{Float64}:\n 23.1  23.2  22.9  22.9  22.8  22.7\n 23.2  23.1  23.4  23.5  23.5  23.4\n 22.7  22.6  22.8  22.8  22.9  22.8\n 23.7  23.6  23.7  23.5  23.5  23.4\n\n\n\n\"\"\"\nProblem 9.1 (a)\n\"\"\"\n\n@proto struct SwimmingModel \n    S = 1000 # Number of sampling\n    # Data\n    y \n    n = length(y) # number of records\n    # Model\n    X = hcat( ones(n), collect(0:2:10) )\n    p = size(X)[2]\n    # Prior\n    β₀ = MvNormal([23., 0.], [0.1 0; 0 0.1])\n    ν₀ = 1.\n    σ₀² = 0.2\nend\n\n\nfunction SSR(β, y, X)\n    ssrV = (y - X*β)' * (y - X*β)\n    return sum(ssrV)\nend\n\nfunction β_FCD(σ², m::SwimmingModel)\n    Σₙ =( m.β₀.Σ^-1 + m.X' * m.X / σ²)^-1\n    μₙ = Σₙ*(m.β₀.Σ^-1 * m.β₀.μ + m.X' * m.y / σ²)\n    return MvNormal(vec(μₙ), Hermitian(Σₙ))\nend\n\nfunction σ²_FCD(β, m::SwimmingModel)\n    α = (m.ν₀ + m.n)/2\n    θ = (m.ν₀*m.σ₀²) + SSR(β, m.y, m.X)\n    return InverseGamma(α, θ)\nend\n\nfunction pred(X, m::SwimmingModel)\n    # Sampling vector\n    βsmp = zeros(m.S, length(m.β₀.μ))\n    σ²smp = zeros(m.S)\n    y = zeros(m.S)\n    # Init\n    βsmp[1,:] = rand(m.β₀)\n    σ²smp[1] = m.σ₀²\n    y[1] = m.y[1]\n    for i in 2:m.S \n        βsmp[i,:] = rand(β_FCD(σ²smp[i-1], m))\n        σ²smp[i] = rand(σ²_FCD(βsmp[i-1,:], m))\n\n        # Predict \n        y[i] = βsmp[i,:]' * X + rand(Normal(0., σ²smp[i]))\n    end\n\n    return (y=y, β=βsmp, σ²=σ²smp)\nend\n\nj_swim = 1\nms = [ SwimmingModel(y = hcat(ys[i,:]), S=10000 ) for i in 1:size(ys)[1] ]\nys_pred = zeros(size(ys)[1], ms[1].S) \nX_pred = [1,12]\n\nfor i in eachindex(ms)\n    ys_pred[i,:] = pred([1,12], ms[i]).y\nend\n\n## Plotting\np = [histogram(ys_pred[i,:], label=\"Swimmer $i\", color=\"black\",\n    xlabel=\"Week\", ylabel=\"Seconds\"\n    ) for i in 1:size(ys)[1]]\nplot(p...)\n\n\n\n\n\n\n13.3.2 (b)\n\nThe coach of the team has to decide which of the four swimmers will compete in a swimming meet in \\(2\\) weeks. Using your predictive distributions, compute \\(Pr(Y^{*}_{j} = \\max\\{Y^{*}_1,\\dots, Y^{*}_4\\}|Y)\\) for each swimmer \\(j\\), and based on this make a recommendation to the coach.\n\n\nam = argmax(ys_pred, dims=1)\n\ny_count = zeros(1, size(ys)[1])\n\nfor a in am \n    y_count[a[1]] += 1\nend\n\npmax = vec(y_count ./ length(am))\n\n## Recommendation\nds = DataFrame( Dict(\"Swimmer\"=> collect(1:size(ys)[1]),\"Pr(Y_i is max)\" => pmax ))\n\n\n4×2 DataFrameRowPr(Y_i is max)SwimmerFloat64Int6410.023120.4773230.0424340.45734\n\n\n\n\nSwimmer 2 is the most probable winner."
  },
  {
    "objectID": "hw/hw9.html#problem-9.2",
    "href": "hw/hw9.html#problem-9.2",
    "title": "13  Homework 9",
    "section": "13.4 Problem 9.2",
    "text": "13.4 Problem 9.2\n\nModel selection: As described in Example 6 of Chapter 7, the file azdiabetes.dat contains data on health-related variables of a population of \\(532\\) women. In this exercise we will be modeling the conditional distribution of glucose level (glu) as a linear combination of the other variables, excluding the variable diabetes.\n\n\n13.4.1 (a)\n\nFit a regression model using the \\(g\\)-prior with \\(g=n\\), \\(\\nu_0 =2\\) and \\(\\sigma^{2}_{0} = 1\\). Obtain posterior confidence intervals for all of the parameters.\n\n\ndata = readdlm(\"data/azdiabetes.dat\")\n\n533×8 Matrix{Any}:\n   \"npreg\"     \"glu\"    \"bp\"    \"skin\"    \"bmi\"   \"ped\"    \"age\"  \"diabetes\"\n  5          86       68      28        30.2     0.364   24       \"No\"\n  7         195       70      33        25.1     0.163   55       \"Yes\"\n  5          77       82      41        35.8     0.156   35       \"No\"\n  0         165       76      43        47.9     0.259   26       \"No\"\n  0         107       60      25        26.4     0.133   23       \"No\"\n  5          97       76      27        35.6     0.378   52       \"Yes\"\n  3          83       58      31        34.3     0.336   25       \"No\"\n  1         193       50      16        25.9     0.655   24       \"No\"\n  3         142       80      15        32.4     0.2     63       \"No\"\n  2         128       78      37        43.3     1.224   31       \"Yes\"\n  0         137       40      35        43.1     2.288   33       \"Yes\"\n  9         154       78      30        30.9     0.164   45       \"No\"\n  ⋮                                              ⋮                \n 12         100       84      33        30.0     0.488   46       \"No\"\n  1         147       94      41        49.3     0.358   27       \"Yes\"\n  3         187       70      22        36.4     0.408   36       \"Yes\"\n  1         121       78      39        39.0     0.261   28       \"No\"\n  3         108       62      24        26.0     0.223   25       \"No\"\n  0         181       88      44        43.3     0.222   26       \"Yes\"\n  1         128       88      39        36.5     1.057   37       \"Yes\"\n  2          88       58      26        28.4     0.766   22       \"No\"\n  9         170       74      31        44.0     0.403   43       \"Yes\"\n 10         101       76      48        32.9     0.171   63       \"No\"\n  5         121       72      23        26.2     0.245   30       \"No\"\n  1          93       70      31        30.4     0.315   23       \"No\"\n\n\n\ndt = data[1:end , 1:end-1]\ny = float.(dt[2:end, 2])\nX = float.(dt[2:end, 1:end .!= 2])\nns = data[1,1:end-1]\nns = ns[1:end .!=2]\n\n@proto struct DiabetesModel \n    S = 500 # Number of sampling\n    # Data\n    y \n    X\n    n = length(y) # number of records\n    p = size(X)[2]\n    # Model \n    # Prior\n    g = n # g prior\n    ν₀ = 2.\n    σ₀² = 1.\nend\n\nfunction β_FCD(σ², m::DiabetesModel)\n    return β_FCD(σ², m.g, m.X, m.y)\nend\n\nfunction β_FCD(σ², g, X, y)\n    Σₙ = g/(g+1) * σ² * (X'X)^-1\n    μₙ = g/(g+1) * β̂(σ², y, X)\n    βₙ =  MvNormal(μₙ, Hermitian(Σₙ))\n    return βₙ    \nend\n\nfunction β̂(σ², y, X)\n    return σ² * (X'X)^-1 * (X'y / σ²)\nend\n\n\n\nfunction σ²_FCD(m::DiabetesModel)\n    return σ²_FCD(m.ν₀, m.σ₀², m.n, m.X, m.y, m.g)\nend\n\nfunction σ²_FCD(ν₀, σ₀², n, X, y, g)\n    α = ν₀ + n / 2.\n    θ = (ν₀ * σ₀² + SSR(X, y, g))/2.\n    σ² = InverseGamma(α, θ)\n    return σ²\nend\n\nfunction SSR(m::DiabetesModel)\n    return SSR(m.X, m.y, m.g)\nend\n\nfunction SSR(X, y, g)\n    return y'*(I - g/(g+1)*X*(X'X)^-1*X')*y\nend\n\n\nm = DiabetesModel(y=y, X=X )\nσ²smp = zeros(m.S, 1)\nβsmp = zeros(m.S, size(m.X)[2])\n\nfor i in 1: m.S\n    σ²smp[i] = rand(σ²_FCD(m))\n    βsmp[i,:] = rand(β_FCD(σ²smp[i], m))\nend\n\nps = [histogram(βsmp[:,i], xlabel=\"Quantity\", \n      ylabel=\"Probability\",normalize=true, label=\"$(ns[i])\", color=\"black\") \n      for i in 1:m.p]\nplot(ps...)\n\n\n\n\n\n\n13.4.2 (b)\n\nPerform the model selection and averaging procedure described in Section 9.3. Obtain \\(Pr(\\beta_j \\neq 0 |y)\\), as well as posterior confidence intervals for all of the parameters. Compare to the results in part (a).\n\n\\[\\{y|X_{z(k)},\\beta_{z(k)}, \\sigma^2\\} \\sim \\text{ multivariate normal }(X_{z(k)}\\beta_{z(k)}, \\sigma^2 I)\\]\n\nm = DiabetesModel(y=y, X=X,  S=1000)\n\n\nfunction σ²_FCD(m::DiabetesModel, zs)\n    Xz = @view m.X[1:end, Bool.(zs)]\n    return σ²_FCD(m.ν₀, m.σ₀², m.n, Xz, m.y, m.g)\nend\n\nfunction β_FCD(σ², m::DiabetesModel, zs)\n    Xz  = @view m.X[1:end, Bool.(zs)]\n    return β_FCD(σ², m.g, Xz, m.y)\nend\n\nfunction y_margin(σz², m::DiabetesModel, zs)\n    ν₀ = m.ν₀\n    n = m.n\n    y = m.y \n    g = m.g\n    pz = sum(zs)\n    Xz  = @view m.X[1:end, Bool.(zs)]\n    ssr = SSR(Xz, y, g)\n\n    pyl = -(pz/2.)log(1. +g) + (ν₀/2.)*log(σz²) - ((ν₀+n)/2)*log((ν₀*σz² + ssr))\n    return pyl\nend\n\nfunction z_FCD(i , σz², zsmp, nSmp,m::DiabetesModel)\n    zs = zsmp[nSmp,:]\n    pj1 = sum(zsmp[1:nSmp, i]) / length(zsmp[1:nSmp, i])\n    pj0 = 1. - pj1\n    pj1_FCD_l =  pj1 * y_margin(σz², m, ones(length(zs)))\n    pj0_FCD_l =  pj0 * y_margin(σz², m, zs)\n    O = exp(pj0_FCD_l - pj1_FCD_l)\n    return Bernoulli( 1/(1+O))\nend\n\n\nzsmp = ones(m.S, size(m.X)[2])\nσ²smp = zeros(m.S, 1)\nβsmp = zeros(m.S, size(m.X)[2])\n\nσ²smp[1] = 0.1\n# Gibbs sampling\nfor i in 2:m.S\n    for j in Random.shuffle(1:m.p)\n        zp = z_FCD(j, σ²smp[i-1], zsmp, i-1, m)\n        zsmp[i, j] = rand(zp)\n    end\n\n    σ²smp[i] = rand(σ²_FCD(m, zsmp[i,:]))\n    βsmp[i, Bool.(zsmp[i,:])] = rand(β_FCD(σ²smp[i], m, zsmp[i,:]))\nend\n\n\n\n\nsum(zsmp, dims=1)/size(zsmp)[1]\nprB = 1. .- vec(sum(zsmp, dims=1))./size(zsmp)[1]\nDataFrame(Dict( \"Bi\"=> ns, \"Pr(Bi != 0 |y)\"=> prB ))\n\n\n6×2 DataFrameRowBiPr(Bi != 0 |y)AnyFloat641npreg0.4992bp0.4993skin0.4994bmi0.4995ped0.4996age0.5\n\n\n\ninds = prB .>= 0.5\nb_select = prB[inds]\nps = [histogram(βsmp[:, i], xlabel=\"Quantity\", \n      ylabel=\"Probability\",normalize=true, label=\"$(ns[i])\", color=\"black\") \n      for i in  findall(inds .== 1)]\nplot(ps...)\n\n\n\n\n\nDataFrame(Dict(\"Parameters\"=> ns[inds], \n               \"Confidence interval\"=> [quantile(βsmp[:,i], [0.25, 0.975]) for i in  findall(inds .== 1)]))\n\n\n1×2 DataFrameRowConfidence intervalParametersArray…Any1[0.0, 1.1847]age\n\n\nConclusion\nThere might be some bugs in FCD formulation. The current results show that all the features are equally important after model selection. The distribution is far different from what it is in part (a)."
  },
  {
    "objectID": "hw/hw10.html#descrition",
    "href": "hw/hw10.html#descrition",
    "title": "14  Homework 10",
    "section": "14.1 Descrition",
    "text": "14.1 Descrition\n\nCourse: STAT638, 2022Fall\nDeadline: 2022/10/29, 12:01 pm > Read Chapter 10 in the Hoff book\n\nSource code is shown here: https://stchiu.quarto.pub/stat638__hw10/"
  },
  {
    "objectID": "hw/hw10.html#computational-environment",
    "href": "hw/hw10.html#computational-environment",
    "title": "14  Homework 10",
    "section": "14.2 Computational Environment",
    "text": "14.2 Computational Environment\n\n14.2.1 Libraries\n\ncd(@__DIR__)\nusing Pkg\nPkg.activate(\"hw10\")\n\nusing Statistics \nusing Distributions\nusing LinearAlgebra\nusing KernelDensity\nusing Plots\n\n  Activating project at `~/Documents/GitHub/STAT638_Applied-Bayes-Methods/hw/hw10`\n\n\n\n\n14.2.2 Version\n\nPkg.status()\nVERSION\n\nStatus `~/Documents/GitHub/STAT638_Applied-Bayes-Methods/hw/hw10/Project.toml`\n  [31c24e10] Distributions v0.25.79\n  [5ab0869b] KernelDensity v0.6.5\n⌃ [91a5bcdd] Plots v1.36.3\n  [10745b16] Statistics\nInfo Packages marked with ⌃ have new versions available and may be upgradable.\n\n\nv\"1.8.2\""
  },
  {
    "objectID": "hw/hw10.html#problem-hw10-1",
    "href": "hw/hw10.html#problem-hw10-1",
    "title": "14  Homework 10",
    "section": "14.3 Problem HW10-1",
    "text": "14.3 Problem HW10-1\n\nAssume we have \\(4\\) observations, \\((-1,0,1,10)\\), where the last observation can be thought of as an outlier. Assume that conditional on an unknown parameter \\(\\theta\\), the data area i.i.d. from some population distribution. Assume a standard normal prior for \\(\\theta\\).\n\n\n14.3.1 (1)\n\nFirst, assume that the population distribution is a normal distribution with mean \\(\\theta\\) and variance \\(1\\). Draw samples from the posterior of \\(\\theta\\) using a Metropolis algorithm and also derive the exact posterior in closed form.\n\nExact\n\\[\\theta \\sim Normal(\\mu=0, \\tau^{2}=1)\\] \\[Y \\sim Normal(\\theta, \\sigma^2 = 1)\\]\n\\[\\begin{align}\n  P(\\theta | y, \\sigma^2)%\n  &\\sim Normal(\\mu_n, \\tau_{n}^2)\n\\end{align}\\]\n\n\\(\\mu_n = \\bar{y}\\frac{n/\\sigma^2}{n/\\sigma^2 + 1/\\tau^2} + \\mu \\frac{1/\\tau^2}{n/\\sigma^2 + 1/\\tau^2}\\)\n\\(\\tau^{2}_{n} = \\frac{1}{n/\\sigma^2 + 1/\\tau^2}\\)\n\n\nys = [-1., 0., 1., 10.]\n\n\n\nθ = Normal()\nk = 300000\nδ² = 1.\n\nfunction sampling_ma(like_dist, θ, k, δ²)\n    θs = zeros(k)\n    th = 0.\n    for i in 1:k \n        θᵒ = th\n        J = Normal(θᵒ, δ²)\n        θʷ = rand(J)\n        r_log = sum(logpdf.( like_dist(θʷ, 1), ys) .- logpdf.( like_dist(θᵒ, 1), ys)) + logpdf(θ, θʷ) - logpdf(θ, θᵒ)\n\n        # Accept\n        u_log = log(rand(Uniform()))\n        if r_log > u_log \n            θs[i] = θʷ\n        else \n            θs[i] = θᵒ\n        end \n        th = θs[i]\n    end\n    return θs\nend\n\nθs_n = sampling_ma(Normal, θ, k, δ²)\n\n# Exact PDF\nn = length(ys)\nτₙ =( 1 / (n/ 1 + 1/ 1)^0.5)\nμₙ = mean(ys) * (n / 1) / (n/ 1 + 1/1) + 0 \nθ_exact = Normal(μₙ, τₙ )\nxs = collect(-3:0.01:4)\nys_exact = pdf.(θ_exact, xs)\n\n# Display\np = histogram(θs_n, normalize=:pdf, xlabel=\"θ\", ylabel=\"P(θ|y)\", label=\"Metropolis\", title=\"Normal Model\")\nplot!(p, xs, ys_exact, label=\"Exact PDF\")\n\n\n\n\n\n\n14.3.2 (2)\n\nNow assume the population distribution is a Cauchy distribution with location parameter \\(\\theta\\) and scale \\(1\\). (This is equivalent to a nonstandardized t distribution with one degree of freedom and location parameter \\(\\theta\\).) Draw samples from the posterior using the Metropolis algorithm.\n\n\nθs_c = sampling_ma(Cauchy, θ, k, δ²)\np2 = histogram(θs_c, normalize=:pdf, xlabel=\"θ\", ylabel=\"P(θ|y)\", label=\"Metropolis\", title=\"Cauchy Model\")\n\ndisplay(p2)\n\n\n\n\n\n\n14.3.3 (3)\n\nPlot the exact posterior density from part \\(1\\), together with kernel density estimates from the two Metropolis samplers. Describe how the outlier has affected the posteriors.\n\nCauchy distribution as likelihood distribution is less sensitive to the outliers than Normal distribution.\n\nUn = kde(θs_n)\nUc = kde(θs_c)\npdf_n = [pdf(Un, x) for x in xs]\npdf_c = [pdf(Uc, x) for x in xs]\np3 = plot(xlabel=\"θ\", ylabel=\"P(θ|y)\", title=\"Pop dist. of Cuachy and Normal\")\nplot!(p3, xs, pdf_n, label=\"Normal\")\nplot!(p3, xs, pdf_c, label=\"Cauchy\")\nplot!(p3, xs, ys_exact, label=\"Exact PDF (Normal)\")\nscatter!(p3, ys, zeros(length(ys)), label=\"data\")\ndisplay(p3)"
  },
  {
    "objectID": "ref.html",
    "href": "ref.html",
    "title": "References",
    "section": "",
    "text": "Diaconis, Persi, and Donald Ylvisaker. 1985. “Quantifying Prior\nOpinion, Bayesian Statistics. Vol. 2.” North Holland Amsterdam:\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical\nMethods. Vol. 580. Springer."
  }
]