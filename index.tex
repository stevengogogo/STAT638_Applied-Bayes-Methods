% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={STAT638: Applied Bayesian Methods},
  pdfauthor={Shao-Ting Chiu},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{STAT638: Applied Bayesian Methods}
\author{Shao-Ting Chiu}
\date{8/25/2022}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[frame hidden, interior hidden, breakable, borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, sharp corners, enhanced]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

This is the lecture notes for STAT638 Applied Bayesian Methods by
Dr.~Matthias Katzfuss.

\hypertarget{course-details}{%
\section*{Course details}\label{course-details}}
\addcontentsline{toc}{section}{Course details}

\begin{itemize}
\tightlist
\item
  Course Number: STAT 638
\item
  Course Title: Introduction to Applied Bayesian Methods
\item
  Time: TuTh 2:20 - 3:35 (Central time)
\item
  Location: Blocker 411
\item
  Textbook: Hoff, (Links to an external site.)A First Course in Bayesian
  Statistical Methods (Links to an external site.) (electronic version
  available through TAMU library)
\item
  Prerequisite: STAT 630 or STAT 650. (Also, familiarity with R or other
  statistical software, training in vector/matrix algebra, and some
  exposure to linear regression will be very helpful.)
\end{itemize}

\hypertarget{schedule}{%
\section*{Schedule}\label{schedule}}
\addcontentsline{toc}{section}{Schedule}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Introduction (Week 1)
\item
  Conditional distributions and Bayes rule (Weeks 1-2)
\item
  One-parameter models (Weeks 3-4)
\item
  Monte Carlo approximation (Weeks 5-6)
\item
  The normal model (Weeks 6-8)
\item
  Gibbs sampling (Weeks 8-9)
\item
  The multivariate normal model (Weeks 9-11)
\item
  Group comparisons and hierarchical modeling (Weeks 11-12)
\item
  Linear regression (Weeks 12-13)
\item
  Markov chain Monte Carlo (Weeks 13-14)
\item
  Mixed effects models (Week 14)
\end{enumerate}

\hypertarget{programming}{%
\section*{Programming}\label{programming}}
\addcontentsline{toc}{section}{Programming}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Majorly use R
\item
  Homework will be R
\item
  Using Python is acceptable
\end{enumerate}

\hypertarget{exam}{%
\section*{Exam}\label{exam}}
\addcontentsline{toc}{section}{Exam}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Exam 1: October 18th
\item
  Exam 2: December 1st
\end{enumerate}

\hypertarget{homework}{%
\section*{Homework}\label{homework}}
\addcontentsline{toc}{section}{Homework}

\begin{itemize}
\tightlist
\item
  \emph{No late homework is acceptable}
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{chapter-1-introduction-and-examples}{%
\chapter{Chapter 1 Introduction and
examples}\label{chapter-1-introduction-and-examples}}

Shao-Ting Chiu (UIN:433002162)\\
today

\hfill\break

\begin{itemize}
\item
  \href{https://tamucs-my.sharepoint.com/:b:/r/personal/stchiu_tamu_edu/Documents/2022Fall/STAT638_BayesMethod/chap01.pdf?csf=1\&web=1\&e=lX1sgw}{Slides}
\item
  Hoff (2009, Ch 1)
\item
  Bayes's rule provides a rational method for updating beliefs in light
  of new information.
\end{itemize}

\hypertarget{what-can-bayesian-methods-provide}{%
\section{What can Bayesian methods
provide?}\label{what-can-bayesian-methods-provide}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Parameter estimates with good statistical properties
\item
  Parsimonious descriptions of observed data
\end{enumerate}

\hypertarget{contrast-between-frequentist-and-bayesian-statistics}{%
\section{Contrast between Frequentist and Bayesian
Statistics}\label{contrast-between-frequentist-and-bayesian-statistics}}

\begin{itemize}
\tightlist
\item
  Frequentist statistics

  \begin{itemize}
  \tightlist
  \item
    Uncertainty about the parameter estimates
  \end{itemize}
\item
  Bayesian statistics

  \begin{itemize}
  \tightlist
  \item
    Unvertainty is quantified by the oberservation of data.
  \end{itemize}
\end{itemize}

\hypertarget{bayesian-learning}{%
\section{Bayesian learning}\label{bayesian-learning}}

\begin{itemize}
\tightlist
\item
  Parameter --- \(\theta\)

  \begin{itemize}
  \tightlist
  \item
    numerical values of population characteristics
  \end{itemize}
\item
  Dataset --- \(y\)

  \begin{itemize}
  \tightlist
  \item
    After a dataset \(y\) is obtained, the information it contains can
    be used to \textbf{decrease our uncertainty} about the population
    characteristics.
  \end{itemize}
\item
  Bayesian inference

  \begin{itemize}
  \tightlist
  \item
    Quantifying this change in uncertainty is the purpose of Bayesian
    inference
  \end{itemize}
\item
  Sample space --- \(\mathcal{Y}\)

  \begin{itemize}
  \tightlist
  \item
    The set of all possible datasets.
  \item
    Single dataset \(y\)
  \end{itemize}
\item
  Parameter space --- \(\Theta\)

  \begin{itemize}
  \tightlist
  \item
    possbile parameter values
  \item
    we hope to identify the value that best represents the true
    population characteristics.
  \end{itemize}
\item
  Bayesian learning begins with joint beliefs about \(y\) and
  \(\theta\), in terms of distribution over \(\mathcal{Y}\) and
  \(\Theta\)

  \begin{itemize}
  \tightlist
  \item
    \emph{Prior distribution} --- \(p(\theta)\)

    \begin{itemize}
    \tightlist
    \item
      Our belief that \(\theta\) represents that true population
      characteristics.
    \end{itemize}
  \item
    \emph{Sampling model} --- \(p(\mathcal{y}|\theta)\)

    \begin{itemize}
    \tightlist
    \item
      describes our belief that \(\mathcal{y}\) would be the outcome of
      our study if we knew \(\theta\) to be true.
    \end{itemize}
  \item
    \emph{Posterior distribution} --- \(p(\theta|\mathcal{y})\)

    \begin{itemize}
    \tightlist
    \item
      Our belief that \(\theta\) is the true value, having observed
      dataset \(\mathcal{y}\)
    \end{itemize}
  \item
    \emph{Bayesian Update} (Equation~\ref{eq-bayes-update})
    \begin{equation}\protect\hypertarget{eq-bayes-update}{}{p(\theta|y) = \frac{\overbrace{p(y|\theta)}^{\text{Sampling model}}\overbrace{p(\theta)}^{\text{Prior distribution}}}{\int_{\Theta}p(y|\tilde{\theta})p(\tilde{\theta})d\tilde{\theta}}}\label{eq-bayes-update}\end{equation}

    \begin{itemize}
    \tightlist
    \item
      Bayes's rule tells us how to change our belief after seeing new
      information.
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{example}{%
\section{Example}\label{example}}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, coltitle=black, titlerule=0mm, colbacktitle=quarto-callout-tip-color!10!white, breakable, opacityback=0, toptitle=1mm, opacitybacktitle=0.6, leftrule=.75mm, left=2mm, toprule=.15mm, colframe=quarto-callout-tip-color-frame, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Beta distribution\footnote{Beta Distribution.
  {[}\href{https://en.wikipedia.org/wiki/Beta_distribution}{Wiki}{]}}}, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colback=white]

\begin{itemize}
\item
  Notation: \(Beta(\alpha, \beta)\)
\item
  Parameters:

  \begin{itemize}
  \tightlist
  \item
    \(\alpha > 0\)
  \item
    \(\beta > 0\)
  \end{itemize}
\item
  Support:

  \begin{itemize}
  \tightlist
  \item
    \(x\in [0,1]\)
  \end{itemize}
\item
  PDF

  \[p(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}\]

  where

  \begin{itemize}
  \tightlist
  \item
    \(B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}\)
  \item
    \(\Gamma(\alpha)\) is a gamma function

    \begin{itemize}
    \tightlist
    \item
      \(\Gamma(\alpha) = (\alpha-1)!\), \(\alpha\) is a positive
      interger\footnote{Gamma function.
        {[}\href{https://en.wikipedia.org/wiki/Gamma_function}{Wiki}{]}}
    \end{itemize}
  \end{itemize}
\item
  Mean: \(E[X] = \frac{\alpha}{\alpha + \beta}\)
\item
  Variancd:
  \(var[X] = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\)
\item
  Bayesian inference

  \begin{itemize}
  \tightlist
  \item
    The use of Beta distribution in Bayesian inference provide a family
    of conjugate prior probability disbritions for binomial and
    geometric dictritutions.
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\bookmarksetup{startatroot}

\hypertarget{chapter-2-conditional-distributions-and-bayes-rule}{%
\chapter{Chapter 2: Conditional distributions and Bayes
rule}\label{chapter-2-conditional-distributions-and-bayes-rule}}

Shao-Ting Chiu\\
20220830

\hfill\break

Let \(F\), \(G\) and \(H\) be three possibly overlapping statements.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  0 = Pr(not H\textbar H) \(\leq\) Pr(F\textbar H) \(\leq\)
  Pr(H\textbar H) = 1
\item
  Pr(F\(\cup\)G\textbar H) = Pr(F\textbar H) + Pr(G\textbar H) if
  \(F\cap G=\emptyset\)
\item
  \(Pr(F\cap G|H)=Pr(G|H)Pr(F|G\cap H)\)
\end{enumerate}

\hypertarget{events-and-partition}{%
\section{Events and partition}\label{events-and-partition}}

\begin{itemize}
\tightlist
\item
  Sample space \(S\)
\item
  Partition

  \begin{itemize}
  \tightlist
  \item
    a collection of sets \(A_1,\dots, A_m\)
  \item
    \(A_{i} \cap A_j = \emptyset\)
  \end{itemize}
\item
  Conditional probability

  \begin{itemize}
  \tightlist
  \item
    Let \(B\) be an event, and \(A_i,\dots,A_m\) be a partition of \(S\)
  \item
    \(P(B|A_i) = \frac{P(B\cap A_i)}{P(A_i)}\)
  \end{itemize}
\item
  Bayes rule

  \begin{itemize}
  \tightlist
  \item
    \(P(A_j |B) = \frac{P(B|A_j)P(A_j)}{P(B)} = \frac{P(B|A_j)P(A_j)}{\sum^{m}_{i=1}P(B|A_i)P(Ai)}\)
  \end{itemize}
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, coltitle=black, titlerule=0mm, colbacktitle=quarto-callout-tip-color!10!white, breakable, opacityback=0, toptitle=1mm, opacitybacktitle=0.6, leftrule=.75mm, left=2mm, toprule=.15mm, colframe=quarto-callout-tip-color-frame, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Example: COVID test}, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colback=white]

\begin{quote}
A college with a Covid-19 prevalencde of 10\% is using a test that is
positive with probability 90\% if an individual is infected, and
positive with probability of 5\% if the individual is not.
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  For a randomly selected student at the college, what is the
  probability that the test will be positive?
\end{itemize}
\end{quote}

\[\begin{align}
P(+) &= P(+|C)P(C) + P(+|H)P(H)\\
     &= 0.9*0.1 + 0.05*0.9\\
     &= 0.135
\end{align}\]

\begin{quote}
\begin{itemize}
\tightlist
\item
  Give that a student has tested positive, what is the probability the
  student is actually infected?
\end{itemize}
\end{quote}

\$\$\begin{align}



P(C|+) &= \frac{P(+|C)P(C)}{P(+)}\\
       &= \frac{0.9*0.1}{0.135}\\ 
       &= 0.67
\end{align}\$\$

\end{tcolorbox}

\hypertarget{random-variables-and-univariate-distributions}{%
\section{Random variables and univariate
distributions}\label{random-variables-and-univariate-distributions}}

\begin{longtable}[]{@{}lll@{}}
\caption{Random variable is an unknown quantity characterized by a
probability distribution}\tabularnewline
\toprule()
RV & Discrete & Continuous \\
\midrule()
\endfirsthead
\toprule()
RV & Discrete & Continuous \\
\midrule()
\endhead
Outcome \(y\) & countable & uncountable \\
prop. of pdf & \(0\leq p(y) \leq1\) & \(0\leq p(y)\) \\
& \(\sum_{y\in Y}p(y)=1\) & \(\int_{Y}p(y)dy = 1\) \\
cdf \(F(a)\) & \(F(a) = \sum_{y\leq a}p(y)\) &
\(F(a)=\int^{a}_{-\infty}p(y)dy\) \\
mean & \(E(Y)=\sum_{y\in Y}p(y)\) & \(E(Y)=\int_{Y}y p(y)dy\) \\
\bottomrule()
\end{longtable}

\begin{itemize}
\tightlist
\item
  CDF: \(F(a) = P(Y\leq a)\)
\item
  Variance: \(Var(Y) = E(Y-E(Y))^2 = E(Y^2) - (E(Y))^2\)
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, coltitle=black, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, breakable, opacityback=0, toptitle=1mm, opacitybacktitle=0.6, leftrule=.75mm, left=2mm, toprule=.15mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Binomial distribution}, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colback=white]
\[p(Y=y|\theta) = dbinom(y,n,\theta) = {n\choose y} \theta^y (1-\theta)^{n-y}\]
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, coltitle=black, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, breakable, opacityback=0, toptitle=1mm, opacitybacktitle=0.6, leftrule=.75mm, left=2mm, toprule=.15mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Poisson distribution}, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colback=white]
Let \(\mathcal{Y} = \{0,1,2,\dots\}\). The uncertain quantity
\(Y\in \mathcal{Y}\) has a \emph{poisson distribution with mean
\(\theta\)} if

\[p(Y=y|\theta) = dpois(y,\theta) = \theta^{y}\frac{e^{-\theta}}{y!}\]
\end{tcolorbox}

\hypertarget{description-of-distributions}{%
\section{Description of
distributions}\label{description-of-distributions}}

\begin{itemize}
\tightlist
\item
  Expection

  \begin{itemize}
  \tightlist
  \item
    \(E[Y] = \sum_{y\in\mathcal{Y}yp(y)}\) if \(Y\) is discrete.
  \item
    \(E[Y] = \int_{y\in\mathcal{Y}yp(y)}\) if \(Y\) is discrete.
  \end{itemize}
\item
  Mode

  \begin{itemize}
  \tightlist
  \item
    The most probable value of \(Y\)
  \end{itemize}
\item
  Median

  \begin{itemize}
  \tightlist
  \item
    The value of \(Y\) in the middle of the distribution
  \end{itemize}
\item
  Variance \[\begin{align}
       Var[Y] &= E[(Y-E[Y])^2]\\
              &= E[Y^2-2YE[Y] + E[Y]^2]\\
              &= E[Y^2] - 2E[Y]^2 + E[Y]^2\\ 
              &= E[Y^2] - E[Y]^2
   \end{align}\]
\end{itemize}

\hypertarget{joint-distribution}{%
\section{Joint distribution}\label{joint-distribution}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\caption{Marginal}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Discrete
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Continuous
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Discrete
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Continuous
\end{minipage} \\
\midrule()
\endhead
\(P_{y1}(y_1)=\sum_{y_2\in Y_2}p_{Y_1, Y_2}(y_1, y_2)\) &
\(p_{Y_1}(y_1) = \int_{y_2}p_{Y_1,Y_2}(y_1,y_2)dy_2\) \\
\bottomrule()
\end{longtable}

\begin{itemize}
\tightlist
\item
  Conditional:
  \(p_{Y_2|Y_1}(y_2|y_1) = \frac{p_{Y_1,Y_2}(y_1,y_2)}{p_{Y_1}(y_1)}\)
\end{itemize}

\hypertarget{proportionality}{%
\section{Proportionality}\label{proportionality}}

\begin{itemize}
\tightlist
\item
  A function \(f(x)\) is proportional to \(g(x)\), denoted by
  \(f(x) \propto g(x)\)

  \begin{itemize}
  \tightlist
  \item
    \[f(x) = cg(x)\]
  \end{itemize}
\end{itemize}

\hypertarget{a-bayesian-model}{%
\section{A Bayesian model}\label{a-bayesian-model}}

\begin{itemize}
\tightlist
\item
  Random vector of data --- \(Y\)
\item
  Probability distribution of \(Y\) --- \(p(y|\theta)\)
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, coltitle=black, titlerule=0mm, colbacktitle=quarto-callout-tip-color!10!white, breakable, opacityback=0, toptitle=1mm, opacitybacktitle=0.6, leftrule=.75mm, left=2mm, toprule=.15mm, colframe=quarto-callout-tip-color-frame, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Bayes theorem applied to statistical model}, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colback=white]
\[p(\theta|y) = \frac{p(y,\theta)}{m(y)} = \frac{p(y|\theta)p(\theta)}{\int_{\Theta}p(y|\theta)p(\theta)d\theta}\]

\begin{itemize}
\tightlist
\item
  \(p(\theta)\): the prior distribution
\item
  \(\Theta\): parameter space
\item
  \(p(y|\theta)\): the likelihood function.
\item
  \(p(\theta|y)\): the posterior distribution
\item
  \(m\): marginal distribution of \(Y\)
\end{itemize}

The \textbf{posterior distribution} expresses the experimenter's updated
beliefs about \(\theta\) in light of the observed data \(y\).
\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  \(m(y)\): doesn't depend on \(\theta\).
\end{itemize}

\hypertarget{conditional-independence-and-exhangeability}{%
\section{Conditional independence and
Exhangeability}\label{conditional-independence-and-exhangeability}}

\begin{tcolorbox}[enhanced jigsaw, bottomtitle=1mm, coltitle=black, titlerule=0mm, colbacktitle=quarto-callout-important-color!10!white, breakable, opacityback=0, toptitle=1mm, opacitybacktitle=0.6, leftrule=.75mm, left=2mm, toprule=.15mm, colframe=quarto-callout-important-color-frame, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Conditional independence}, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colback=white]

\[P(A\cap B|C) = P(A|C)P(B|C)\]

\begin{itemize}
\tightlist
\item
  Two events \(A\) and \(B\) are conditionally independent given event
  \(C\)

  \begin{itemize}
  \tightlist
  \item
    \(A\perp B|C\)
  \end{itemize}
\end{itemize}

\begin{quote}
Knowing \(C\) and \(B\) gives no more information about \(A\) thatn does
\(C\) by itself.
\end{quote}

\begin{itemize}
\tightlist
\item
  \[P(A|C\perp B) = P(A|C)\]
\end{itemize}

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  Conditional independence

  \begin{itemize}
  \tightlist
  \item
    Let \(Y_1,\dots,Y_n\) are conditionally indep. given \(\theta\). for
    every collection \(A_1,\dots,A_n\) of sets:
  \item
    \[P(Y_1\in A_1,\dots, Y_n\in A_n |\theta) = \amalg^{n}_{i=1} P(Y_i\in A_i |\theta)\]
  \end{itemize}
\end{itemize}

\hypertarget{exchangeability}{%
\subsection{Exchangeability}\label{exchangeability}}

\[p(y_1,\dots,y_n) = p(y_{\pi_1},\dots,y_{\pi_n})\]

for all permutations \(\pi\) of \(\{1,\dots,n\}\)

\begin{quote}
If we think of \(Y_1,\dots,Y_n\) as data, exchangeability says that the
\emph{ordering} of the data conveys no extra information than that in
the observations themselves.
\end{quote}

\begin{itemize}
\item
  For example: time-series of weather is not exhangeable
\item
  i.i.d. data is exchangeable.
\item
  exchangeable does \emph{not} imply \textbf{unconditional independence}
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{homework-1}{%
\chapter{Homework 1}\label{homework-1}}

Shao-Ting Chiu (UIN:433002162)\\
today

\hfill\break

\begin{quote}
Read Chapters 1 and 2 in the Hoff book. Then, do the following exercises
in Hoff (p.~225-226): 2.1, 2.2, 2.3, 2.5 You must turn in your solutions
as a pdf file here on Canvas. Use the Submit Assignment button on the
top right. If your solutions are on paper, please scan them to pdf using
a scanner or a scanner app on your phone. Please do not take a regular
photo, as this can result in very large file sizes. Make sure that
everything is legible. Please note that late homework will not be
accepted and will result in a score of zero. To avoid late submissions
due to technical issues, we recommend turning in your homework the night
before the due date.
\end{quote}

\begin{itemize}
\tightlist
\item
  Deadline: Sep.~8 by 12:01pm
\end{itemize}

\hypertarget{problem-2.1}{%
\section{Problem 2.1}\label{problem-2.1}}

Marginal and conditional probability: The social mobility data from
Section 2.5 gives a joint probability distribution on \((Y_1, Y_2)=\)
(fathers's occupation, son's occupation)

\hypertarget{tbl-social}{}
\begin{longtable}[]{@{}llllll@{}}
\caption{\label{tbl-social}The social mobility data (Hoff 2009,
580:24)}\tabularnewline
\toprule()
& & & son's occupation & & \\
\midrule()
\endfirsthead
\toprule()
& & & son's occupation & & \\
\midrule()
\endhead
\textbf{father's occupation} & farm & operatives & craftsmen & sales &
professional \\
farm & 0.018 & 0.035 & 0.031 & 0.008 & 0.018 \\
operatives & 0.002 & 0.112 & 0.064 & 0.032 & 0.069 \\
craftsman & 0.001 & 0.066 & 0.094 & 0.032 & 0.084 \\
sales & 0.001 & 0.018 & 0.019 & 0.010 & 0.051 \\
professional & 0.001 & 0.029 & 0.032 & 0.043 & 0.130 \\
\bottomrule()
\end{longtable}

\hypertarget{a-the-marginal-probability-distribution-of-a-fathers-occupation}{%
\subsection{(a) The marginal probability distribution of a father's
occupation}\label{a-the-marginal-probability-distribution-of-a-fathers-occupation}}

According to Table~\ref{tbl-social}, let \(\mathbb{Y_1}\) and
\(\mathbb{Y_2}\) be sets of father's and son's occupations:

\[\mathbb{Y_1} = \mathbb{Y_2} = \{\text{farm},\text{operatives},\text{craftsmen},\text{sales},\text{professional}\}\]

\[
\begin{aligned}
    Pr(Y_1 = \text{farm}) &= \sum_{y_2\in\mathbb{Y_2}}Pr(Y_1= \text{farm} \cap Y_2=y_2)\\
    &= Pr(Y_1= \text{farm} \cap Y_2=\text{farm}) + Pr(Y_1= \text{farm} \cap Y_2=\text{operatives}) \\
    & +Pr(Y_1= \text{farm} \cap Y_2=\text{craftsmen}) + Pr(Y_1= \text{farm} \cap Y_2=\text{sales}) \\ 
    & +Pr(Y_1= \text{farm} \cap Y_2=\text{professional})\\
    &= 0.018 + 0.035 + 0.031 + 0.008 + 0.018\\ 
    &= 0.11
\end{aligned}
\]

\[
\begin{aligned}
    Pr(Y_1 = \text{operatives}) &= \sum_{y_2\in\mathbb{Y_2}}Pr(Y_1= \text{operatives} \cap Y_2=y_2)\\
    &= Pr(Y_1= \text{operatives} \cap Y_2=\text{farm}) + Pr(Y_1= \text{operatives} \cap Y_2=\text{operatives}) \\
    & +Pr(Y_1= \text{operatives} \cap Y_2=\text{craftsmen}) + Pr(Y_1= \text{operatives} \cap Y_2=\text{sales}) \\ 
    & +Pr(Y_1= \text{operatives} \cap Y_2=\text{professional})\\
    &= 0.002 + 0.112 + 0.064 + 0.032 + 0.069\\ 
    &= 0.279
\end{aligned}
\]

\[
\begin{aligned}
    Pr(Y_1 = \text{craftsman}) &= \sum_{y_2\in\mathbb{Y_2}}Pr(Y_1= \text{craftsman} \cap Y_2=y_2)\\
    &= Pr(Y_1= \text{craftsman} \cap Y_2=\text{farm}) + Pr(Y_1= \text{craftsman} \cap Y_2=\text{operatives}) \\
    & +Pr(Y_1= \text{craftsman} \cap Y_2=\text{craftsmen}) + Pr(Y_1= \text{craftsman} \cap Y_2=\text{sales}) \\ 
    & +Pr(Y_1= \text{craftsman} \cap Y_2=\text{professional})\\
    &= 0.001+0.066+0.094+0.032+0.084\\ 
    &= 0.277
\end{aligned}
\]

\[
\begin{aligned}
    Pr(Y_1 = \text{sales}) &= \sum_{y_2\in\mathbb{Y_2}}Pr(Y_1= \text{sales} \cap Y_2=y_2)\\
    &= Pr(Y_1= \text{sales} \cap Y_2=\text{farm}) + Pr(Y_1= \text{sales} \cap Y_2=\text{operatives}) \\
    & +Pr(Y_1= \text{sales} \cap Y_2=\text{craftsmen}) + Pr(Y_1= \text{sales} \cap Y_2=\text{sales}) \\ 
    & +Pr(Y_1= \text{sales} \cap Y_2=\text{professional})\\
    &= 0.001 + 0.018 + 0.019 + 0.010 + 0.051\\ 
    &= 0.099
\end{aligned}
\]

\[
\begin{aligned}
    Pr(Y_1 = \text{professional}) &= \sum_{y_2\in\mathbb{Y_2}}Pr(Y_1= \text{professional} \cap Y_2=y_2)\\
    &= Pr(Y_1= \text{professional} \cap Y_2=\text{farm}) + Pr(Y_1= \text{professional} \cap Y_2=\text{operatives}) \\
    & +Pr(Y_1= \text{professional} \cap Y_2=\text{craftsmen}) + Pr(Y_1= \text{professional} \cap Y_2=\text{sales}) \\ 
    & +Pr(Y_1= \text{professional} \cap Y_2=\text{professional})\\
    &= 0.001 + 0.029 + 0.032 + 0.043 + 0.130\\ 
    &= 0.235
\end{aligned}
\]

\hypertarget{tbl-father}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-father}Marginal probability of father's
occupation}\tabularnewline
\toprule()
marginal probability & value \\
\midrule()
\endfirsthead
\toprule()
marginal probability & value \\
\midrule()
\endhead
\(p(Y_1=\text{farm})\) & 0.11 \\
\(p(Y_1=\text{operatives})\) & 0.279 \\
\(p(Y_1=\text{craftsmen})\) & 0.277 \\
\(p(Y_1=\text{sales})\) & 0.099 \\
\(p(Y_1=\text{professional})\) & 0.235 \\
\emph{SUM} & 1.0 \\
\bottomrule()
\end{longtable}

Table~\ref{tbl-father} shows that the sum of marginal probability is
\(1\).

\hypertarget{b-the-marginal-probability-distribution-of-a-sons-occupation}{%
\paragraph{(b) The marginal probability distribution of a son's
occupation}\label{b-the-marginal-probability-distribution-of-a-sons-occupation}}

\[
\begin{aligned}
    Pr(Y_2 = \text{farm}) &= \sum_{y_1\in\mathbb{Y_1}}Pr(Y_2= \text{farm} \cap Y_1=y_1)\\
    &= Pr(Y_2= \text{farm} \cap Y_1=\text{farm}) + Pr(Y_2= \text{farm} \cap Y_1=\text{operatives}) \\
    & +Pr(Y_2= \text{farm} \cap Y_1=\text{craftsmen}) + Pr(Y_2= \text{farm} \cap Y_1=\text{sales}) \\ 
    & +Pr(Y_2= \text{farm} \cap Y_1=\text{professional})\\
    &= 0.018 + 0.002 + 0.001 + 0.001 + 0.001\\ 
    &= 0.023
\end{aligned}
\]

\[
\begin{aligned}
    Pr(Y_2 = \text{operatives}) &= \sum_{y_1\in\mathbb{Y_1}}Pr(Y_2= \text{operatives} \cap Y_1=y_1)\\
    &= Pr(Y_2= \text{operatives} \cap Y_1=\text{farm}) + Pr(Y_2= \text{operatives} \cap Y_1=\text{operatives}) \\
    & +Pr(Y_2= \text{operatives} \cap Y_1=\text{craftsmen}) + Pr(Y_2= \text{operatives} \cap Y_1=\text{sales}) \\ 
    & +Pr(Y_2= \text{operatives} \cap Y_1=\text{professional})\\
    &= 0.035 + 0.112 + 0.066 + 0.018 + 0.029\\ 
    &= 0.26
\end{aligned}
\]

\[
\begin{aligned}
    Pr(Y_2 = \text{craftsmen}) &= \sum_{y_1\in\mathbb{Y_1}}Pr(Y_2= \text{craftsmen} \cap Y_1=y_1)\\
    &= Pr(Y_2= \text{craftsmen} \cap Y_1=\text{farm}) + Pr(Y_2= \text{craftsmen} \cap Y_1=\text{operatives}) \\
    & +Pr(Y_2= \text{craftsmen} \cap Y_1=\text{craftsmen}) + Pr(Y_2= \text{craftsmen} \cap Y_1=\text{sales}) \\ 
    & +Pr(Y_2= \text{craftsmen} \cap Y_1=\text{professional})\\
    &= 0.031 + 0.064 + 0.094 + 0.019 + 0.032\\ 
    &= 0.24
\end{aligned}
\]

\[
\begin{aligned}
    Pr(Y_2 = \text{sales}) &= \sum_{y_1\in\mathbb{Y_1}}Pr(Y_2= \text{sales} \cap Y_1=y_1)\\
    &= Pr(Y_2= \text{sales} \cap Y_1=\text{farm}) + Pr(Y_2= \text{sales} \cap Y_1=\text{operatives}) \\
    & +Pr(Y_2= \text{sales} \cap Y_1=\text{craftsmen}) + Pr(Y_2= \text{sales} \cap Y_1=\text{sales}) \\ 
    & +Pr(Y_2= \text{sales} \cap Y_1=\text{professional})\\
    &= 0.008+0.032+0.032+0.010+0.043\\ 
    &= 0.125
\end{aligned}
\]

\[
\begin{aligned}
    Pr(Y_2 = \text{professional}) &= \sum_{y_1\in\mathbb{Y_1}}Pr(Y_2= \text{professional} \cap Y_1=y_1)\\
    &= Pr(Y_2= \text{professional} \cap Y_1=\text{farm}) + Pr(Y_2= \text{professional} \cap Y_1=\text{operatives}) \\
    & +Pr(Y_2= \text{professional} \cap Y_1=\text{craftsmen}) + Pr(Y_2= \text{professional} \cap Y_1=\text{sales}) \\ 
    & +Pr(Y_2= \text{professional} \cap Y_1=\text{professional})\\
    &= 0.018+ 0.069+0.084+0.051+0.130\\ 
    &= 0.352
\end{aligned}
\]

\hypertarget{tbl-son}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-son}Marginal probability of son's
occupation}\tabularnewline
\toprule()
marginal probability & value \\
\midrule()
\endfirsthead
\toprule()
marginal probability & value \\
\midrule()
\endhead
\(p(Y_2=\text{farm})\) & 0.023 \\
\(p(Y_2=\text{operatives})\) & 0.26 \\
\(p(Y_2=\text{craftsmen})\) & 0.24 \\
\(p(Y_2=\text{sales})\) & 0.125 \\
\(p(Y_2=\text{professional})\) & 0.352 \\
\emph{SUM} & 1.0 \\
\bottomrule()
\end{longtable}

Table~\ref{tbl-son} shows that the sum of marginal probability is \(1\).

\hypertarget{c-the-conditional-distribution-of-a-sons-occupation-given-that-the-father-is-a-farmer}{%
\subsection{(c) The conditional distribution of a son's occupation,
given that the father is a
farmer;}\label{c-the-conditional-distribution-of-a-sons-occupation-given-that-the-father-is-a-farmer}}

The conditional distribution of a son's occupation can be expressed as
\(p(y_2 | y_1 = \text{farmer})\).

\[p(y_2= * | y_1 = \text{farmer}) = \frac{p(y_1=\text{farm} \cap y_2 = *)}{p(y_1 = \text{farm})}\]

where \(* \in \mathbb{Y_2}\). As described in Table~\ref{tbl-father},
\(p(y_1=\text{farm})= 0.11\). Use Table~\ref{tbl-social} to calculate
the distribution:

\[
\begin{aligned}
    p(y_2 = \text{farm} | y_1 = \text{farm}) &= \frac{0.018}{0.11} \approx 0.16\\
    p(y_2 = \text{operatives} | y_1 = \text{farm}) &= \frac{0.035}{0.11} \approx 0.32\\
    p(y_2 = \text{craftsman} | y_1 = \text{farm}) &= \frac{0.031}{0.11} \approx 0.28\\
    p(y_2 = \text{sales} | y_1 = \text{farm}) &= \frac{0.008}{0.11} \approx 0.072\\
    p(y_2 = \text{professional} | y_1 = \text{farm}) &= \frac{0.018}{0.11} \approx 0.16 \\
\end{aligned}
\]

\hypertarget{d-the-conditional-distribution-of-a-fathers-occupation-given-that-the-son-is-a-farmer.}{%
\subsection{(d) The conditional distribution of a father's occupation,
given that the son is a
farmer.}\label{d-the-conditional-distribution-of-a-fathers-occupation-given-that-the-son-is-a-farmer.}}

\[p(y_1= * | y_2 = \text{farm}) = \frac{p(y_1=\text{*} \cap y_2 = \text{farm})}{p(y_2 = \text{farm})}\]

According to Table~\ref{tbl-son}, \(p(y_2 = \text{farm}) = 0.023\). Use
Table~\ref{tbl-social} to calculate the distribution:

\[
\begin{aligned}
    p(y_1 = \text{farm} | y_2 = \text{farm}) &= \frac{0.018}{0.023} \approx 0.78\\
    p(y_1 = \text{operatives} | y_2 = \text{farm}) &=\frac{0.002}{0.023} \approx 0.09\\
    p(y_1 = \text{craftsman} | y_2 = \text{farm}) &= \frac{0.001}{0.023} \approx 0.04\\
    p(y_1 = \text{sales} | y_2 = \text{farm}) &= \frac{0.001}{0.023} \approx 0.04\\
    p(y_1 = \text{professional} | y_2 = \text{farm}) &= \frac{0.001}{0.023} \approx 0.04\\
\end{aligned}
\]

\hypertarget{problem-2.2}{%
\section{Problem 2.2}\label{problem-2.2}}

\begin{quote}
Expectations and variances: Let \(Y_1\) and \(Y_2\) be two independent
random variables, such that \(E[Y_i]=\mu_i\) and
\(Var[Y_i] = \sigma_{i}^2\). Using the definition of expectation and
variance, computing the following quantities, where \(a_1\) and \(a_2\)
are given constants.
\end{quote}

\hypertarget{a-ea_1-y_1-a_2-y_2-vara_1-y_1-a_2-y_2}{%
\subsection{\texorpdfstring{(a) \(E[a_1 Y_1 +a_2 Y_2]\),
\(Var[a_1 Y_1 + a_2 Y_2]\)}{(a) E{[}a\_1 Y\_1 +a\_2 Y\_2{]}, Var{[}a\_1 Y\_1 + a\_2 Y\_2{]}}}\label{a-ea_1-y_1-a_2-y_2-vara_1-y_1-a_2-y_2}}

Because \(Y_1\) and \(Y_2\) are independent:

\[
E[Y_1 Y_2] = E[Y_1] E[Y_2]
\]

Thus

\[
\begin{aligned}
  E[a_1 Y_1 + a_2 Y_2] &= E[a_1 Y_1] + E[a_2 Y_2]\\
                       &= a_1 E[Y_1] + a_2 E[Y_2]\\
                       &= \underline{a_1 \mu_1 + a_2 \mu_{2}}\\
\end{aligned}
\]

\[
\begin{aligned}
    Var[a_1 Y_1 + a_2 Y_2] &= E[ [(a_1 Y_1 + a_2 Y_2) - E[a_1 Y_1 + a_2 Y_2]]^{2}]\\
    &= E[(a_1 Y_1 + a_2 Y_2)^2] - E[a_1 Y_1 + a_2 Y_2]^2\\
    &= E[a_{1}^{2}Y_{1}^{2} + 2a_1 a_2 Y_1 Y_2 + a_{2}^{2}Y_{2}^{2}] - (a_1 \mu_1 + a_2 \mu_2)^2\\
    &= a^{2}_{1}\sigma^{2}_{1} + 2a_1 a_2 \underbrace{\mu_1 \mu_2}_{E[Y_1 Y_2]=E[Y_1]E[Y_2]} + a_{2}^{2}\sigma^{2}_{2} - (a_1 \mu_1 + a_2 \mu_2)^2\\
    &= a^{2}_{1}\sigma^{2}_{1} + 2a_1 a_2 \mu_1 \mu_2 + a_{2}^{2}\sigma^{2}_{2} - (a_{1}^{2}\mu_{1}^{2} + 2a_1 a_2 \mu_1 \mu_2 + a_{2}^{2}\mu_{2}^{2} )\\
    &= \underline{a_{1}^{2}(\sigma^{2}_{1} - \mu^{2}_{1}) + a_{2}^{2}(\sigma^{2}_{2} - \mu^{2}_{2})}
\end{aligned}
\]

\hypertarget{b-ea_1-y_1---a_2-y_2-vara_1-y_1---a_2-y_2}{%
\subsection{\texorpdfstring{(b) \(E[a_1 Y_1 - a_2 Y_2]\),
\(Var[a_1 Y_1 - a_2 Y_2]\)}{(b) E{[}a\_1 Y\_1 - a\_2 Y\_2{]}, Var{[}a\_1 Y\_1 - a\_2 Y\_2{]}}}\label{b-ea_1-y_1---a_2-y_2-vara_1-y_1---a_2-y_2}}

\[
\begin{aligned}
  E[a_1 Y_1 - a_2 Y_2] &= E[a_1 Y_1] - E[a_2 Y_2]\\
                       &= a_1 E[Y_1] - a_2 E[Y_2]\\
                       &= \underline{a_1 \mu_1 - a_2 \mu_{2}}\\
\end{aligned}
\]

\[
\begin{aligned}
    Var[a_1 Y_1 - a_2 Y_2] &= E[ [(a_1 Y_1 - a_2 Y_2) - E[a_1 Y_1 - a_2 Y_2]]^{2}]\\
    &= E[(a_1 Y_1 - a_2 Y_2)^2] - E[a_1 Y_1 - a_2 Y_2]^2\\
    &= E[a_{1}^{2}Y_{1}^{2} - 2a_1 a_2 Y_1 Y_2 + a_{2}^{2}Y_{2}^{2}] - (a_1 \mu_1 - a_2 \mu_2)^2\\
    &= a^{2}_{1}\sigma^{2}_{1} - 2a_1 a_2 \underbrace{\mu_1 \mu_2}_{E[Y_1 Y_2]=E[Y_1]E[Y_2]} + a_{2}^{2}\sigma^{2}_{2} - (a_1 \mu_1 - a_2 \mu_2)^2\\
    &= a^{2}_{1}\sigma^{2}_{1} + 2a_1 a_2 \mu_1 \mu_2 + a_{2}^{2}\sigma^{2}_{2} - (a_{1}^{2}\mu_{1}^{2} - 2a_1 a_2 \mu_1 \mu_2 + a_{2}^{2}\mu_{2}^{2} )\\
    &= \underline{a_{1}^{2}(\sigma^{2}_{1} - \mu^{2}_{1}) + a_{2}^{2}(\sigma^{2}_{2} - \mu^{2}_{2}) + 4a_1 a_2 \mu_1 \mu_2}
\end{aligned}
\]

\hypertarget{problem-2.3}{%
\section{Problem 2.3}\label{problem-2.3}}

\begin{quote}
Full conditionals: Let \(X\), \(Y\), \(Z\) be random variables with
joint density (discrete or continuous)
\(p(x,y,z) \propto f(x,z)g(y,z)h(z)\). Show that
\end{quote}

\hypertarget{a-pxyz-propto-fxz-i.e.-pxyz-is-a-function-of-x-and-z}{%
\subsection{\texorpdfstring{(a) \(p(x|y,z) \propto f(x,z)\)
i.e.~\(p(x|y,z)\) is a function of \(x\) and
\(z\);}{(a) p(x\textbar y,z) \textbackslash propto f(x,z) i.e.~p(x\textbar y,z) is a function of x and z;}}\label{a-pxyz-propto-fxz-i.e.-pxyz-is-a-function-of-x-and-z}}

Let \(c, d\in \mathbb{R}\) constants

\[
\begin{aligned}
    p(x|y,z) &= \frac{p(x,y,z)}{p(y,z)}\\
             &= \frac{c\cdot f(x,z)g(y,z)h(z)}{p(y,z)}
             &= \frac{c\cdot f(x,z)g(y,z)h(z)}{\int_{x\in\mathbb{X}} p(x,y,z)dx}\\ 
             &= \frac{c\cdot f(x,z)g(y,z)h(z)}{d\cdot \int_{x\in \mathbb{X}} f(x,z)g(y,z)h(z) dx}\\ 
             &= \frac{c\cdot f(x,z)g(y,z)h(z)}{d\cdot g(y,z)h(z)\int_{x\in \mathbb{X}} f(x,z) dx}\\ 
             &= \frac{c\cdot f(x,z)}{d\cdot\int_{x\in \mathbb{X}} f(x,z) dx}\\
             &\propto \underline{\frac{f(x,z)}{\int_{x\in \mathbb{X}} f(x,z) dx}}
\end{aligned}
\]

Thus, \(p(x|y,z)\) is a function of \(f(x,z)\).

\hypertarget{b-pyxz-propto-gyz-i.e.-pyxz-is-a-function-of-y-and-z}{%
\subsection{\texorpdfstring{(b) \(p(y|x,z) \propto g(y,z)\)
i.e.~\(p(y|x,z)\) is a function of \(y\) and
\(z\);}{(b) p(y\textbar x,z) \textbackslash propto g(y,z) i.e.~p(y\textbar x,z) is a function of y and z;}}\label{b-pyxz-propto-gyz-i.e.-pyxz-is-a-function-of-y-and-z}}

Let \(c, d\in \mathbb{R}\) constants

\[
\begin{aligned}
    p(y|x,z) &= \frac{p(x,y,z)}{p(x,z)}\\
             &= \frac{p(x,y,z)}{\int_{y\in \mathbb{Y}}p(x,y,z)dy}\\ 
             &= \frac{c\cdot f(x,z)g(y,z)h(z)}{d\cdot \int_{y\in\mathbb{Y}} f(x,z)g(y,z)h(z)dy}\\ 
             &= \frac{c\cdot f(x,z)g(y,z)h(z)}{d\cdot f(x,z)h(z)\int_{y\in\mathbb{Y}} g(y,z)dy}\\ 
             &\propto \frac{f(y,z)}{\int_{y\in\mathbb{Y}}g(y,z)dy}\\
\end{aligned}
\]

\hypertarget{c-x-and-y-are-conditionally-independent-given-z.}{%
\subsection{\texorpdfstring{(c) \(X\) and \(Y\) are conditionally
independent given
\(Z\).}{(c) X and Y are conditionally independent given Z.}}\label{c-x-and-y-are-conditionally-independent-given-z.}}

Let \(a_1, a_2 \in \mathbb{R}\) constant,

\[
\begin{aligned}
p(x|z) &= \frac{p(x,z)}{p(z)}\\
       &= \frac{\int_{y\in\mathbb{Y}}p(x,y,z)dy}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} p(x,y,z) dydx}\\ 
       &= \frac{\int_{y\in\mathbb{Y}}f(x,z)g(y,z)h(z) dy}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} p(x,y,z) dydx}\\ 
       &= \frac{f(x,z)h(z)\int_{y\in\mathbb{Y}}g(y,z)dy}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} p(x,y,z) dydx}\\ 
       &= \frac{f(x,z)\int_{y\in\mathbb{Y}}g(y,z)dy}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} f(x,z)g(y,z) dydx}
\end{aligned}
\]

\[
\begin{aligned}
    p(y|z) &= \frac{p(y,z)}{p(z)}\\
           &=  \frac{\int_{x\in\mathbb{X}}p(x,y,z)dx}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} p(x,y,z) dydx}\\
           &= \frac{\int_{x\in \mathbb{X}} f(x,z)g(y,z)h(z) dx}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} p(x,y,z) dydx}\\ 
           &= \frac{g(y,z)h(z)\int_{x\in \mathbb{X}} f(x,z)dx}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} p(x,y,z) dydx}\\
           &= \frac{g(y,z)\int_{x\in \mathbb{X}} f(x,z)dx}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} f(x,z)g(y,z) dydx}\\
\end{aligned}
\]

\[
\begin{aligned}
p(x|z)p(y|z) &= \frac{f(x,z)\int_{y\in\mathbb{Y}}g(y,z)dy}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} f(x,z)g(y,z) dydx} \cdot \frac{g(y,z)\int_{x\in \mathbb{X}} f(x,z)dx}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} f(x,z)g(y,z) dydx}\\
            &= \frac{f(x,z)g(y,z)}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} f(x,z)g(y,z) dydx}\\
\end{aligned}
\]

\[
\begin{aligned}
    p(x,y|z) &= \frac{p(x,y,z)}{p(z)}\\
             &= \frac{f(x,z)g(y,z)h(z)}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} p(x,y,z) dy dx}\\ 
             &= \frac{f(x,z)g(y,z)h(z)}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} f(x,z)g(y,z)h(z) dy dx}\\
             &= \frac{f(x,z)g(y,z)h(z)}{\int_{x\in\mathbb{X}}f(x,z)h(z)\left[\int_{y\in\mathbb{Y}} g(y,z) dy\right] dx}\\ 
             &= \frac{f(x,z)g(y,z)h(z)}{\int_{x\in\mathbb{X}}f(x,z)h(z)dx \cdot \int_{y\in\mathbb{Y}} g(y,z) dy}\\ 
             &=  \frac{f(x,z)}{\int_{x\in\mathbb{X}}f(x,z)dx}\frac{g(y,z)}{\int_{y\in\mathbb{Y}} g(y,z) dy}\\
             &= \frac{f(x,z)g(y,z)}{\int_{x\in\mathbb{X}}\int_{y\in\mathbb{Y}} f(x,z)g(y,z) dydx}\\
             &= \underline{p(x|z)p(y|z)}\\
\end{aligned}
\]

Thus, \(p(x,y|z) = p(x|z)p(y|z)\) that means \(p(x,y|z)\) is
conditionally independent given \(z\).

\hypertarget{problem-2.5}{%
\section{Problem 2.5}\label{problem-2.5}}

\begin{quote}
Urns: Suppose urn \(H\) is filled with \(40\%\) green balls and \(60\%\)
red balls, and urn \(T\) is filled with \(60\%\) green balls and
\(40\%\) red balls. Someone will flip a coin and then select a ball from
urn \(H\) or urn \(T\) depending on whether the coin lands heads or
tails, respectively. Let \(X\) be 1 or 0 if the coin lands heads or
tails, and let \(Y\) be \(1\) or \(0\) if the ball is green or red.
\end{quote}

\hypertarget{tbl-balls}{}
\begin{longtable}[]{@{}lll@{}}
\caption{\label{tbl-balls}Probability of choosing a certain ball in a
given urn.}\tabularnewline
\toprule()
& Green & Red \\
\midrule()
\endfirsthead
\toprule()
& Green & Red \\
\midrule()
\endhead
\(H\) (chosen if head{[}1{]}) & 0.4 & 0.6 \\
\(T\) (chosen if tail{[}0{]}) & 0.6 & 0.4 \\
\bottomrule()
\end{longtable}

\begin{longtable}[]{@{}lll@{}}
\caption{Event coding}\tabularnewline
\toprule()
& 1 & 0 \\
\midrule()
\endfirsthead
\toprule()
& 1 & 0 \\
\midrule()
\endhead
\(X\) & Head & Tail \\
\(Y\) & Green & Red \\
\bottomrule()
\end{longtable}

\hypertarget{a-write-out-the-joint-distribution-of-x-and-y-in-a-table.}{%
\subsection{\texorpdfstring{(a) Write out the joint distribution of
\(X\) and \(Y\) in a
table.}{(a) Write out the joint distribution of X and Y in a table.}}\label{a-write-out-the-joint-distribution-of-x-and-y-in-a-table.}}

Suppose the coin is fair,

\[
\begin{aligned}
    p(X=0 \cap Y=0) &= p(X=0) p(Y=0|X=0)= 0.5\cdot 0.4 = 0.2\\
    p(X=0 \cap Y=1) &= p(X=0) p(Y=1|X=0)= 0.5\cdot 0.6 = 0.3\\
    p(X=1 \cap Y=0) &= p(X=1) p(Y=0|X=1)= 0.5\cdot 0.6 = 0.3\\
    p(X=1 \cap Y=1) &= p(X=1) p(Y=1|X=1)= 0.5\cdot 0.4 = 0.2\\
\end{aligned}
\]

\hypertarget{b-find-ey.-what-is-the-probability-that-the-ball-is-green}{%
\subsection{\texorpdfstring{(b) Find \(E[Y]\). What is the probability
that the ball is
green?}{(b) Find E{[}Y{]}. What is the probability that the ball is green?}}\label{b-find-ey.-what-is-the-probability-that-the-ball-is-green}}

\[
\begin{aligned}
    E[Y] &= \sum_{y\in \{0,1\}} p(Y=y)y\\
         &= p(Y=1)\cdot 1\\ 
         &= \sum_{x\in\{0,1\}} p(Y=1 | X=x)p(X=x)\\
         &= p(Y=1|X=0)p(X=0) + p(Y=1|X=1)p(X=1)\\ 
         &= 0.6\cdot 0.5 + 0.4 \cdot 0.5 \\ 
         &= 0.3 + 0.2 \\ 
         &= 0.5
\end{aligned}
\]

\hypertarget{c-find-varyx0-varyx1-and-vary.-thinking-of-variance-as-measuring-uncertainty-explain-intuitively-why-one-of-these-variances-is-larger-than-others.}{%
\subsection{\texorpdfstring{(c) Find \(Var[Y|X=0]\), \(Var[Y|X=1]\) and
\(Var[Y]\). Thinking of variance as measuring uncertainty, explain
intuitively why one of these variances is larger than
others.}{(c) Find Var{[}Y\textbar X=0{]}, Var{[}Y\textbar X=1{]} and Var{[}Y{]}. Thinking of variance as measuring uncertainty, explain intuitively why one of these variances is larger than others.}}\label{c-find-varyx0-varyx1-and-vary.-thinking-of-variance-as-measuring-uncertainty-explain-intuitively-why-one-of-these-variances-is-larger-than-others.}}

\[
\begin{aligned}
    E[Y|X=0] &= \sum_{y \in\{0,1\}} P_{Y|X=0}(Y=y|X=0)y\\
             &= P_{Y|X=0}(Y=1|X=0)\cdot 1 \\ 
             &= 0.6
\end{aligned}
\]

\[
\begin{aligned}
    E[(Y|X=0)^2] &= \sum_{y \in\{0,1\}} P_{Y|X=0}(Y=y|X=0)y^2\\
             &= P_{Y|X=0}(Y=1|X=0)\cdot 1 \\ 
             &= 0.6
\end{aligned}
\]

\[
\begin{aligned}
    E[Y|X=1] &= \sum_{y \in\{0,1\}} P_{Y|X=1}(Y=y|X=1)y\\
             &= P_{Y|X=1}(Y=1|X=1)\cdot 1 \\ 
             &= 0.4
\end{aligned}
\]

\[
\begin{aligned}
    E[(Y|X=1)^2] &= \sum_{y \in\{0,1\}} P_{Y|X=0}(Y=y|X=1)y^2\\
             &= P_{Y|X=1}(Y=1|X=1)\cdot 1 \\ 
             &= 0.4
\end{aligned}
\]

\[
\begin{aligned}
    E[Y^2] &= \sum_{y\in\{0,1\}} P(Y=y)y^2\\ 
           &= P(Y=1)\cdot 1^2\\ 
           &= P(Y=1 \cap X=1) + P(Y=1 \cap X=0)\\ 
           &= 0.2 + 0.3 = 0.5\\
\end{aligned}
\]

Thus,

\[Var[Y|X=0] = E[(Y|X=0)^2] - E[Y|X=0]^2 = 0.6 - 0.6^2 = \underline{0.24}\]

\[Var[Y|X=1] = E[(Y|X=1)^2] - E[Y|X=1]^2 = 0.4 - 0.4^2 = \underline{0.24}\]

\[Var[Y] = E[Y^2] - E[Y]^2 = 0.5 - 0.5^2 = \underline{0.25}\]

\textbf{Explaination}

\(Var[Y]\) is larger than \(Var[Y|X=0]\) and \(Var[Y|X=1]\) because
\(Y\) can be more determined by the information of \(X\). With known
\(X\), the distribution of \(Y\) is set, and less uncertain with single
confirmed distribution.

\hypertarget{d-suppose-you-see-that-the-ball-is-green.-what-is-the-probability-that-the-coin-turned-up-tails}{%
\subsection{(d) Suppose you see that the ball is green. What is the
probability that the coin turned up
tails?}\label{d-suppose-you-see-that-the-ball-is-green.-what-is-the-probability-that-the-coin-turned-up-tails}}

\[\begin{aligned}
    p(X=0 | Y=1) &= \frac{p(X=0 \cap Y=1)}{p(Y=1)}\\
                 &= \frac{p(X=0 \cap Y=1)}{p(Y=1 | X=0)p(X=0) + p(Y=1|X=1)p(X=1)}\\
                 &= \frac{0.5 \cdot 0.6}{0.6\cdot 0.5 + 0.4\cdot 0.5}\\ 
                 &= \frac{0.3}{0.3+0.2}\\ 
                 &= \frac{0.3}{0.5}\\ 
                 &= \underline{0.6}\\
\end{aligned}\]

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-hoff2009first}{}}%
Hoff, Peter D. 2009. \emph{A First Course in Bayesian Statistical
Methods}. Vol. 580. Springer.

\end{CSLReferences}

\bookmarksetup{startatroot}

\hypertarget{homework-2}{%
\chapter{Homework 2}\label{homework-2}}

Shao-Ting Chiu (UIN:433002162)\\
today

\hfill\break

\begin{quote}
Pending
\end{quote}

\bookmarksetup{startatroot}

\hypertarget{summary}{%
\chapter{Summary}\label{summary}}

In summary, this book has no content whatsoever.

\bookmarksetup{startatroot}

\hypertarget{analysis-example}{%
\chapter{Analysis example}\label{analysis-example}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# import library}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"Hello world"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Hello world
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{references-1}{%
\chapter*{References}\label{references-1}}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-hoff2009first}{}}%
Hoff, Peter D. 2009. \emph{A First Course in Bayesian Statistical
Methods}. Vol. 580. Springer.

\end{CSLReferences}



\end{document}
